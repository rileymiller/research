{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Analysis of HIT Dataset",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF_jKRgrEEQx",
        "colab_type": "text"
      },
      "source": [
        "# Text Analysis of HITS\n",
        "In this notebook I will be performing basic text mining techniques to gain some additional understanding from our scrape of Amazon MTURK.\n",
        "\n",
        "I will be leveraging the scikitlearn framework to perform some of this analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "668eb2d9-6068-462e-b877-78a3cd5aed59",
        "id": "f7pKWDSwOvaY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Library Imports\n",
        "from getpass import getpass\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from time import time\n",
        "import re\n",
        "\n",
        "# Vectorizers from scikitlearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "\n",
        "t0 = time()\n",
        "! curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "! sudo apt-get install git-lfs\n",
        "! git lfs install\n",
        "# wipe all files\n",
        "! rm -rf *\n",
        "# Clone Dataset, utilizes getpass so only users with access to dataset can run the dataset\n",
        "user = getpass('GitHub user')\n",
        "password = getpass('GitHub password')\n",
        "os.environ['GITHUB_AUTH'] = user + ':' + password\n",
        "os.environ['USER'] = user\n",
        "! rm -rf research/\n",
        "! git clone https://$GITHUB_AUTH@github.com/$USER/research.git\n",
        "# ! ls research/datasets/\n",
        "# ! echo \"untarring raw_mongodb_json_dump.tar.xz\" && tar -xzvf research/datasets/raw_mongodb_json_dump.tar.gz\n",
        "! cd research/datasets && ls\n",
        "print('done importing libraries and dataset in %0.3fs.' % (time() - t0))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.9.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 68 not upgraded.\n",
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "GitHub user··········\n",
            "GitHub password··········\n",
            "Cloning into 'research'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 731 (delta 31), reused 72 (delta 27), pack-reused 650\u001b[K\n",
            "Receiving objects: 100% (731/731), 11.68 MiB | 38.84 MiB/s, done.\n",
            "Resolving deltas: 100% (475/475), done.\n",
            "Filtering content: 100% (4/4), 225.48 MiB | 68.27 MiB/s, done.\n",
            "clean_extracted_dirs.sh  hits_unmerged.json\t   preview.json\n",
            "hits.json\t\t mongodb_json_dump.tar.xz  raw_mongodb_json_dump.tar.gz\n",
            "done importing libraries and dataset in 29.779s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3QxjH_Ipi8L",
        "colab_type": "text"
      },
      "source": [
        "## Creating a Bag of Words for Statistical Analysis\n",
        "\n",
        "Bag of Words is an good approach for cleansing data to be used in NLP models. Below I utilize the spaCy API to lemmatize the HIT dataset which combines words that mean the same thing but may have been written in a different tense or case, then I instantiate a vectorizer from `sklearn` which shows the number of occurences of each word in the corpus (simple BoW). This is then displayed in a `pandas` dataframe. The `sklearn` `CountVectorizer` also removes english stop words (words that don't bring much value i.e the, an, etc). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ3PAXwav5PO",
        "colab_type": "code",
        "outputId": "14e69d88-8943-4938-ec31-7029b8354b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# CountVectorizer Bag of Words\n",
        "\n",
        "# create a spaCy tokenizer\n",
        "spacy.load('en')\n",
        "\n",
        "# instantiate a lemmatizer to get rid of different words that mean the same thing\n",
        "lemmatizer = spacy.lang.en.English()\n",
        "\n",
        "hits = []\n",
        "descriptions = []\n",
        "titles = []\n",
        "previews = []\n",
        "\n",
        "t0 = time()\n",
        "# convert hits.json file into dictionary, populate hits list, descriptions list, and titles list\n",
        "for line in open(\"research/datasets/hits.json\"):\n",
        "  hit = json.loads(line)\n",
        "  hits.append(hit)\n",
        "  descriptions.append(hit['description'])\n",
        "  titles.append(hit['title'])\n",
        "print('INFO: done parsing hits dataset, finished in %0.3fs.' % (time() - t0))\n",
        "\n",
        "t0 = time()\n",
        "# convert preview.json file into dictionary, populate previews list\n",
        "for line in open(\"research/datasets/preview.json\"):\n",
        "  preview = json.loads(line)\n",
        "  page_src = preview['page_src']\n",
        "  clean_src = re.sub(\"<.*?>\", \"\", page_src)\n",
        "  previews.append(clean_src)\n",
        "\n",
        "print('INFO: done parsing preview dataset, finished in %0.3fs.' % (time() - t0))\n",
        "# print(hits[0]['title'])\n",
        "# print(previews[0])\n",
        "# create a dataframe from a word matrix\n",
        "def word_matrix_to_data_frame(word_matrix, feat_names):\n",
        "    # create an index for each row\n",
        "    doc_names = ['Description{:d}'.format(idx) for idx, _ in enumerate(word_matrix)]\n",
        "    df = pd.DataFrame(data=word_matrix.toarray(), index=doc_names,\n",
        "                      columns=feat_names)\n",
        "    return(df)\n",
        "\n",
        "\n",
        "# using spaCy lemmatizer, it returns that list of lemmatized tokens\n",
        "def lemma_tokenizer(data): \n",
        "  lemma_tokens = lemmatizer(data)\n",
        "  return [tok.lemma_ for tok in lemma_tokens]\n",
        "\n",
        "# t0 = time()\n",
        "# descriptions = lemma_tokenizer(' '.join([str(s) for s in descriptions]))\n",
        "# print('INFO: done lemmatizing descriptions, finished in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# t0 = time()\n",
        "# titles = lemma_tokenizer(' '.join([str(s) for s in titles]))\n",
        "# print('INFO: done lemmatizing titles, finished in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# t0 = time()\n",
        "# titles = lemma_tokenizer(' '.join([str(s) for s in previews]))\n",
        "# print('INFO: done lemmatizing previews, finished in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# instantiate the CountVectorizer, unigram\n",
        "count_vec = CountVectorizer(lowercase=False, stop_words='english')\n",
        "\n",
        "t0 = time()\n",
        "# convert the documents into a word matrix\n",
        "cv_word_matrix = count_vec.fit_transform(descriptions)\n",
        "print('INFO: done converting descriptions into word matrix, finished in %0.3fs.' % (time() - t0))\n",
        "\n",
        "t0 = time()\n",
        "# extract the features from the description\n",
        "cv_tokens = count_vec.get_feature_names()\n",
        "print('INFO: done extracting features from dataset, finished in %0.3fs.' % (time() - t0))\n",
        "\n",
        "t0 = time()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: done parsing hits dataset, finished in 0.037s.\n",
            "INFO: done parsing preview dataset, finished in 3.288s.\n",
            "INFO: done converting descriptions into word matrix, finished in 0.015s.\n",
            "INFO: done extracting features from dataset, finished in 0.001s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP1GAjnA4Bc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "3967a26b-55ab-4549-c146-efee5a764b4b"
      },
      "source": [
        "# print out the panda dataframe for the CountVectorizer\n",
        "word_matrix_to_data_frame(cv_word_matrix, cv_tokens)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>01</th>\n",
              "      <th>05</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>200</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>25mins</th>\n",
              "      <th>27</th>\n",
              "      <th>2755</th>\n",
              "      <th>28</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>37</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>420</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>4701</th>\n",
              "      <th>...</th>\n",
              "      <th>visible</th>\n",
              "      <th>visit</th>\n",
              "      <th>visual</th>\n",
              "      <th>voice</th>\n",
              "      <th>voluntary</th>\n",
              "      <th>volunteer</th>\n",
              "      <th>volunteers</th>\n",
              "      <th>want</th>\n",
              "      <th>war</th>\n",
              "      <th>watch</th>\n",
              "      <th>watching</th>\n",
              "      <th>waves</th>\n",
              "      <th>way</th>\n",
              "      <th>wealth</th>\n",
              "      <th>web</th>\n",
              "      <th>webpage</th>\n",
              "      <th>website</th>\n",
              "      <th>websites</th>\n",
              "      <th>week</th>\n",
              "      <th>weeks</th>\n",
              "      <th>weight</th>\n",
              "      <th>win</th>\n",
              "      <th>window</th>\n",
              "      <th>word</th>\n",
              "      <th>words</th>\n",
              "      <th>work</th>\n",
              "      <th>worker</th>\n",
              "      <th>workers</th>\n",
              "      <th>working</th>\n",
              "      <th>workout</th>\n",
              "      <th>workplace</th>\n",
              "      <th>world</th>\n",
              "      <th>worth</th>\n",
              "      <th>write</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>years</th>\n",
              "      <th>youth</th>\n",
              "      <th>zillow</th>\n",
              "      <th>zin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Description0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description1050</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description1051</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description1052</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description1053</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Description1054</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1055 rows × 1568 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 00  01  05  10  100  ...  written  years  youth  zillow  zin\n",
              "Description0      0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description1      0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description2      0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description3      0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description4      0   0   0   0    0  ...        0      0      0       0    0\n",
              "...              ..  ..  ..  ..  ...  ...      ...    ...    ...     ...  ...\n",
              "Description1050   0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description1051   0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description1052   0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description1053   0   0   0   0    0  ...        0      0      0       0    0\n",
              "Description1054   0   0   0   0    0  ...        0      0      0       0    0\n",
              "\n",
              "[1055 rows x 1568 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-yB9IxY4C6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1afe274-6758-4d22-c239-07f7a3ccc2cc"
      },
      "source": [
        "print('INFO: done printing dataframe for BoW CountVectorizer on dataset descriptions, finished in %0.3fs.' % (time() - t0))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: done printing dataframe for BoW CountVectorizer on dataset descriptions, finished in 0.192s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC9w5Xetzvh5",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling using NNMF and LDA\n",
        "For the next part of this analysis I will be leveraging Non-negative Matrix Factorization (LSI) and Latentent Dirichlet Allocation to extract topics from the MTURK web scrape.\n",
        "\n",
        "For the NNMF I will utilize a TF-IDF BoW (Vectorizer) which weights words based of their appearance in an individual document as well as how frequently they appear in the corpus as a whole.\n",
        "\n",
        "**Global variables and common functions below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeGRWmDq0sMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare the number of features to extract from the dataset\n",
        "NUM_FEATURES = 1000\n",
        "# Declare the number of topics to fit to\n",
        "NUM_COMPONENTS = 10\n",
        "# Grabs the top words for each topic\n",
        "NUM_TOP_WORDS = 20\n",
        "\n",
        "# remove html entities from docs and\n",
        "# set everything to lowercase\n",
        "def strip_html_preprocessor(doc):\n",
        "    return(unescape(doc).lower())\n",
        "\n",
        "# returns TFIDF word matrix, used in NNMF, dataset: list\n",
        "def get_tfidf_word_matrix_vectorizer(dataset): \n",
        "  # extract tfidf features for use in the NNMF\n",
        "  tfidf_vectorizer = TfidfVectorizer(preprocessor=strip_html_preprocessor, max_df=0.95, min_df=2, max_features=NUM_FEATURES,\n",
        "                                    stop_words='english', lowercase=False)\n",
        "  t0 = time()\n",
        "  tf_word_matrix = tfidf_vectorizer.fit_transform(dataset)\n",
        "  print(\"INFO: done converting dataset to TFIDF word matrix, done in %0.3fs.\" % (time() - t0))\n",
        "  return tf_word_matrix, tfidf_vectorizer\n",
        "\n",
        "# returns the raw word count word vector for use in LDA\n",
        "def get_cv_word_matrix_vectorizer(dataset):\n",
        "  cv_vectorizer = CountVectorizer(preprocessor=strip_html_preprocessor, lowercase=False, stop_words='english')\n",
        "\n",
        "  t0 = time()\n",
        "  cv_word_matrix = cv_vectorizer.fit_transform(dataset)\n",
        "  print(\"INFO: done converting dataset to CV (raw count) word matrix, done in %0.3fs.\" % (time() - t0))\n",
        "  return cv_word_matrix, cv_vectorizer\n",
        "\n",
        "# prints the top words from the dataset\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcbonqG9B5NO",
        "colab_type": "text"
      },
      "source": [
        "# Text Analysis on Descriptions\n",
        "\n",
        "First Output is for a Non-negative Matrix Factorization fitted with Frobenius Norm and TFIDF features.\n",
        "\n",
        "The second output is for a Non-negative Matrix Factorization generalized on Kullback-Leibler Divergence on TFIDF features. This is equivalent to Probabilistic Latent Semantic Indexing (LSI).\n",
        "\n",
        "The third output is a Latent Dirilecht Allocation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNUU2Caz6fBy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "185d8b07-a23d-4338-be75-9204c0df558c"
      },
      "source": [
        "# Extracts the TFIDF word matrix and Vectorizer\n",
        "tf_word_matrix_descriptions, tf_vec_descriptions = get_tfidf_word_matrix_vectorizer(descriptions)\n",
        "\n",
        "# fits NNMF with Frobenius norms on descriptions\n",
        "t0 = time()\n",
        "nnmf_fr = NMF(n_components=NUM_COMPONENTS, random_state=1,\n",
        "              alpha=0.1, l1_ratio=.5).fit(tf_word_matrix_descriptions)\n",
        "print('INFO: Finished fitting NNMF with Frobenius Norm and TFIDF word matrix, done in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# extracts top features from dataset\n",
        "tfidf_top_features = tf_vec_descriptions.get_feature_names()\n",
        "\n",
        "# prints topics for NNMF with Frobenius norms and TFIDF on HIT descriptions\n",
        "print_top_words(nnmf_fr, tfidf_top_features, NUM_TOP_WORDS)\n",
        "\n",
        "# fits NNMF(generalized with Kullback-Leibler divergence) with TFIDF\n",
        "t0 = time()\n",
        "nnmf_kld = NMF(n_components=NUM_COMPONENTS, random_state=1, \n",
        "               beta_loss='kullback-leibler',\n",
        "               solver='mu', max_iter=1000, alpha=.1,\n",
        "               l1_ratio=0.5).fit(tf_word_matrix_descriptions)\n",
        "print('INFO: Finished fitting NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix, done in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# prints topics for NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix\n",
        "print_top_words(nnmf_kld, tfidf_top_features, NUM_TOP_WORDS)\n",
        "\n",
        "# fits LDA on raw count word matrix\n",
        "t0 = time()\n",
        "lda = LatentDirichletAllocation(n_components=NUM_COMPONENTS, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50,\n",
        "                                random_state=0)\n",
        "\n",
        "cv_word_matrix_descriptions, cv_vec_descriptions = get_cv_word_matrix_vectorizer(descriptions)\n",
        "\n",
        "# fit LDA on raw count word matrix\n",
        "lda.fit(cv_word_matrix_descriptions)\n",
        "print('INFO: Finished fitting LDA with raw word matrix, done in %0.3fs.', (time() - t0))\n",
        "\n",
        "cv_top_features = cv_vec_descriptions.get_feature_names()\n",
        "print_top_words(lda, cv_top_features, NUM_TOP_WORDS)\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: done converting dataset to TFIDF word matrix, done in 0.018s.\n",
            "INFO: Finished fitting NNMF with Frobenius Norm and TFIDF word matrix, done in 0.163s.\n",
            "Topic #0: audit user contained transcribed receipt information image transcribe amounts grocery items given enter numbers upcs scanned data recipe expected experience\n",
            "Topic #1: validate pair question provided answer questions read based filling filled file fields feedback fast zoster family failure extract extra experimental\n",
            "Topic #2: video short transcribe data images amounts receipt grocery watch following search upcs numbers surgeon recording total complete minute card say\n",
            "Topic #3: classify following image text images data transcribe adult content write replacement words shown displayed extra extract failure family fast feedback\n",
            "Topic #4: audio installed said flash listen clip review seconds transcribe minute minutes 31 14 recording data second images address 22 11\n",
            "Topic #5: shopping extract receipts summary items information total date business phone number fields 12 24 purchased receipt readable transcribe tell bonus\n",
            "Topic #6: errors grammar spelling correct english text style choose punctuation correcting casing edit answers write unique words questions brand help type\n",
            "Topic #7: hit item rules select following matching class picture type description based data images replacement provided transcribe grocery amounts numbers receipt\n",
            "Topic #8: survey complete minute minutes study short questions answer 15 group people 10 research brief simple attitudes consumer qualify understand participate\n",
            "Topic #9: draw boxes person image bounding specified objects images data group fields flagging financial finally filling filled file feedback follow fast\n",
            "\n",
            "INFO: Finished fitting NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix, done in 0.174s.\n",
            "Topic #0: receipt contained transcribed user audit information image feedback finally filling filled file fields zoster financial family failure extract extra experimental\n",
            "Topic #1: answer question pair validate provided pick best choice answers categories meeting limited pickfu responders previous target demographics steps return story\n",
            "Topic #2: transcribe short video search surgeon domain watch results identification surgical skill undefined procedure level rate meant patient shown research gallon\n",
            "Topic #3: following classify data text image images content shown displayed adult words corresponding id need task page contains online ensure satisfies\n",
            "Topic #4: audio review listen said flash installed seconds clip transcribe website address collect email recording given requested minute script info length\n",
            "Topic #5: shopping extract receipts items summary total number business phone date fields information 12 company tell bonus 24 card scanned store\n",
            "Topic #6: text correct grammar errors english spelling style unique write words choose article help brand accepted sentences edit questions le reviews\n",
            "Topic #7: hit item rules select amounts grocery description following type based numbers test matching upcs receipt picture class score replacement accept\n",
            "Topic #8: survey study complete questions minutes read minute opinion asked people task tasks decisions 10 understand participate 15 like social research\n",
            "Topic #9: person draw boxes image given taskdescription enter view create similar identify queries examples rate form looking recipe objects input images\n",
            "\n",
            "INFO: done converting dataset to CV (raw count) word matrix, done in 0.014s.\n",
            "INFO: Finished fitting LDA with raw word matrix, done in %0.3fs. 0.8198854923248291\n",
            "Topic #0: short survey video transcribe study complete questions minutes answer minute asked research participate rate surgeon address make 10 read email\n",
            "Topic #1: following provided image answer question validate pair classify hit shopping extract item data text images receipts person draw boxes items\n",
            "Topic #2: receipt transcribe amounts grocery item numbers upcs pick best card gift transfer inside lock unlock details identify le les materials\n",
            "Topic #3: recipe given view determine typed worker reasonably shows punchline anti respuestas modera la específicos señalando teniendo dirección problemas las en\n",
            "Topic #4: image information receipt contained transcribed user audit hit play certain test requires score chessgame qualification accept enter id task given\n",
            "Topic #5: audio review listen transcribe seconds clip installed flash said given search song enjoy minute create website examples queries similar people\n",
            "Topic #6: grammar spelling text correct english errors style questions edit answers casing correcting punctuation verify store single payment value specific point\n",
            "Topic #7: voice objects visit game satisy configuration arrange spatial need screen talking record upload spend computer website recording minute say virtual\n",
            "Topic #8: study minutes unique join article earn words hit fast write read group participants text tasks takes instructions 50 complete accepted\n",
            "Topic #9: taskdescription select read description rate estimate issue educated guess reputation polarity quality category software hit given make choices worker judge\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNUNE7ROHyuK",
        "colab_type": "text"
      },
      "source": [
        "# Text Analysis on Title\n",
        "\n",
        "First Output is for a Non-negative Matrix Factorization fitted with Frobenius Norm and TFIDF features.\n",
        "\n",
        "The second output is for a Non-negative Matrix Factorization generalized on Kullback-Leibler Divergence on TFIDF features. This is equivalent to Probabilistic Latent Semantic Indexing (LSI).\n",
        "\n",
        "The third output is a Latent Dirilecht Allocation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qOTDzffIB5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "9b3d78ca-6498-45fb-e162-d36fcf09aa9d"
      },
      "source": [
        "# Extracts the TFIDF word matrix and Vectorizer\n",
        "tf_word_matrix_titles, tf_vec_titles = get_tfidf_word_matrix_vectorizer(titles)\n",
        "\n",
        "# fits NNMF with Frobenius norms on titles\n",
        "t0 = time()\n",
        "nnmf_fr = NMF(n_components=NUM_COMPONENTS, random_state=1,\n",
        "              alpha=0.1, l1_ratio=.5).fit(tf_word_matrix_titles)\n",
        "print('INFO: Finished fitting NNMF with Frobenius Norm and TFIDF word matrix, done in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# extracts top features from dataset\n",
        "tfidf_top_features = tf_vec_titles.get_feature_names()\n",
        "\n",
        "# prints topics for NNMF with Frobenius norms and TFIDF on HIT titles\n",
        "print_top_words(nnmf_fr, tfidf_top_features, NUM_TOP_WORDS)\n",
        "\n",
        "# fits NNMF(generalized with Kullback-Leibler divergence) with TFIDF\n",
        "t0 = time()\n",
        "nnmf_kld = NMF(n_components=NUM_COMPONENTS, random_state=1, \n",
        "               beta_loss='kullback-leibler',\n",
        "               solver='mu', max_iter=1000, alpha=.1,\n",
        "               l1_ratio=0.5).fit(tf_word_matrix_titles)\n",
        "print('INFO: Finished fitting NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix, done in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# prints topics for NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix\n",
        "print_top_words(nnmf_kld, tfidf_top_features, NUM_TOP_WORDS)\n",
        "\n",
        "# fits LDA on raw count word matrix\n",
        "t0 = time()\n",
        "lda = LatentDirichletAllocation(n_components=NUM_COMPONENTS, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50,\n",
        "                                random_state=0)\n",
        "\n",
        "cv_word_matrix_titles, cv_vec_titles = get_cv_word_matrix_vectorizer(titles)\n",
        "\n",
        "# fit LDA on raw count word matrix\n",
        "lda.fit(cv_word_matrix_titles)\n",
        "print('INFO: Finished fitting LDA with raw word matrix, done in %0.3fs.', (time() - t0))\n",
        "\n",
        "cv_top_features = cv_vec_titles.get_feature_names()\n",
        "print_top_words(lda, cv_top_features, NUM_TOP_WORDS)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: done converting dataset to TFIDF word matrix, done in 0.012s.\n",
            "INFO: Finished fitting NNMF with Frobenius Norm and TFIDF word matrix, done in 0.135s.\n",
            "Topic #0: audit receipt transcription items purchased invoice restaurant enter itemization bonus card emotions dollars determine development dirección english discretion document does\n",
            "Topic #1: validation answer survey question song listen questions personality restaurant emotions quick hiphop rap editing discretion desktop details determine development electronics\n",
            "Topic #2: video short transcribe shown clip skill surgeon rate search samples recording task recipe minute address store discretion editing details determine\n",
            "Topic #3: audio transcription recording address email survey transcribe hieroglyph train_sec_prefilledcontent_11222019_rectified_v1 french partially filled card document dollars does discretion draw domain development\n",
            "Topic #4: following image classify classification images enter shown contain description summary mark lines bits evaluate entry simple adult content quality email\n",
            "Topic #5: shopping extract items receipts summary information purchased date business total phone number fields 12 receipt 24 10 general enter bonus\n",
            "Topic #6: survey minutes study 10 15 complete decision minute answer consumer research short 30 personality psychology perceptions song listen making 45\n",
            "Topic #7: text bits classify short english edit copy proofread review question media minute summary earn draw discretion connectivity document does comprendere\n",
            "Topic #8: person draw box bounding boxes question bee 75 earn discretion document does dollars domain zoster easy development edit editing educational\n",
            "Topic #9: data transcribe collect website images pdf files entry collection contact card extract document search connectivity internet websites post recording info\n",
            "\n",
            "INFO: Finished fitting NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix, done in 0.188s.\n",
            "Topic #0: receipt audit transcription restaurant invoice itemization bonus gas value single verify station price document dirección determine details discretion desktop development\n",
            "Topic #1: answer validation survey question listen song questions quick hiphop rap telephone indie alternative simple photo brands preferences la le include\n",
            "Topic #2: transcribe video short rate search shown clip skill surgeon recipe domain results registrars category samples use pick store written easy\n",
            "Topic #3: audio transcription address email recording train_sec_prefilledcontent_11222019_rectified_v1 hieroglyph info filled partially clips recordings french spanish judge polarity reputation website contenuti domain\n",
            "Topic #4: image following classify write enter classification product images email quality simple address shown description lines hotel contain moderation match values\n",
            "Topic #5: extract shopping items receipts information summary business purchased total number fields 12 date phone card enter 24 10 16 bonuses\n",
            "Topic #6: minutes survey study game 10 15 minute complete research decision play consumer task chess min perceptions social bonus language emotions\n",
            "Topic #7: text bits classify english short copy edit tasktitle proofread review article virtual 700 choose assistant earn send media oil cbd\n",
            "Topic #8: person draw box identify bounding boxes 75 moving bee question prices message books include earn does english en emotions email\n",
            "Topic #9: data item id batch tell collect general hit contents content select words worker website adult company warning discretion contain advised\n",
            "\n",
            "INFO: done converting dataset to CV (raw count) word matrix, done in 0.010s.\n",
            "INFO: Finished fitting LDA with raw word matrix, done in %0.3fs. 0.7055566310882568\n",
            "Topic #0: audio transcription receipts shopping extract information summary total business number phone date items 12 address fields 50 24 earn bonuses\n",
            "Topic #1: text english edit copy company website proofread minute agents screencast targeting upload estate real page url linkedin determine contact record\n",
            "Topic #2: minutes survey study minute complete 10 15 short consumer perceptions 19 decision min group 30 bonus academic 50 task owner\n",
            "Topic #3: item id batch tell general contents select matching type class 31177 tasktitle electronics cost replace furniture 31182 assistant 31183 virtual\n",
            "Topic #4: data transcribe person box draw items receipt extract shopping purchased search email restaurant enter invoice collect itemization bonus domain results\n",
            "Topic #5: receipt transcription audit short transcribe video text classify bits game play chess objects arrange spanish train_sec_prefilledcontent_11222019_rectified_v1 hieroglyph partner world cooperative\n",
            "Topic #6: answer validation survey listen song collect value single verify questions stg question hiphop rap linkedin telephone simple tweets indie alternative\n",
            "Topic #7: article study words car hello smart 420 provided write 700 visual attention author quote select interesting health behaviours listening lifestyle\n",
            "Topic #8: image following classify card pinterest gift reverse recipe le classification produit qualifier les meilleures sélectionner pour informations relevance task images\n",
            "Topic #9: content hit contain adult worker warning discretion advised image product editing copy batch shown rate video skill surgeon clip transcribe\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuLlmv7Zb9vD",
        "colab_type": "text"
      },
      "source": [
        "# Text Analysis on Previews\n",
        "\n",
        "**For the page source of the previews I'm stripping the html tags with a regex to get just the strings from the page source. Need to remove the css and the javascript to improve the analysis.**\n",
        "\n",
        "First Output is for a Non-negative Matrix Factorization fitted with Frobenius Norm and TFIDF features.\n",
        "\n",
        "The second output is for a Non-negative Matrix Factorization generalized on Kullback-Leibler Divergence on TFIDF features. This is equivalent to Probabilistic Latent Semantic Indexing (LSI).\n",
        "\n",
        "The third output is a Latent Dirilecht Allocation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ghla3zcGqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "09996ee5-f474-4559-aed4-eb8c9217be76"
      },
      "source": [
        "# Extracts the TFIDF word matrix and Vectorizer\n",
        "tf_word_matrix_previews, tf_vec_previews = get_tfidf_word_matrix_vectorizer(previews)\n",
        "\n",
        "# fits NNMF with Frobenius norms on previews\n",
        "t0 = time()\n",
        "nnmf_fr = NMF(n_components=NUM_COMPONENTS, random_state=1,\n",
        "              alpha=0.1, l1_ratio=.5).fit(tf_word_matrix_previews)\n",
        "print('INFO: Finished fitting NNMF with Frobenius Norm and TFIDF word matrix, done in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# extracts top features from dataset\n",
        "tfidf_top_features = tf_vec_previews.get_feature_names()\n",
        "\n",
        "# prints topics for NNMF with Frobenius norms and TFIDF on HIT previews\n",
        "print_top_words(nnmf_fr, tfidf_top_features, NUM_TOP_WORDS)\n",
        "\n",
        "# fits NNMF(generalized with Kullback-Leibler divergence) with TFIDF\n",
        "t0 = time()\n",
        "nnmf_kld = NMF(n_components=NUM_COMPONENTS, random_state=1, \n",
        "               beta_loss='kullback-leibler',\n",
        "               solver='mu', max_iter=1000, alpha=.1,\n",
        "               l1_ratio=0.5).fit(tf_word_matrix_previews)\n",
        "print('INFO: Finished fitting NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix, done in %0.3fs.' % (time() - t0))\n",
        "\n",
        "# prints topics for NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix\n",
        "print_top_words(nnmf_kld, tfidf_top_features, NUM_TOP_WORDS)\n",
        "\n",
        "# fits LDA on raw count word matrix\n",
        "t0 = time()\n",
        "lda = LatentDirichletAllocation(n_components=NUM_COMPONENTS, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50,\n",
        "                                random_state=0)\n",
        "\n",
        "cv_word_matrix_previews, cv_vec_previews = get_cv_word_matrix_vectorizer(previews)\n",
        "\n",
        "# fit LDA on raw count word matrix\n",
        "lda.fit(cv_word_matrix_previews)\n",
        "print('INFO: Finished fitting LDA with raw word matrix, done in %0.3fs.', (time() - t0))\n",
        "\n",
        "cv_top_features = cv_vec_previews.get_feature_names()\n",
        "print_top_words(lda, cv_top_features, NUM_TOP_WORDS)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: done converting dataset to TFIDF word matrix, done in 5.561s.\n",
            "INFO: Finished fitting NNMF with Frobenius Norm and TFIDF word matrix, done in 2.892s.\n",
            "Topic #0: receipt business total gas fuel valid number store excluding slip bal phone doctored items zoomed spanish french fake receipts english\n",
            "Topic #1: item function console true false input error brand let result class var price val abp return quantity isvalid hit info\n",
            "Topic #2: ng p9r shreds cloak u0026awsaccesskeyid u0026expires cipher akiajz57f2vtbzpy6uma 3d signature amazonaws s3 production jpeg media mary id com blankunclearrotatezoom special\n",
            "Topic #3: msg var conversation_id socket log message function div glass window null url button command css data id magnifier agent_id ui\n",
            "Topic #4: paper layout flex font _font webkit elevation light blue material common apply grid var smoothing ms deep shadow host purple\n",
            "Topic #5: nreum function countdown window var seconds exports return minutes init __nr_require performanceobserver page_view_timing id ga typeof glyphicon nr lines new\n",
            "Topic #6: 2019 receipt 12 11 date transaction coupon 02 modal dialog total 09 item 10 items warning 18 29 28 example\n",
            "Topic #7: break word survey collapse expand text link content margin click paste form wrap auto website normal group code instructions copy\n",
            "Topic #8: gtin items border var table color font padding parseint function solid align data input size valid div vertical background item\n",
            "Topic #9: transform rotate css cost prop 100 count receipt focus zoomstring required disabled function input receipt_image quantity left unreadable valid item\n",
            "\n",
            "INFO: Finished fitting NNMF (generalized with Kullback-Leibler divergence) with TFIDF word matrix, done in 9.809s.\n",
            "Topic #0: receipt 2019 total number items 11 valid 12 gas example business fuel choose date bal 02 store phone examples receipts\n",
            "Topic #1: function item true input return type var price false val hit error console quantity class brand result let ui alert\n",
            "Topic #2: ng p9r shreds cloak amazonaws s3 u0026awsaccesskeyid u0026expires cipher akiajz57f2vtbzpy6uma 3d signature production jpeg media com https work mary special\n",
            "Topic #3: msg conversation_id socket var log message div glass null window url command button css data magnifier agent_id send connected id\n",
            "Topic #4: paper layout font flex webkit _font var ctx elevation corners blue mode light rgba center grey common shadow apply evt\n",
            "Topic #5: nreum var id function window return countdown exports seconds init invoice minutes rate ga __nr_require page_view_timing performanceobserver input_value data document\n",
            "Topic #6: coupon 2019 date item modal dialog transaction 12 void 18 wants warning validation hit 59 47 42 31 14 99\n",
            "Topic #7: word break survey collapse expand link text content answer click turksetassignmentid margin like exactly profile form code paste skip copy\n",
            "Topic #8: border var items color gtin padding font background table input display solid valid align margin data size width 10px val\n",
            "Topic #9: rotate transform services receipt css fullcontact address 100 cost prop business count zoomstring focus enter readable receipt_image reason required disabled\n",
            "\n",
            "INFO: done converting dataset to CV (raw count) word matrix, done in 5.548s.\n",
            "INFO: Finished fitting LDA with raw word matrix, done in %0.3fs. 33.35632133483887\n",
            "Topic #0: var wants data ctx mode corners new appointment evt annotations function length options break study push 04 case array customer\n",
            "Topic #1: id return function input var item val data nreum form code rate invoice input_value true weight container mturk length index\n",
            "Topic #2: function var item input true false type text return val items color background prop css click width form border valid\n",
            "Topic #3: counselor farms foods organic company la groupe natural health fresh country good nature brand gourmet clinical red naturals dr products\n",
            "Topic #4: address image fullcontact contact information hit data card com example email company terms click enter phone hits button worker work\n",
            "Topic #5: margin left right width fa border position color transform size padding group inline child height media button active list business\n",
            "Topic #6: paper font layout flex var webkit _font color common ms elevation apply input light blue 100 align material content app\n",
            "Topic #7: com text copy info profile exactly appears https p9r amazonaws s3 ng image linkedin work shreds company media id production\n",
            "Topic #8: receipt 2019 var function 12 total 11 msg valid image number data return text business items date message id div\n",
            "Topic #9: receipt 2019 date 12 item hit function modal example false dialog transaction items 11 var warning coupon click 18 10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}