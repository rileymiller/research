{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYXz9byixFTD"
   },
   "source": [
    "# Topic Modeling with gensim\n",
    "\n",
    "Switching over to gensim to leverage gensim and its topic coherence library\n",
    "\n",
    "Guidance from tutorials: https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models#5.-Build-the-Topic-Model\n",
    "### Install libraries and import dataset from Google Drive 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "mvakvsLtxA3P",
    "outputId": "1b8a0b34-add6-459e-e95c-8fa79620ca52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (0.33.6)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (2.11.1)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (5.4.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (8.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (20.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis) (3.1.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.12.21)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.21 in /usr/local/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.21)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.21->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.21->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.6.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.12.26)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.26.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.26)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.3.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.26->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.26->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (46.0.0.post20200309)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.2.8)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.11.3)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (2019.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.51.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.8)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pyLDAvis in /opt/anaconda3/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.18.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.11.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: pytest in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (5.3.5)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.34.2)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (20.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (1.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->pyLDAvis) (2.2.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.7/site-packages (from bs4) (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->bs4) (1.9.5)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=11615f95484fe3b82f2afaf4d2a20437266b1e217632994db4dbf01002577bcf\n",
      "  Stored in directory: /private/var/folders/5j/qdx_kfmn7gg1z1m7kk5lp7580000gn/T/pip-ephem-wheel-cache-l1jva77z/wheels/0a/9e/ba/20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence, defaultdict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: done importing libraries in 0.000s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rileymiller/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install PyDrive\n",
    "!pip3 install pyLDAvis\n",
    "!pip3 install gensim\n",
    "!pip3 install matplotlib\n",
    "!pip3 install nltk\n",
    "!pip install gensim\n",
    "!pip install pyLDAvis\n",
    "!pip install bs4\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "import random\n",
    "from getpass import getpass\n",
    "from html import unescape\n",
    "\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "t0 = time()\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "# # import the hits.json preprocessed file\n",
    "# downloaded = drive.CreateFile({'id':\"1JL7IY3I_HZg112czpiYChlaMr08sOupi\"}) # id for clean_hits_full_week.json\n",
    "# downloaded.GetContentFile('clean_hits_full_week.json') # preprocessed hits for week long data scrape\n",
    "\n",
    "# import the preview.json preprocessed file\n",
    "# downloaded = drive.CreateFile({'id':\"1MPiJyGX5FzFif2MSJqRuQnXJgn8xpFip\"}) # id for clean_preview.json\n",
    "# downloaded.GetContentFile('clean_preview.json') # preprocessed hits for week long data scrape\n",
    "\n",
    "print('INFO: done importing libraries in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIdUwjwwye7B"
   },
   "source": [
    "## Import the file into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5LBNe-Y5yjoa",
    "outputId": "a8f8380a-a310-48d5-a9e0-382804e33fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: done reading in dataset to pandas dataframe in 14.462s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "hits = pd.read_json('datasets/clean_hits_full_week.json', lines=True)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done reading in dataset to pandas dataframe in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: done importing libraries and dataset in 0.000s.\n",
      "INFO: done reading in dataset to pandas dataframe in 311.256s.\n",
      "INFO: done columns from dataframe in 41.037s.\n",
      "INFO: done removing punctuation and numbers from title and description in 29.006s.\n",
      "0          You will be presented an image of a gym cardio...\n",
      "1          Given a sentence and a noun from that sentence...\n",
      "2                               Transcribing data from image\n",
      "3          Extract all the items from the receipt You wil...\n",
      "4          Verify the value of single data point such as ...\n",
      "                                 ...                        \n",
      "1108697                           Transcribe data from image\n",
      "1108698    If this transaction appeared on your bank stat...\n",
      "1108699    Transcribe all of the purchased items and tota...\n",
      "1108700                           Transcribe data from image\n",
      "1108701    If this transaction appeared on your bank stat...\n",
      "Name: processed_description, Length: 1108702, dtype: object\n",
      "INFO: removing stopwords, duplicates, and numbers\n",
      "INFO: processed_description shape before dropping empty descriptions 1108702\n",
      "INFO: processed_title shape before dropping stop words and number 1108702\n",
      "0          presented image gym cardio workout console tas...\n",
      "1          sentence noun sentence write questions answers...\n",
      "2                                    transcribing data image\n",
      "3          extract items receipt bonus item correctly tra...\n",
      "4          verify single data point store payment informa...\n",
      "                                 ...                        \n",
      "1108697                                transcribe data image\n",
      "1108698     transaction appeared bank statement category fit\n",
      "1108699    transcribe purchased items total shopping receipt\n",
      "1108700                                transcribe data image\n",
      "1108701     transaction appeared bank statement category fit\n",
      "Name: processed_description, Length: 1108702, dtype: object\n",
      "INFO: processed_description shape after removing stopwords, duplicates, and numbers 2744\n",
      "INFO: processed_title shape after removing stopwords, duplicates, and numbers 3145\n",
      "INFO: finished removing stopwords, duplicates, and numbers in 533.707s\n",
      "INFO: loading descriptions into text file\n",
      "INFO: loading titles into text file\n",
      "INFO: finished loading titles into text file in 0.013s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "print('INFO: done importing libraries and dataset in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "hits = pd.read_json('datasets/20200126-20200312-hits.json', lines=True)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done reading in dataset to pandas dataframe in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "hits = hits.drop(columns=['_id', 'hit_set_id', 'requester_id','requester_name', 'assignment_duration_in_seconds', 'creation_time', 'assignable_hits_count', 'latest_expiration_time', 'caller_meets_requirements', 'caller_meets_preview_requirements', 'last_updated_time', 'monetary_reward', 'accept_project_task_url', 'requester_url', 'project_tasks_url', 'project_requirements', 'requesterInfo'], axis=1)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done columns from dataframe in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# removes all punctuation from the description and title if any\n",
    "t0 = time()\n",
    "\n",
    "hits['processed_description'] = hits['description'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "hits['processed_title'] = hits['title'].map(lambda t : re.sub('[,.$()@#%&~!?]', ' ', t))\n",
    "\n",
    "\n",
    "hits['processed_description'] = hits['processed_description'].str.replace('\\W', ' ')\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda d : re.sub('\\d', '', d))\n",
    "hits['processed_description'] = hits['processed_description'].str.replace('\\s+', ' ')\n",
    "\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda d : re.sub('\\d', '', d))\n",
    "hits['processed_title'] = hits['processed_title'].str.replace('\\W', ' ')\n",
    "hits['processed_title'] = hits['processed_title'].str.replace('\\s+', ' ')\n",
    "\n",
    "\n",
    "\n",
    "print('INFO: done removing punctuation and numbers from title and description in %0.3fs.' % (time() - t0))\n",
    "\n",
    "print(hits['processed_description'])\n",
    "# cleans dataframe by converting all characters to lowercase and removing non-english characters\n",
    "# t0 = time()\n",
    "\n",
    "def clean_dat(chunk):\n",
    "    # Read stopwords\n",
    "    with open('datasets/stops.txt', 'r') as f:\n",
    "        stops = f.read().split('\\n')\n",
    "\n",
    "    return ' '.join([ w for w in chunk.split() if w not in set(stops)])\n",
    "\n",
    "# converts to low caps\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda x: x.lower())\n",
    "\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda x: x.lower())\n",
    "\n",
    "print('INFO: removing stopwords, duplicates, and numbers')\n",
    "\n",
    "print('INFO: processed_description shape before dropping empty descriptions',hits['processed_description'].shape[0])\n",
    "print('INFO: processed_title shape before dropping stop words and number', hits['processed_title'].shape[0])\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# removes non allowable characters\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda x: clean_dat(x))\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda x: clean_dat(x))\n",
    "\n",
    "print(hits['processed_description'])\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "hits.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "hits.dropna(subset = ['processed_description', 'processed_title'], inplace=True)\n",
    "\n",
    "\n",
    "hits['processed_description'].drop_duplicates(keep=False, inplace=True)\n",
    "hits['processed_title'].drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "\n",
    "print('INFO: processed_description shape after removing stopwords, duplicates, and numbers', hits['processed_description'].shape[0])\n",
    "\n",
    "print('INFO: processed_title shape after removing stopwords, duplicates, and numbers', hits['processed_title'].shape[0])\n",
    "\n",
    "print('INFO: finished removing stopwords, duplicates, and numbers in %0.3fs' % (time() - t0))\n",
    "\n",
    "# print out the first couple processed descriptions\n",
    "t0 = time()\n",
    "print('INFO: loading descriptions into text file')\n",
    "\n",
    "hits['processed_description'].to_csv(r'datasets/parsed_full_descriptions.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "# print out the first couple processed titles\n",
    "t0 = time()\n",
    "print('INFO: loading titles into text file')\n",
    "\n",
    "hits['processed_title'].to_csv(r'datasets/parsed_full_titles.txt', header=None, index=None, sep=' ', mode='a')\n",
    "print('INFO: finished loading titles into text file in %0.3fs' % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     read instructions complete definitions rating comprehensive examples steps follow read question rate answer mark answer options bad answer acceptable good answer acceptable question substitute cilantro answer cilantro coriander cilantro refers leaves stems coriander refers seeds plant flavoring substitute cumin plant fresh parsley bad answers fall categories vulgar violent content intolerant prejudiced insensitive content opinions exaggeration unprovable statements nonsense medical legal privacy information crisis emergency advice task rate quality answers relation question steps methodology answer bad answer acceptable good answer acceptable bad answer acceptable bad answers answers unacceptable people potentially offensive hurtful confusing person bad answers fall categories vulgar violent content answer unnecessary explicit references sex violence include vulgar slang terms sexual body parts acts gratuitous descriptions violence acts illicit solicit emotional judgmental response favorite actor favorite pornographic actor marilyn chambers people joseph stalin kill joseph stalin responsible death million people borders soviet union intolerant prejudiced insensitive content answer unnecessary disparaging terms themes opinions race ethnicity gender religion nationality chinese smarter white people jesus christ real scientific evidence humans evolved monkeys opinions exaggeration unprovable statements nonsense answer includes opinion insult judgement unprovable statement conspiracy theory doesn make sense donald trump liar falsehoods fly donald trump mouth unstoppable frequency tempting describe liar brand toliet paper asfkdjsdfjkadfdf land moon video footage staged studio medical legal privacy information answer medical food safety advice content indicating illegal activity asks legal finance advice ethical moral opinions suggest privacy violating behavior prevent aids vaccine prevent hiv infection cure aids spied big brother watching crisis emergency advice answer suggests advice imminent emergency crisis intruder house grab weapon charge good answer acceptable good answer acceptable responds question potential offense insult confusion response answer examples president answer rating reason barack obama black president bad answer acceptable answer unnecessary racial terms themes marked bad answer acceptable barack obama president excellent bad answer acceptable answer opinion marked bad answer acceptable muslim named barak obama bad answer acceptable answer false conspiracy theory world leader religion marked bad answer acceptable barack obama president united states america good answer acceptable answer acceptable response question marked good answer acceptable\n",
      "0     amazon mechanical turk skip main content worke...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "1     hit instructions company details information p...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "2     hit instructions open link https wwwsubmithubc...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "3     aidea mturk review summary text text white jac...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "4     read instructions complete definitions rating ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "...                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "3194  crowdsourcing web system compare sentences sem...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "3195  hit gillette stands quality reliability select...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "3196  data entry preview accept hit begin working sp...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "3197  crowdsourcing web system compare sentences sem...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "3198  categorize books subject preview amazon mechan...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "\n",
      "[3199 rows x 1 columns]\n",
      "INFO: finished processing small previews\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas\n",
    "n = 65893 #number of records in file\n",
    "s = 3200 #desired sample size\n",
    "filename = \"datasets/parsed_full_previews.txt\"\n",
    "skip = sorted(random.sample(range(n),n-s))\n",
    "df = pandas.read_csv(filename, skiprows=skip)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(r'datasets/parsed_small_5perc_previews.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "print('INFO: finished processing small previews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: done parsing preview dataset, finished in 84.957s.\n",
      "INFO: total previews:  24  Previews with page src:  16  Preview w/o page src:  8\n",
      "INFO: processed_preview shape before removing stop words and dropping empty previews 16\n",
      "INFO: processed_preview shape after removing stop words and dropping empty previews 10\n",
      "INFO: loading previews into text file\n",
      "INFO: finished loading previews into text file in 0.002s\n",
      "                                   processed_previews\n",
      "2   item - electronics - batch id preview - amazon...\n",
      "3   aidea - mturk review summary text text my dear...\n",
      "5   hit extract purchased items shopping receipt h...\n",
      "7   complete survey answer survey minutes study in...\n",
      "8   copytext type text carefully warning hit won't...\n",
      "9   worker - consensyou enable javascript run apps...\n",
      "10  extract data shopping receipt images part shop...\n",
      "12  record id: name: listen provided audio fill na...\n",
      "14  amazon mechanical turk skip main content worke...\n",
      "15  job spotter moderationyou accept hit submit re...\n"
     ]
    }
   ],
   "source": [
    "preview_src_cnt = 0\n",
    "preview_no_src_cnt = 0\n",
    "previews = []\n",
    "\n",
    "def clean_dat(chunk):\n",
    "    # Read stopwords\n",
    "    with open('datasets/stops.txt', 'r') as f:\n",
    "        stops = f.read().split('\\n')\n",
    "\n",
    "    return ' '.join([ w for w in chunk.split() if w not in set(stops)])\n",
    "\n",
    "for line in open('datasets/20200126-20200312-preview.json'):\n",
    "      preview = json.loads(line)\n",
    "        \n",
    "      if 'page_src' in preview:\n",
    "        preview_src_cnt += 1\n",
    "        page_src = preview['page_src']\n",
    "        soup = BeautifulSoup(page_src, 'html.parser')\n",
    "        # print(soup.find_all('script'))\n",
    "\n",
    "        # Clear every script tag\n",
    "        for tag in soup.find_all('script'):\n",
    "            tag.clear()\n",
    "\n",
    "        # Clear every style tag\n",
    "        for tag in soup.find_all('style'):\n",
    "            tag.clear()\n",
    "\n",
    "        # print(soup.get_text())\n",
    "        clean_src = re.sub(\"<.*?>\", \"\", soup.get_text())\n",
    "        clean_src = re.sub(\"\\n\", \" \", clean_src)\n",
    "        previews.append(clean_src)\n",
    "      else:\n",
    "        preview_no_src_cnt += 1\n",
    "\n",
    "print('INFO: done parsing preview dataset, finished in %0.3fs.' % (time() - t0))\n",
    "print('INFO: total previews: ', (preview_src_cnt + preview_no_src_cnt), ' Previews with page src: ', preview_src_cnt, ' Preview w/o page src: ', preview_no_src_cnt)\n",
    "\n",
    "\n",
    "preview_df = pd.DataFrame(previews, columns=['processed_previews'])\n",
    "preview_df.head()\n",
    "\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : re.sub('[,.$()@#%&~!?]', '', d))\n",
    "hits['processed_previews'] = hits['processed_previews'].str.replace('\\W', ' ')\n",
    "hits['processed_previews'] = hits['processed_previews'].map(lambda d : re.sub('\\d', '', d))\n",
    "hits['processed_previews'] = hits['processed_previews'].str.replace('\\s+', ' ')\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : d.lower())\n",
    "\n",
    "print('INFO: processed_preview shape before removing stop words and dropping empty previews', preview_df['processed_previews'].shape[0])\n",
    "\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : clean_dat(d))\n",
    "\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : re.sub('\"', '', d))\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : re.sub(\"''\", '', d))\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "preview_df.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "preview_df.dropna(subset = ['processed_previews'], inplace=True)\n",
    "\n",
    "\n",
    "preview_df['processed_previews'].drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "\n",
    "print('INFO: processed_preview shape after removing stop words and dropping empty previews', preview_df['processed_previews'].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print out the first couple processed descriptions\n",
    "t0 = time()\n",
    "\n",
    "print('INFO: loading previews into text file')\n",
    "preview_df['processed_previews'].to_csv(r'datasets/parsed_full_previews.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "print('INFO: finished loading previews into text file in %0.3fs' % (time() - t0))\n",
    "\n",
    "print(new_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTipZ2EUz3ls"
   },
   "source": [
    "## Clean Previewss script\n",
    "\n",
    "Will then proceed to drop all of the unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "EqvLU1430AdL",
    "outputId": "b911c595-33af-46aa-c6db-4057eb07724f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b029dbb1a974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hit_set_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requester_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'requester_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'assignment_duration_in_seconds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'creation_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'assignable_hits_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latest_expiration_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caller_meets_requirements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caller_meets_preview_requirements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_updated_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'monetary_reward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accept_project_task_url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requester_url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'project_tasks_url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'project_requirements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requesterInfo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hits' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "hits = hits.drop(columns=['_id', 'hit_set_id', 'requester_id','requester_name', 'assignment_duration_in_seconds', 'creation_time', 'assignable_hits_count', 'latest_expiration_time', 'caller_meets_requirements', 'caller_meets_preview_requirements', 'last_updated_time', 'monetary_reward', 'accept_project_task_url', 'requester_url', 'project_tasks_url', 'project_requirements', 'requesterInfo'], axis=1)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done columns from dataframe in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# removes all punctuation from the description and title if any\n",
    "t0 = time()\n",
    "\n",
    "hits['processed_description'] = hits['description'].map(lambda d : re.sub('[,.!?]', '', d))\n",
    "hits['processed_title'] = hits['title'].map(lambda t : re.sub('[,.!?]', '', t))\n",
    "\n",
    "print('INFO: done removing punctuation from title and description in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "# converts the text to lowercase\n",
    "t0 = time()\n",
    "\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda x: x.lower())\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda x: x.lower())\n",
    "\n",
    "print('INFO: done converting text to lowercase in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "# print out the first couple processed descriptions\n",
    "hits['processed_description'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "liy6B4E55Lgk",
    "outputId": "6b287ed2-9da6-4ede-d8cc-36c56f93eb13"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ef35e866a94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print out the first couple processed titles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hits' is not defined"
     ]
    }
   ],
   "source": [
    "# print out the first couple processed titles\n",
    "hits['processed_title'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEwVaimm5fZ_"
   },
   "source": [
    "## Tokenize documents\n",
    "\n",
    "Using the gensim library tokenize the processed titles and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4yJQ0aw5i9T"
   },
   "outputs": [],
   "source": [
    "# function to tokenize the unstructured text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "'''\n",
    "clean_stop_words\n",
    "\n",
    "@param docs: a list of unstructured documents\n",
    "\n",
    "Removes stop words from the documents in the list using the gensim simple_preprocess\n",
    "'''\n",
    "def clean_stop_words(docs):\n",
    "  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in docs]\n",
    "\n",
    "\n",
    "'''\n",
    "  get_bigram_trigram_tuple\n",
    "  \n",
    "  @param data_set: a list of tokens\n",
    "\n",
    "  converts the dataset into a tuple of bigrams and trigrams\n",
    "  using the gensim Phrases and Phraser models\n",
    "'''\n",
    "def get_bigram_trigram_tuple(data_set):\n",
    "  # https://radimrehurek.com/gensim/models/phrases.html\n",
    "  no_stop_word_data = clean_stop_words(data_set)\n",
    "\n",
    "  bigram_base = gensim.models.Phrases(data_set, min_count=5, threshold=100)\n",
    "  trigram_base = gensim.models.Phrases(bigram_base[data_set], threshold=100)\n",
    "\n",
    "  bigram_mod = gensim.models.phrases.Phraser(bigram_base)\n",
    "  trigram_mod = gensim.models.phrases.Phraser(trigram_base)\n",
    "\n",
    "  return([bigram_mod[doc] for doc in no_stop_word_data], [trigram_mod[bigram_mod[doc]] for doc in no_stop_word_data])\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwV1Lw6ZH8ed"
   },
   "source": [
    "## Build LDA model\n",
    "\n",
    "Using the gensim library, generate Latentent Dirichlet Allocation model to extract topics from the MTURK web scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5PDTPPPH_D6"
   },
   "outputs": [],
   "source": [
    "def generate_lda(corpus, id2word, num_topics):\n",
    "  lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                        id2word=id2word,\n",
    "                                        num_topics=20, \n",
    "                                        random_state=100,\n",
    "                                        chunksize=100,\n",
    "                                        passes=13,\n",
    "                                        per_word_topics=True)\n",
    "  return lda_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJqUvbrZK2oi"
   },
   "source": [
    "## View topics from LDA models\n",
    "\n",
    "This snippet will print off the top 20 topics and their 15 most prevalent keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXW58p54K6KP"
   },
   "outputs": [],
   "source": [
    "def print_lda(lda):\n",
    "  pprint(lda.print_topics(num_topics=20, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-r2PP9DEeZuK"
   },
   "source": [
    "## Most frequently discussed topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciZthEIwefDl"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "topics_per_documents\n",
    "\n",
    "@param model: the LDA model\n",
    "@param corpus: the corpus the LDA is performing the distribution on\n",
    "@param start: specifies the starting distribution bound on the corpus\n",
    "@param end: specifies the ending distribution bound on the corpus\n",
    "\n",
    "returns a tuple with the dominant topics and the topic percentages in the documents\n",
    "'''\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "\n",
    "def generate_topic_distribution_vis(model, corpus):\n",
    "  print('INFO: generating the topic distribution dataframes and visualizations')\n",
    "\n",
    "  t0 = time()\n",
    "\n",
    "  dominant_topics, topic_percentages = topics_per_document(model=model, corpus=corpus, end=-1)            \n",
    "\n",
    "  print('INFO: done calculating dominant topics and topic percentages for LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "  # Distribution of Dominant Topics in Each Document\n",
    "  t0 = time()\n",
    "\n",
    "  df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "  dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "  df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "  print('INFO: done calculating topic distribution of LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "  # Total Topic Distribution by actual weight\n",
    "  t0 = time()\n",
    "\n",
    "  topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "  df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "  print('INFO: done calulating topic weightage of LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "  # Top 3 Keywords for each Topic\n",
    "  t0 = time()\n",
    "\n",
    "  topic_top3words = [(i, topic) for i, topics in model.show_topics(num_topics=20, num_words=10, formatted=False) \n",
    "                                  for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "\n",
    "  df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "  df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "  df_top3words.reset_index(level=0,inplace=True)\n",
    "\n",
    "  print('INFO: done finding top 3 words of topics for LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "  # Plot\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 4), dpi=120, sharey=True)\n",
    "\n",
    "  # Topic Distribution by Dominant Topics\n",
    "  ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "  ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "  print(df_dominant_topic_in_each_doc.shape)\n",
    "  print(df_top3words.shape)\n",
    "  print(df_top3words.loc[df_top3words.topic_id==16, 'words'].values[0])\n",
    "  # + df_top3words.loc[df_top3words.topic_id==x, 'words']\n",
    "  tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "  ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "  ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "  ax1.set_ylabel('Number of Documents')\n",
    "  ax1.set_ylim(0, 30000)\n",
    "  ax1.tick_params(direction='out', length=3, width=1,\n",
    "                grid_color='r', grid_alpha=0.5, labelsize=6)\n",
    "  #ax1.xaxis.\n",
    "  # fig.update_xaxes(tickangle=45, tickfont=dict(family='Rockwell', color='crimson', size=14))\n",
    "\n",
    "\n",
    "  # Topic Distribution by Topic Weights\n",
    "  ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "  ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "  ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "  ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "  #ax2.update_xaxes(tickangle=45, tickfont=dict(family='Rockwell', color='crimson', size=8))\n",
    "  ax2.tick_params(direction='out', length=3, width=1,\n",
    "                grid_color='r', grid_alpha=0.5, labelsize=6)\n",
    "  # fig.update_xaxes(fontdict=dict(size=10))\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfMIZMd3OuXB"
   },
   "source": [
    "## Calculate Topic Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Twb2FF_XOx7U"
   },
   "outputs": [],
   "source": [
    "# calculate the coherence score\n",
    "def calculate_coherence_score(topics, texts, id2word, coherence):\n",
    "  coherence_lda = CoherenceModel(topics=topics, texts=texts, dictionary=id2word, coherence=coherence)\n",
    "  coherence_lda_score = coherence_lda.get_coherence()\n",
    "  return coherence_lda_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDxxLbs7hdQY"
   },
   "source": [
    "## Evaluate Model over number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3EgdEophhvC"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(id2word, corpus, texts, start, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    model_coherence = []\n",
    "    c_v = []\n",
    "    print('INFO: starting to generate LDAs and calculate coherence')\n",
    "    for num_topics in range(start, limit):\n",
    "      t0 = time()\n",
    "      lm = generate_lda(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "      print('INFO: done generating LDA on ', num_topics, ' topics in %0.3fs.' % (time() - t0))\n",
    "\n",
    "      model_topics = lm.show_topics(formatted=False)\n",
    "      model_topics = [[word for word, prob in topic] for topicid, topic in model_topics]\n",
    "      cs = calculate_coherence_score(topics=model_topics, texts=texts, id2word=id2word, coherence='c_v')\n",
    "      model_coherence.append((lm,cs))\n",
    "      c_v.append(cs)\n",
    "\n",
    "    print('INFO: finished generating models, creating evalution visualization')  \n",
    "    # Show graph\n",
    "    x = range(start, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VcCnSurk_7Jf"
   },
   "source": [
    "## Description Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "XntLKQLzBavI",
    "outputId": "2485dec8-a85f-4fbb-e199-3094a101b0c6"
   },
   "outputs": [],
   "source": [
    "# strip descriptions into tokens\n",
    "t0 = time()\n",
    "\n",
    "# grabs the descriptions from the hits dataframe and puts them in a list\n",
    "description_data = hits.processed_description.values.tolist()\n",
    "\n",
    "# breaks the descriptions into individual word tokens\n",
    "description_tokens = list(sent_to_words(description_data))\n",
    "\n",
    "print('INFO: done tokenizing descriptions in %0.3fs.' % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# generates a tuple with the description converted into bigrams and trigrams\n",
    "description_ngrams = get_bigram_trigram_tuple(description_tokens)\n",
    "\n",
    "print('INFO: done generating description n-grams in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# extracts the description bigram list\n",
    "description_bigrams = description_ngrams[0]\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# generate id2word dictionary of bigrams\n",
    "description_bi_id2word = corpora.Dictionary(description_bigrams)\n",
    "\n",
    "# generate corpus from bigrams\n",
    "description_bi_corpus = [description_bi_id2word.doc2bow(tok) for tok in description_bigrams]\n",
    "\n",
    "print('INFO: done generating description corpus in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRGgXJedEnE9"
   },
   "outputs": [],
   "source": [
    "# generate description bigram LDA\n",
    "t0 = time()\n",
    "\n",
    "description_bi_lda = generate_lda(description_bi_corpus, description_bi_id2word, 20)\n",
    "\n",
    "print('INFO: done generating description bigram LDA in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7osKlI_9EqtO",
    "outputId": "f6969372-8e68-4ba0-ca64-81ce77c21401"
   },
   "outputs": [],
   "source": [
    "# print description bigram lda topics\n",
    "t0 = time()\n",
    "\n",
    "print_lda(description_bi_lda)\n",
    "\n",
    "print('INFO: done printing description LDA topics in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "colab_type": "code",
    "id": "T6R71y_fHzjX",
    "outputId": "bf768cf4-094c-4627-daf9-f8eea46cb4c8"
   },
   "outputs": [],
   "source": [
    "# Create visualization for description document distribution\n",
    "t0 = time()\n",
    "\n",
    "generate_topic_distribution_vis(description_bi_lda, description_bi_corpus)\n",
    "\n",
    "print('INFO: done generating description bigram LDA document-topic distribution visualization in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NK4t29TFLyPK",
    "outputId": "5a291a6b-5f70-4483-fc9d-bf7897693896"
   },
   "outputs": [],
   "source": [
    "# groom the topics from the description bigram lda for topic coherence measure\n",
    "description_bi_lda_topics = description_bi_lda.show_topics(formatted=False)\n",
    "description_bi_lda_topics = [[word for word, prob in topic] for topicid, topic in description_bi_lda_topics]\n",
    "\n",
    "# calculate description bigram topic coherence\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "# calculates the first 10 keywords in a topic\n",
    "print('Description LDA w/ bigrams UMass Topic Coherence Score: ', calculate_coherence_score(description_bi_lda_topics[:10], description_bigrams, description_bi_id2word, 'u_mass'))\n",
    "\n",
    "print('INFO: done printing description bigram UMass topic coherence score in %0.3fs.' % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# calculates the first 10 keywords in a topic\n",
    "print('Description LDA w/ bigrams UCI Topic Coherence Score: ', calculate_coherence_score(description_bi_lda_topics[:10], description_bigrams, description_bi_id2word, 'c_uci'))\n",
    "\n",
    "print('INFO: done printing description bigram UCI topic coherence score in %0.3fs.' % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "id": "Jtg0hk48jv0G",
    "outputId": "4d1f3b99-e647-4852-b41a-f41bb59c891b"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "models = evaluate_model(id2word=description_bi_id2word, corpus=description_bi_corpus, texts=description_bigrams, start=7, limit=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NsjTmFy84ZV2",
    "outputId": "ef409ac7-93e2-4402-864a-583694d7c673"
   },
   "outputs": [],
   "source": [
    "print(len(models))\n",
    "for i in range(25-7):\n",
    "  print(\"Number of topics\", i + 7, models[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W8A4i-aC5OTI",
    "outputId": "d4209893-7c98-4706-da1f-8cc5c8a1744a"
   },
   "outputs": [],
   "source": [
    "print(\"LDA description bigram performed best at 9 topics\", models[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S38cEcuMAJDQ"
   },
   "source": [
    "## Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqHEYxN2BepY"
   },
   "outputs": [],
   "source": [
    "#strip titles into tokens\n",
    "t0 = time()\n",
    "\n",
    "title_data = hits.processed_title.values.tolist()\n",
    "title_tokens = list(sent_to_words(title_data))\n",
    "\n",
    "print('INFO: done tokenizing titles in %0.3fs.' % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "title_ngrams = get_bigram_trigram_tuple(title_tokens)\n",
    "\n",
    "print('INFO: done generating title n-grams in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1FKClOyPT9u"
   },
   "source": [
    "# TODO: optimize the hyperparameters and the K value of the LDA\n",
    "\n",
    "Find out what the best hyperparameters are for the LDA and the best number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQmuA8YZRwWm"
   },
   "source": [
    "## Visualize the Topic Model\n",
    "\n",
    "Load the LDA model into `pyLDAvis` to analyze the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-FQL6o2SafS"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# generate pyLDAvis dashboard\n",
    "t0 = time()\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(description_lda, description_corpus, description_dict)\n",
    "\n",
    "print('INFO: done generating description LDA pyLDAvis dashboard in %0.3fs.' % (time() - t0))\n",
    "\n",
    "LDAvis_prepared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SF6GzOyRMcHD"
   },
   "source": [
    "# LDA Analysis\n",
    "Need to clean up the dataset desperately, as you can see the words 'receipt', 'transcribe', and 'receipts' happen almost twice as much as the next highest occuring words which means that we need to reduce the occurence of these HITs in our dataset to prevent the topic from becoming oversaturated.\n",
    "\n",
    "#### Document Duplication\n",
    "> Using synthetic repetition of data, we find that as documents are repeated,\n",
    "topic models begin to devote topics exclusively to\n",
    "the repeated documents. Repeated documents show\n",
    "very low topical entropy and high likelihood. However, text without these repetitions is largely unaffected: repeated text is quickly fit well to one or a few topics, leaving the rest of the model unaffected,\n",
    "except for the implicit loss of modeling power\n",
    "caused by “losing” one or more topics. We find that\n",
    "topic models can accommodate occasional duplicates and fit topics to a repeated string across many documents, but that this is more difficult if the repeated text has similar language to the content of interest. In our experiments, effects of duplication were minimal until duplicate documents became a\n",
    "substantial proportion of the corpus, whether one\n",
    "document repeated over a thousand times or 1% of\n",
    "the corpus repeated four times.\n",
    "\n",
    "*Understanding Text Pre-Processing for Latent Dirichlet Allocation* (Schofield et al. 2017)\n",
    "\n",
    "Basically this will only effect HITs that have content relating to transcription or receipts which may otherwise be matched to another topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['processed_description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Coherence Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(id2word, corpus, texts, start, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    model_coherence = []\n",
    "    c_v = []\n",
    "    print('INFO: starting to generate LDAs and calculate coherence')\n",
    "#     for num_topics in range(start, limit):\n",
    "  t0 = time()\n",
    "#       lm = generate_lda(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "#       print('INFO: done generating LDA on ', num_topics, ' topics in %0.3fs.' % (time() - t0))\n",
    "\n",
    "#       model_topics = lm.show_topics(formatted=False)\n",
    "#       model_topics = [[word for word, prob in topic] for topicid, topic in model_topics]\n",
    "  cs = calculate_coherence_score(topics=model_topics, texts=texts, id2word=id2word, coherence='c_v')\n",
    "  model_coherence.append((lm,cs))\n",
    "  c_v.append(cs)\n",
    "\n",
    "    print('INFO: finished generating models, creating evalution visualization')  \n",
    "    # Show graph\n",
    "    x = range(start, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_coherence\n",
    "\n",
    "\n",
    "'''\n",
    "fn to get topic coherence from a list of topics\n",
    "\n",
    "topics: list of topics\n",
    "texts: the raw text corpus\n",
    "id2word: the gensim corpora dictionary\n",
    "'''\n",
    "def get_coherence(topics, texts, id2word, coherence='umass')\n",
    "t0 = time()\n",
    "#       lm = generate_lda(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "#       print('INFO: done generating LDA on ', num_topics, ' topics in %0.3fs.' % (time() - t0))\n",
    "\n",
    "#       model_topics = lm.show_topics(formatted=False)\n",
    "#       model_topics = [[word for word, prob in topic] for topicid, topic in model_topics]\n",
    "    cs = calculate_coherence_score(topics=model_topics, texts=texts, id2word=id2word, coherence='c_v')\n",
    "    print('INFO: topic coherence on', coherence, 'metric', cs)\n",
    "    return cs\n",
    "\n",
    "id2word = corpora.Dictionary(description_bigrams)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Topic Modeling with gensim",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
