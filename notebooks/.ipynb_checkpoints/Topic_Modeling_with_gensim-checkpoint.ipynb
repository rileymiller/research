{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYXz9byixFTD"
   },
   "source": [
    "# Topic Modeling with gensim\n",
    "\n",
    "Switching over to gensim to leverage gensim and its topic coherence library\n",
    "\n",
    "Guidance from tutorials: https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models#5.-Build-the-Topic-Model\n",
    "### Install libraries and import dataset from Google Drive ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "mvakvsLtxA3P",
    "outputId": "1b8a0b34-add6-459e-e95c-8fa79620ca52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (0.33.6)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (2.11.1)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (5.4.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.7/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (20.3)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (8.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis) (3.1.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.12.21)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.21 in /usr/local/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.21)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.21->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.21->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.6.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (3.8.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: bz2file in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (1.9.66)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2019.11.28)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.66 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.189)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.1.13)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.66->boto3->smart-open>=1.7.0->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.66->boto3->smart-open>=1.7.0->gensim) (0.16)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pyLDAvis in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (2.1.2)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (1.18.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (0.21.1)\n",
      "Requirement already satisfied: future in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (2.11.1)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (0.34.2)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: pytest in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (5.4.1)\n",
      "Requirement already satisfied: funcy in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pytest->pyLDAvis) (20.3)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pytest->pyLDAvis) (8.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.5.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.1.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from python-dateutil>=2->pandas>=0.17.0->pyLDAvis) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from packaging->pytest->pyLDAvis) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis) (2.2.0)\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: bs4 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from bs4) (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/anaconda3/envs/lda2vec/lib/python3.6/site-packages (from beautifulsoup4->bs4) (2.0)\n",
      "INFO: done importing libraries in 0.000s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rileymiller/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install PyDrive\n",
    "!pip3 install pyLDAvis\n",
    "!pip3 install gensim\n",
    "!pip3 install matplotlib\n",
    "!pip3 install nltk\n",
    "!pip install gensim\n",
    "!pip install pyLDAvis\n",
    "!pip install bs4\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from getpass import getpass\n",
    "from html import unescape\n",
    "\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "t0 = time()\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "# # import the hits.json preprocessed file\n",
    "# downloaded = drive.CreateFile({'id':\"1JL7IY3I_HZg112czpiYChlaMr08sOupi\"}) # id for clean_hits_full_week.json\n",
    "# downloaded.GetContentFile('clean_hits_full_week.json') # preprocessed hits for week long data scrape\n",
    "\n",
    "# import the preview.json preprocessed file\n",
    "# downloaded = drive.CreateFile({'id':\"1MPiJyGX5FzFif2MSJqRuQnXJgn8xpFip\"}) # id for clean_preview.json\n",
    "# downloaded.GetContentFile('clean_preview.json') # preprocessed hits for week long data scrape\n",
    "\n",
    "print('INFO: done importing libraries in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIdUwjwwye7B"
   },
   "source": [
    "## Import the file into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5LBNe-Y5yjoa",
    "outputId": "a8f8380a-a310-48d5-a9e0-382804e33fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: done reading in dataset to pandas dataframe in 14.462s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "hits = pd.read_json('datasets/clean_hits_full_week.json', lines=True)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done reading in dataset to pandas dataframe in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: done importing libraries and dataset in 0.000s.\n",
      "INFO: done reading in dataset to pandas dataframe in 355.548s.\n",
      "INFO: done columns from dataframe in 40.626s.\n",
      "INFO: done removing punctuation and numbers from title and description in 30.712s.\n",
      "0          You will be presented an image of a gym cardio...\n",
      "1          Given a sentence and a noun from that sentence...\n",
      "2                               Transcribing data from image\n",
      "3          Extract all the items from the receipt You wil...\n",
      "4          Verify the value of single data point such as ...\n",
      "                                 ...                        \n",
      "1108697                           Transcribe data from image\n",
      "1108698    If this transaction appeared on your bank stat...\n",
      "1108699    Transcribe all of the purchased items and tota...\n",
      "1108700                           Transcribe data from image\n",
      "1108701    If this transaction appeared on your bank stat...\n",
      "Name: processed_description, Length: 1108702, dtype: object\n",
      "INFO: removing stopwords, duplicates, and numbers\n",
      "INFO: processed_description shape before dropping empty descriptions 1108702\n",
      "INFO: processed_title shape before dropping stop words and number 1108702\n",
      "0          presented image gym cardio workout console tas...\n",
      "1          sentence noun sentence write questions answers...\n",
      "2                                    transcribing data image\n",
      "3          extract items receipt bonus item correctly tra...\n",
      "4          verify single data point store payment informa...\n",
      "                                 ...                        \n",
      "1108697                                transcribe data image\n",
      "1108698     transaction appeared bank statement category fit\n",
      "1108699    transcribe purchased items total shopping receipt\n",
      "1108700                                transcribe data image\n",
      "1108701     transaction appeared bank statement category fit\n",
      "Name: processed_description, Length: 1108702, dtype: object\n",
      "                                     processed_description\n",
      "32       spend minute ranking images based drag drop im...\n",
      "301      complete worker awviolzukbnvu hit compensate p...\n",
      "759          give opinion presidential candidates election\n",
      "768                     rate similarity pictures furniture\n",
      "1276                approx minute survey opinions employee\n",
      "...                                                    ...\n",
      "1082062  shown pairs images bunnies inserted locations ...\n",
      "1086531                       figure audio english minutes\n",
      "1095776            answer questions advertising technology\n",
      "1097161  multi choice test measures knowledge security ...\n",
      "1106992  answer survey experiences environments study m...\n",
      "\n",
      "[2744 rows x 1 columns]\n",
      "INFO: processed_description shape after removing stopwords, duplicates, and numbers 2744\n",
      "INFO: processed_title shape after removing stopwords, duplicates, and numbers 3145\n",
      "INFO: finished removing stopwords, duplicates, and numbers in 548.990s\n",
      "INFO: loading descriptions into text file\n",
      "INFO: finished loading descriptions into text file in 0.039s\n",
      "                                     processed_description\n",
      "32       spend minute ranking images based drag drop im...\n",
      "301      complete worker awviolzukbnvu hit compensate p...\n",
      "759          give opinion presidential candidates election\n",
      "768                     rate similarity pictures furniture\n",
      "1276                approx minute survey opinions employee\n",
      "...                                                    ...\n",
      "1082062  shown pairs images bunnies inserted locations ...\n",
      "1086531                       figure audio english minutes\n",
      "1095776            answer questions advertising technology\n",
      "1097161  multi choice test measures knowledge security ...\n",
      "1106992  answer survey experiences environments study m...\n",
      "\n",
      "[2744 rows x 1 columns]\n",
      "INFO: loading titles into text file\n",
      "INFO: finished loading titles into text file in 0.012s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "print('INFO: done importing libraries and dataset in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "hits = pd.read_json('datasets/20200126-20200312-hits.json', lines=True)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done reading in dataset to pandas dataframe in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "hits = hits.drop(columns=['_id', 'hit_set_id', 'requester_id','requester_name', 'assignment_duration_in_seconds', 'creation_time', 'assignable_hits_count', 'latest_expiration_time', 'caller_meets_requirements', 'caller_meets_preview_requirements', 'last_updated_time', 'monetary_reward', 'accept_project_task_url', 'requester_url', 'project_tasks_url', 'project_requirements', 'requesterInfo'], axis=1)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done columns from dataframe in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# removes all punctuation from the description and title if any\n",
    "t0 = time()\n",
    "\n",
    "hits['processed_description'] = hits['description'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "hits['processed_title'] = hits['title'].map(lambda t : re.sub('[,.$()@#%&~!?]', ' ', t))\n",
    "\n",
    "\n",
    "hits['processed_description'] = hits['processed_description'].str.replace('\\W', ' ')\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda d : re.sub('\\d', '', d))\n",
    "hits['processed_description'] = hits['processed_description'].str.replace('\\s+', ' ')\n",
    "\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda d : re.sub('\\d', '', d))\n",
    "hits['processed_title'] = hits['processed_title'].str.replace('\\W', ' ')\n",
    "hits['processed_title'] = hits['processed_title'].str.replace('\\s+', ' ')\n",
    "\n",
    "\n",
    "\n",
    "print('INFO: done removing punctuation and numbers from title and description in %0.3fs.' % (time() - t0))\n",
    "\n",
    "print(hits['processed_description'])\n",
    "# cleans dataframe by converting all characters to lowercase and removing non-english characters\n",
    "# t0 = time()\n",
    "\n",
    "def clean_dat(chunk):\n",
    "    # Read stopwords\n",
    "    with open('datasets/stops.txt', 'r') as f:\n",
    "        stops = f.read().split('\\n')\n",
    "\n",
    "    return ' '.join([ w for w in chunk.split() if w not in set(stops)])\n",
    "\n",
    "# converts to low caps\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda x: x.lower())\n",
    "\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda x: x.lower())\n",
    "\n",
    "print('INFO: removing stopwords, duplicates, and numbers')\n",
    "\n",
    "print('INFO: processed_description shape before dropping empty descriptions',hits['processed_description'].shape[0])\n",
    "print('INFO: processed_title shape before dropping stop words and number', hits['processed_title'].shape[0])\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# removes non allowable characters\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda x: clean_dat(x))\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda x: clean_dat(x))\n",
    "\n",
    "print(hits['processed_description'])\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "hits.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "hits.dropna(subset = ['processed_description', 'processed_title'], inplace=True)\n",
    "\n",
    "\n",
    "hits['processed_description'].drop_duplicates(keep=False, inplace=True)\n",
    "hits['processed_title'].drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "\n",
    "print('INFO: processed_description shape after removing stopwords, duplicates, and numbers', hits['processed_description'].shape[0])\n",
    "\n",
    "print('INFO: processed_title shape after removing stopwords, duplicates, and numbers', hits['processed_title'].shape[0])\n",
    "\n",
    "print('INFO: finished removing stopwords, duplicates, and numbers in %0.3fs' % (time() - t0))\n",
    "\n",
    "# print out the first couple processed descriptions\n",
    "t0 = time()\n",
    "print('INFO: loading descriptions into text file')\n",
    "\n",
    "hits['processed_description'].to_csv(r'datasets/parsed_full_descriptions.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "# print out the first couple processed titles\n",
    "t0 = time()\n",
    "print('INFO: loading titles into text file')\n",
    "\n",
    "hits['processed_title'].to_csv(r'datasets/parsed_full_titles.txt', header=None, index=None, sep=' ', mode='a')\n",
    "print('INFO: finished loading titles into text file in %0.3fs' % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: done parsing preview dataset, finished in 84.957s.\n",
      "INFO: total previews:  24  Previews with page src:  16  Preview w/o page src:  8\n",
      "INFO: processed_preview shape before removing stop words and dropping empty previews 16\n",
      "INFO: processed_preview shape after removing stop words and dropping empty previews 10\n",
      "INFO: loading previews into text file\n",
      "INFO: finished loading previews into text file in 0.002s\n",
      "                                   processed_previews\n",
      "2   item - electronics - batch id preview - amazon...\n",
      "3   aidea - mturk review summary text text my dear...\n",
      "5   hit extract purchased items shopping receipt h...\n",
      "7   complete survey answer survey minutes study in...\n",
      "8   copytext type text carefully warning hit won't...\n",
      "9   worker - consensyou enable javascript run apps...\n",
      "10  extract data shopping receipt images part shop...\n",
      "12  record id: name: listen provided audio fill na...\n",
      "14  amazon mechanical turk skip main content worke...\n",
      "15  job spotter moderationyou accept hit submit re...\n"
     ]
    }
   ],
   "source": [
    "preview_src_cnt = 0\n",
    "preview_no_src_cnt = 0\n",
    "previews = []\n",
    "\n",
    "def clean_dat(chunk):\n",
    "    # Read stopwords\n",
    "    with open('datasets/stops.txt', 'r') as f:\n",
    "        stops = f.read().split('\\n')\n",
    "\n",
    "    return ' '.join([ w for w in chunk.split() if w not in set(stops) and not w.isnumeric()])\n",
    "\n",
    "for line in open('datasets/20200126-20200312-preview.json'):\n",
    "      preview = json.loads(line)\n",
    "        \n",
    "      if 'page_src' in preview:\n",
    "        preview_src_cnt += 1\n",
    "        page_src = preview['page_src']\n",
    "        soup = BeautifulSoup(page_src, 'html.parser')\n",
    "        # print(soup.find_all('script'))\n",
    "\n",
    "        # Clear every script tag\n",
    "        for tag in soup.find_all('script'):\n",
    "            tag.clear()\n",
    "\n",
    "        # Clear every style tag\n",
    "        for tag in soup.find_all('style'):\n",
    "            tag.clear()\n",
    "\n",
    "        # print(soup.get_text())\n",
    "        clean_src = re.sub(\"<.*?>\", \"\", soup.get_text())\n",
    "        clean_src = re.sub(\"\\n\", \" \", clean_src)\n",
    "        previews.append(clean_src)\n",
    "      else:\n",
    "        preview_no_src_cnt += 1\n",
    "\n",
    "print('INFO: done parsing preview dataset, finished in %0.3fs.' % (time() - t0))\n",
    "print('INFO: total previews: ', (preview_src_cnt + preview_no_src_cnt), ' Previews with page src: ', preview_src_cnt, ' Preview w/o page src: ', preview_no_src_cnt)\n",
    "\n",
    "\n",
    "preview_df = pd.DataFrame(previews, columns=['processed_previews'])\n",
    "preview_df.head()\n",
    "\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : re.sub('[,.$()@#%&~!?]', '', d))\n",
    "\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : d.lower())\n",
    "\n",
    "print('INFO: processed_preview shape before removing stop words and dropping empty previews', preview_df['processed_previews'].shape[0])\n",
    "\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : clean_dat(d))\n",
    "\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : re.sub('\"', '', d))\n",
    "preview_df['processed_previews'] = preview_df['processed_previews'].map(lambda d : re.sub(\"''\", '', d))\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "preview_df.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "preview_df.dropna(subset = ['processed_previews'], inplace=True)\n",
    "\n",
    "\n",
    "preview_df['processed_previews'].drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "\n",
    "print('INFO: processed_preview shape after removing stop words and dropping empty previews', preview_df['processed_previews'].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print out the first couple processed descriptions\n",
    "t0 = time()\n",
    "\n",
    "print('INFO: loading previews into text file')\n",
    "new_preview = pd.DataFrame(preview_df['processed_previews'])\n",
    "new_preview.to_csv(r'datasets/parsed_full_previews.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "print('INFO: finished loading previews into text file in %0.3fs' % (time() - t0))\n",
    "\n",
    "print(new_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTipZ2EUz3ls"
   },
   "source": [
    "## Clean Previewss script\n",
    "\n",
    "Will then proceed to drop all of the unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "EqvLU1430AdL",
    "outputId": "b911c595-33af-46aa-c6db-4057eb07724f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b029dbb1a974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hit_set_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requester_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'requester_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'assignment_duration_in_seconds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'creation_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'assignable_hits_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latest_expiration_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caller_meets_requirements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caller_meets_preview_requirements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_updated_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'monetary_reward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accept_project_task_url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requester_url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'project_tasks_url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'project_requirements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requesterInfo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hits' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "hits = hits.drop(columns=['_id', 'hit_set_id', 'requester_id','requester_name', 'assignment_duration_in_seconds', 'creation_time', 'assignable_hits_count', 'latest_expiration_time', 'caller_meets_requirements', 'caller_meets_preview_requirements', 'last_updated_time', 'monetary_reward', 'accept_project_task_url', 'requester_url', 'project_tasks_url', 'project_requirements', 'requesterInfo'], axis=1)\n",
    "hits.head()\n",
    "\n",
    "print('INFO: done columns from dataframe in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# removes all punctuation from the description and title if any\n",
    "t0 = time()\n",
    "\n",
    "hits['processed_description'] = hits['description'].map(lambda d : re.sub('[,.!?]', '', d))\n",
    "hits['processed_title'] = hits['title'].map(lambda t : re.sub('[,.!?]', '', t))\n",
    "\n",
    "print('INFO: done removing punctuation from title and description in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "# converts the text to lowercase\n",
    "t0 = time()\n",
    "\n",
    "hits['processed_description'] = hits['processed_description'].map(lambda x: x.lower())\n",
    "hits['processed_title'] = hits['processed_title'].map(lambda x: x.lower())\n",
    "\n",
    "print('INFO: done converting text to lowercase in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "# print out the first couple processed descriptions\n",
    "hits['processed_description'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "liy6B4E55Lgk",
    "outputId": "6b287ed2-9da6-4ede-d8cc-36c56f93eb13"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ef35e866a94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print out the first couple processed titles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hits' is not defined"
     ]
    }
   ],
   "source": [
    "# print out the first couple processed titles\n",
    "hits['processed_title'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEwVaimm5fZ_"
   },
   "source": [
    "## Tokenize documents\n",
    "\n",
    "Using the gensim library tokenize the processed titles and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4yJQ0aw5i9T"
   },
   "outputs": [],
   "source": [
    "# function to tokenize the unstructured text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "'''\n",
    "clean_stop_words\n",
    "\n",
    "@param docs: a list of unstructured documents\n",
    "\n",
    "Removes stop words from the documents in the list using the gensim simple_preprocess\n",
    "'''\n",
    "def clean_stop_words(docs):\n",
    "  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in docs]\n",
    "\n",
    "\n",
    "'''\n",
    "  get_bigram_trigram_tuple\n",
    "  \n",
    "  @param data_set: a list of tokens\n",
    "\n",
    "  converts the dataset into a tuple of bigrams and trigrams\n",
    "  using the gensim Phrases and Phraser models\n",
    "'''\n",
    "def get_bigram_trigram_tuple(data_set):\n",
    "  # https://radimrehurek.com/gensim/models/phrases.html\n",
    "  no_stop_word_data = clean_stop_words(data_set)\n",
    "\n",
    "  bigram_base = gensim.models.Phrases(data_set, min_count=5, threshold=100)\n",
    "  trigram_base = gensim.models.Phrases(bigram_base[data_set], threshold=100)\n",
    "\n",
    "  bigram_mod = gensim.models.phrases.Phraser(bigram_base)\n",
    "  trigram_mod = gensim.models.phrases.Phraser(trigram_base)\n",
    "\n",
    "  return([bigram_mod[doc] for doc in no_stop_word_data], [trigram_mod[bigram_mod[doc]] for doc in no_stop_word_data])\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwV1Lw6ZH8ed"
   },
   "source": [
    "## Build LDA model\n",
    "\n",
    "Using the gensim library, generate Latentent Dirichlet Allocation model to extract topics from the MTURK web scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5PDTPPPH_D6"
   },
   "outputs": [],
   "source": [
    "def generate_lda(corpus, id2word, num_topics):\n",
    "  lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                        id2word=id2word,\n",
    "                                        num_topics=20, \n",
    "                                        random_state=100,\n",
    "                                        chunksize=100,\n",
    "                                        passes=13,\n",
    "                                        per_word_topics=True)\n",
    "  return lda_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJqUvbrZK2oi"
   },
   "source": [
    "## View topics from LDA models\n",
    "\n",
    "This snippet will print off the top 20 topics and their 15 most prevalent keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXW58p54K6KP"
   },
   "outputs": [],
   "source": [
    "def print_lda(lda):\n",
    "  pprint(lda.print_topics(num_topics=20, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-r2PP9DEeZuK"
   },
   "source": [
    "## Most frequently discussed topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciZthEIwefDl"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "topics_per_documents\n",
    "\n",
    "@param model: the LDA model\n",
    "@param corpus: the corpus the LDA is performing the distribution on\n",
    "@param start: specifies the starting distribution bound on the corpus\n",
    "@param end: specifies the ending distribution bound on the corpus\n",
    "\n",
    "returns a tuple with the dominant topics and the topic percentages in the documents\n",
    "'''\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "\n",
    "def generate_topic_distribution_vis(model, corpus):\n",
    "  print('INFO: generating the topic distribution dataframes and visualizations')\n",
    "\n",
    "  t0 = time()\n",
    "\n",
    "  dominant_topics, topic_percentages = topics_per_document(model=model, corpus=corpus, end=-1)            \n",
    "\n",
    "  print('INFO: done calculating dominant topics and topic percentages for LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "  # Distribution of Dominant Topics in Each Document\n",
    "  t0 = time()\n",
    "\n",
    "  df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "  dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "  df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "  print('INFO: done calculating topic distribution of LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "  # Total Topic Distribution by actual weight\n",
    "  t0 = time()\n",
    "\n",
    "  topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "  df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "  print('INFO: done calulating topic weightage of LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "  # Top 3 Keywords for each Topic\n",
    "  t0 = time()\n",
    "\n",
    "  topic_top3words = [(i, topic) for i, topics in model.show_topics(num_topics=20, num_words=10, formatted=False) \n",
    "                                  for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "\n",
    "  df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "  df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "  df_top3words.reset_index(level=0,inplace=True)\n",
    "\n",
    "  print('INFO: done finding top 3 words of topics for LDA in %0.3fs.' % (time() - t0))\n",
    "\n",
    "  # Plot\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 4), dpi=120, sharey=True)\n",
    "\n",
    "  # Topic Distribution by Dominant Topics\n",
    "  ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "  ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "  print(df_dominant_topic_in_each_doc.shape)\n",
    "  print(df_top3words.shape)\n",
    "  print(df_top3words.loc[df_top3words.topic_id==16, 'words'].values[0])\n",
    "  # + df_top3words.loc[df_top3words.topic_id==x, 'words']\n",
    "  tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "  ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "  ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "  ax1.set_ylabel('Number of Documents')\n",
    "  ax1.set_ylim(0, 30000)\n",
    "  ax1.tick_params(direction='out', length=3, width=1,\n",
    "                grid_color='r', grid_alpha=0.5, labelsize=6)\n",
    "  #ax1.xaxis.\n",
    "  # fig.update_xaxes(tickangle=45, tickfont=dict(family='Rockwell', color='crimson', size=14))\n",
    "\n",
    "\n",
    "  # Topic Distribution by Topic Weights\n",
    "  ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "  ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "  ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "  ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "  #ax2.update_xaxes(tickangle=45, tickfont=dict(family='Rockwell', color='crimson', size=8))\n",
    "  ax2.tick_params(direction='out', length=3, width=1,\n",
    "                grid_color='r', grid_alpha=0.5, labelsize=6)\n",
    "  # fig.update_xaxes(fontdict=dict(size=10))\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfMIZMd3OuXB"
   },
   "source": [
    "## Calculate Topic Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Twb2FF_XOx7U"
   },
   "outputs": [],
   "source": [
    "# calculate the coherence score\n",
    "def calculate_coherence_score(topics, texts, id2word, coherence):\n",
    "  coherence_lda = CoherenceModel(topics=topics, texts=texts, dictionary=id2word, coherence=coherence)\n",
    "  coherence_lda_score = coherence_lda.get_coherence()\n",
    "  return coherence_lda_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDxxLbs7hdQY"
   },
   "source": [
    "## Evaluate Model over number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3EgdEophhvC"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(id2word, corpus, texts, start, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    model_coherence = []\n",
    "    c_v = []\n",
    "    print('INFO: starting to generate LDAs and calculate coherence')\n",
    "    for num_topics in range(start, limit):\n",
    "      t0 = time()\n",
    "      lm = generate_lda(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "      print('INFO: done generating LDA on ', num_topics, ' topics in %0.3fs.' % (time() - t0))\n",
    "\n",
    "      model_topics = lm.show_topics(formatted=False)\n",
    "      model_topics = [[word for word, prob in topic] for topicid, topic in model_topics]\n",
    "      cs = calculate_coherence_score(topics=model_topics, texts=texts, id2word=id2word, coherence='c_v')\n",
    "      model_coherence.append((lm,cs))\n",
    "      c_v.append(cs)\n",
    "\n",
    "    print('INFO: finished generating models, creating evalution visualization')  \n",
    "    # Show graph\n",
    "    x = range(start, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VcCnSurk_7Jf"
   },
   "source": [
    "## Description Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "XntLKQLzBavI",
    "outputId": "2485dec8-a85f-4fbb-e199-3094a101b0c6"
   },
   "outputs": [],
   "source": [
    "# strip descriptions into tokens\n",
    "t0 = time()\n",
    "\n",
    "# grabs the descriptions from the hits dataframe and puts them in a list\n",
    "description_data = hits.processed_description.values.tolist()\n",
    "\n",
    "# breaks the descriptions into individual word tokens\n",
    "description_tokens = list(sent_to_words(description_data))\n",
    "\n",
    "print('INFO: done tokenizing descriptions in %0.3fs.' % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# generates a tuple with the description converted into bigrams and trigrams\n",
    "description_ngrams = get_bigram_trigram_tuple(description_tokens)\n",
    "\n",
    "print('INFO: done generating description n-grams in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# extracts the description bigram list\n",
    "description_bigrams = description_ngrams[0]\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# generate id2word dictionary of bigrams\n",
    "description_bi_id2word = corpora.Dictionary(description_bigrams)\n",
    "\n",
    "# generate corpus from bigrams\n",
    "description_bi_corpus = [description_bi_id2word.doc2bow(tok) for tok in description_bigrams]\n",
    "\n",
    "print('INFO: done generating description corpus in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRGgXJedEnE9"
   },
   "outputs": [],
   "source": [
    "# generate description bigram LDA\n",
    "t0 = time()\n",
    "\n",
    "description_bi_lda = generate_lda(description_bi_corpus, description_bi_id2word, 20)\n",
    "\n",
    "print('INFO: done generating description bigram LDA in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7osKlI_9EqtO",
    "outputId": "f6969372-8e68-4ba0-ca64-81ce77c21401"
   },
   "outputs": [],
   "source": [
    "# print description bigram lda topics\n",
    "t0 = time()\n",
    "\n",
    "print_lda(description_bi_lda)\n",
    "\n",
    "print('INFO: done printing description LDA topics in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "colab_type": "code",
    "id": "T6R71y_fHzjX",
    "outputId": "bf768cf4-094c-4627-daf9-f8eea46cb4c8"
   },
   "outputs": [],
   "source": [
    "# Create visualization for description document distribution\n",
    "t0 = time()\n",
    "\n",
    "generate_topic_distribution_vis(description_bi_lda, description_bi_corpus)\n",
    "\n",
    "print('INFO: done generating description bigram LDA document-topic distribution visualization in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NK4t29TFLyPK",
    "outputId": "5a291a6b-5f70-4483-fc9d-bf7897693896"
   },
   "outputs": [],
   "source": [
    "# groom the topics from the description bigram lda for topic coherence measure\n",
    "description_bi_lda_topics = description_bi_lda.show_topics(formatted=False)\n",
    "description_bi_lda_topics = [[word for word, prob in topic] for topicid, topic in description_bi_lda_topics]\n",
    "\n",
    "# calculate description bigram topic coherence\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "# calculates the first 10 keywords in a topic\n",
    "print('Description LDA w/ bigrams UMass Topic Coherence Score: ', calculate_coherence_score(description_bi_lda_topics[:10], description_bigrams, description_bi_id2word, 'u_mass'))\n",
    "\n",
    "print('INFO: done printing description bigram UMass topic coherence score in %0.3fs.' % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# calculates the first 10 keywords in a topic\n",
    "print('Description LDA w/ bigrams UCI Topic Coherence Score: ', calculate_coherence_score(description_bi_lda_topics[:10], description_bigrams, description_bi_id2word, 'c_uci'))\n",
    "\n",
    "print('INFO: done printing description bigram UCI topic coherence score in %0.3fs.' % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "id": "Jtg0hk48jv0G",
    "outputId": "4d1f3b99-e647-4852-b41a-f41bb59c891b"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "models = evaluate_model(id2word=description_bi_id2word, corpus=description_bi_corpus, texts=description_bigrams, start=7, limit=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NsjTmFy84ZV2",
    "outputId": "ef409ac7-93e2-4402-864a-583694d7c673"
   },
   "outputs": [],
   "source": [
    "print(len(models))\n",
    "for i in range(25-7):\n",
    "  print(\"Number of topics\", i + 7, models[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W8A4i-aC5OTI",
    "outputId": "d4209893-7c98-4706-da1f-8cc5c8a1744a"
   },
   "outputs": [],
   "source": [
    "print(\"LDA description bigram performed best at 9 topics\", models[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S38cEcuMAJDQ"
   },
   "source": [
    "## Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqHEYxN2BepY"
   },
   "outputs": [],
   "source": [
    "#strip titles into tokens\n",
    "t0 = time()\n",
    "\n",
    "title_data = hits.processed_title.values.tolist()\n",
    "title_tokens = list(sent_to_words(title_data))\n",
    "\n",
    "print('INFO: done tokenizing titles in %0.3fs.' % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "title_ngrams = get_bigram_trigram_tuple(title_tokens)\n",
    "\n",
    "print('INFO: done generating title n-grams in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1FKClOyPT9u"
   },
   "source": [
    "# TODO: optimize the hyperparameters and the K value of the LDA\n",
    "\n",
    "Find out what the best hyperparameters are for the LDA and the best number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQmuA8YZRwWm"
   },
   "source": [
    "## Visualize the Topic Model\n",
    "\n",
    "Load the LDA model into `pyLDAvis` to analyze the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-FQL6o2SafS"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# generate pyLDAvis dashboard\n",
    "t0 = time()\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(description_lda, description_corpus, description_dict)\n",
    "\n",
    "print('INFO: done generating description LDA pyLDAvis dashboard in %0.3fs.' % (time() - t0))\n",
    "\n",
    "LDAvis_prepared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SF6GzOyRMcHD"
   },
   "source": [
    "# LDA Analysis\n",
    "Need to clean up the dataset desperately, as you can see the words 'receipt', 'transcribe', and 'receipts' happen almost twice as much as the next highest occuring words which means that we need to reduce the occurence of these HITs in our dataset to prevent the topic from becoming oversaturated.\n",
    "\n",
    "#### Document Duplication\n",
    "> Using synthetic repetition of data, we find that as documents are repeated,\n",
    "topic models begin to devote topics exclusively to\n",
    "the repeated documents. Repeated documents show\n",
    "very low topical entropy and high likelihood. However, text without these repetitions is largely unaffected: repeated text is quickly fit well to one or a few topics, leaving the rest of the model unaffected,\n",
    "except for the implicit loss of modeling power\n",
    "caused by â€œlosingâ€ one or more topics. We find that\n",
    "topic models can accommodate occasional duplicates and fit topics to a repeated string across many documents, but that this is more difficult if the repeated text has similar language to the content of interest. In our experiments, effects of duplication were minimal until duplicate documents became a\n",
    "substantial proportion of the corpus, whether one\n",
    "document repeated over a thousand times or 1% of\n",
    "the corpus repeated four times.\n",
    "\n",
    "*Understanding Text Pre-Processing for Latent Dirichlet Allocation* (Schofield et al. 2017)\n",
    "\n",
    "Basically this will only effect HITs that have content relating to transcription or receipts which may otherwise be matched to another topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUx2EwqmL2OQ"
   },
   "source": [
    "# lda2vec\n",
    "\n",
    "Will be implementing `lda2vec` based on (Moody 2016).\n",
    "\n",
    "#### TensorFlow implementation of `lda2vec`\n",
    "https://github.com/nateraw/Lda2vec-Tensorflow\n",
    "\n",
    "#### Repo for GloVe encodings\n",
    "https://github.com/stanfordnlp/GloVe/blob/master/README.md\n",
    "\n",
    "#### Pip package\n",
    "https://pypi.org/project/lda2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['processed_description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove Embeddings\n",
    "**repo:** https://github.com/stanfordnlp/GloVe\n",
    "from Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: beginning preprocesssing tokens from descriptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4378it [00:00, 14217.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- Tokenizing Texts ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231021it [00:06, 36722.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 3210 low frequency tokens out of 4448 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:00, 394.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- Getting Skipgrams ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231021it [01:34, 2432.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: finished preprocessing tokens from descriptions in 140.010s.\n",
      "INFO: loading glove embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: finished loading glove embeddings in 151.287s.\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install lda2vec\n",
    "\n",
    "from lda2vec.nlppipe import Preprocessor\n",
    "# Where to save preprocessed data\n",
    "clean_data_dir = \"data/clean_data\"\n",
    "# Name of input file. Should be inside of data_dir\n",
    "# input_file = \"20_newsgroups.txt\"\n",
    "# Should we load pretrained embeddings from file\n",
    "load_embeds = True\n",
    "\n",
    "# Read in data file\n",
    "# df = pd.read_csv(data_dir+\"/\"+input_file, sep=\"\\t\")\n",
    "\n",
    "# Initialize a preprocessor\n",
    "P = Preprocessor(hits, \"processed_description\", max_features=30000, maxlen=10000, min_count=30)\n",
    "\n",
    "# Run the preprocessing on your dataframe\n",
    "t0 = time()\n",
    "    \n",
    "print('INFO: beginning preprocesssing tokens from descriptions')\n",
    "\n",
    "P.preprocess()\n",
    "\n",
    "print('INFO: finished preprocessing tokens from descriptions in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# Load embeddings from file if we choose to do so\n",
    "if load_embeds:\n",
    "    # Load embedding matrix from file path - change path to where you saved them\n",
    "    t0 = time()\n",
    "    \n",
    "    print('INFO: loading glove embeddings')\n",
    "    embedding_matrix = P.load_glove(\"embeddings/glove.42B.300d.txt\")\n",
    "    \n",
    "    print('INFO: finished loading glove embeddings in %0.3fs.' % (time() - t0))\n",
    "else:\n",
    "    embedding_matrix = None\n",
    "\n",
    "# Save data to data_dir\n",
    "P.save_data(clean_data_dir, embedding_matrix=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/rileymiller/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/site-packages (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.27.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/rileymiller/Library/Python/3.7/lib/python/site-packages (from tensorflow==1.15) (1.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.33.6)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (3.11.3)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.18.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (3.2.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15) (41.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "2.1.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow==1.15\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "INFO: loading preprocessed data\n",
      "INFO: finished loading preprocessed data in 9.121s.\n",
      "INFO: initializing lda2vec model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ConfigProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b305d368d39f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mfreqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           save_graph_def=save_graph)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INFO: finished initializing lda2vec model in %0.3fs.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lda2vec/Lda2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_unique_documents, vocab_size, num_topics, freqs, save_graph_def, embedding_size, num_sampled, learning_rate, lmbda, alpha, power, batch_size, logdir, restore, fixed_words, factors_in, pretrained_embeddings)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ConfigProto'"
     ]
    }
   ],
   "source": [
    "from lda2vec import utils, model\n",
    "# from l2v.lda2vec.utils import load_preprocessed_data\n",
    "# from l2v.lda2vec.Lda2vec import Lda2vec as model\n",
    "print(tf.__version__)\n",
    "# Path to preprocessed data\n",
    "clean_data_dir = \"data/clean_data\"\n",
    "# Whether or not to load saved embeddings file\n",
    "load_embeds = True\n",
    "\n",
    "# Load data from files\n",
    "t0 = time()\n",
    "\n",
    "print('INFO: loading preprocessed data')\n",
    "(idx_to_word, word_to_idx, freqs, pivot_ids,\n",
    " target_ids, doc_ids, embed_matrix) = utils.load_preprocessed_data(clean_data_dir, load_embed_matrix=load_embeds)\n",
    "\n",
    "print('INFO: finished loading preprocessed data in %0.3fs.' % (time() - t0))\n",
    "\n",
    "# Number of unique documents\n",
    "num_docs = doc_ids.max() + 1\n",
    "# Number of unique words in vocabulary (int)\n",
    "vocab_size = len(freqs)\n",
    "# Embed layer dimension size\n",
    "# If not loading embeds, change 128 to whatever size you want.\n",
    "embed_size = embed_matrix.shape[1] if load_embeds else 128\n",
    "# Number of topics to cluster into\n",
    "num_topics = 9\n",
    "# Amount of iterations over entire dataset\n",
    "num_epochs = 200\n",
    "# Batch size - Increase/decrease depending on memory usage\n",
    "batch_size = 4096\n",
    "# Epoch that we want to \"switch on\" LDA loss\n",
    "switch_loss_epoch = 0\n",
    "# Pretrained embeddings value\n",
    "pretrained_embeddings = embed_matrix if load_embeds else None\n",
    "# If True, save logdir, otherwise don't\n",
    "save_graph = True\n",
    "\n",
    "# Initialize the model\n",
    "t0 = time()\n",
    "\n",
    "print('INFO: initializing lda2vec model')\n",
    "m = model(num_docs,\n",
    "          vocab_size,\n",
    "          num_topics,\n",
    "          embedding_size=embed_size,\n",
    "          pretrained_embeddings=pretrained_embeddings,\n",
    "          freqs=freqs,\n",
    "          batch_size = batch_size,\n",
    "          save_graph_def=save_graph)\n",
    "\n",
    "print('INFO: finished initializing lda2vec model in %0.3fs.' % (time() - t0))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "t0 = time()\n",
    "\n",
    "print('INFO: training lda2vec')\n",
    "m.train(pivot_ids,\n",
    "        target_ids,\n",
    "        doc_ids,\n",
    "        len(pivot_ids),\n",
    "        num_epochs,\n",
    "        idx_to_word=idx_to_word,\n",
    "        switch_loss_epoch=switch_loss_epoch)\n",
    "\n",
    "print('INFO: finished training lda2vec model in %0.3fs.' % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Topic Modeling with gensim",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
