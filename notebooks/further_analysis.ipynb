{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# hits = pd.read_json('datasets/20200126-20200312-hits.json', lines=True)\n",
    "\n",
    "\n",
    "d_1 = pd.read_json('../mturk/20200126-20200202/hits.json', lines=True)\n",
    "\n",
    "d_2 = pd.read_json('../mturk/20200217-20200227/hits.json', lines=True)\n",
    "\n",
    "d_3 = pd.read_json('../mturk/20200227-20200303/hits.json', lines=True)\n",
    "\n",
    "d_4 = pd.read_json('../mturk/20200303-20200312/hits.json', lines=True)\n",
    "\n",
    "\n",
    "d_1['description'] = d_1['description'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "d_2['description'] = d_2['description'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "d_3['description'] = d_3['description'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "d_4['description'] = d_4['description'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "\n",
    "\n",
    "d_1['title'] = d_1['title'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "d_2['title'] = d_2['title'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "d_3['title'] = d_3['title'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "d_4['title'] = d_4['title'].map(lambda d : re.sub('[,.$()@#%&~!?]', ' ', d))\n",
    "\n",
    "\n",
    "\n",
    "print('INFO: done reading in dataset to pandas dataframes in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-56e7275e9070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(d_1.description.str.lower().str.split(' '))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdesc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdesc_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdesc_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd_1' is not defined"
     ]
    }
   ],
   "source": [
    "# print(d_1.description.value_counts())\n",
    "# d1_desc = d_1.description.str.lower().str.split()\n",
    "# print(d1_desc)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# print(d_1.description.str.lower().str.split())\n",
    "# print(d_1.description.str.lower().str.split(' '))\n",
    "\n",
    "desc_1 = Counter(\" \".join(d_1.description.str.lower()).split(\" \"))\n",
    "desc_2 = Counter(\" \".join(d_2.description.str.lower()).split(\" \"))\n",
    "desc_3 = Counter(\" \".join(d_3.description.str.lower()).split(\" \"))\n",
    "desc_4 = Counter(\" \".join(d_4.description.str.lower()).split(\" \"))\n",
    "\n",
    "tit_1 = Counter(\" \".join(d_1.title.str.lower()).split(\" \"))\n",
    "tit_2 = Counter(\" \".join(d_2.title.str.lower()).split(\" \"))\n",
    "tit_3 = Counter(\" \".join(d_3.title.str.lower()).split(\" \"))\n",
    "tit_4 = Counter(\" \".join(d_4.title.str.lower()).split(\" \"))\n",
    "\n",
    "\n",
    "# print(d_1.description.count())\n",
    "# counts = Counter(d_1)\n",
    "\n",
    "# print(counter_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Model &       Corpus &   K &                                                                        Keywords \\\\\n",
      "\\midrule\n",
      "  LSI &        Title &  15 &        fox, game, watch, superbowl, nfl, calculate, latency, sports, tv, online \\\\\n",
      "  LSI &  Description &  40 &  participants, game, qualify, related, fox, work, compare, rate, superbowl, nfl \\\\\n",
      "  LDA &  Description &  85 &         compare, watch, fox, game, superbowl, cable, satellite, nfl, delay, air \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "   Model &   Corpus &    K &                                                                                                Keywords \\\\\n",
      "\\midrule\n",
      "     lda &  Preview &  100 &         workplace, survey, candidates, study, final, behavior, congressional, actual, emotions, running \\\\\n",
      " lda2vec &  Preview &   95 &                   candidate, bob, candidates, democratic, amy, editorial, cat, becky, klobuchar, zoomed \\\\\n",
      " lda2vec &  Preview &   85 &  screenshots, candidates, klobuchar, amy, sentiment, ethnicity, tagline, democratic, presidential, race \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "   Model &   Corpus &    K &                                                                                           Keywords \\\\\n",
      "\\midrule\n",
      " lda2vec &  Preview &  100 &  fake, exaggeration, prepare, sausage, reject, presidential, holiday, breakfast, insult, violation \\\\\n",
      " lda2vec &  Preview &   55 &           already, stalin, falsehoods, donald, intolerant, more, never, insensitive, death, homage \\\\\n",
      " lda2vec &  Preview &   45 &                            tweets, trk, facebook, photo, insult, trump, cat, joseph, christ, feeds \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "   Model &   Corpus &   K &                                                                            Keywords \\\\\n",
      "\\midrule\n",
      "     LDA &  Preview &  80 &   march, people, tweet, tweets, information, vote, relevant, attended, person, term \\\\\n",
      " lda2vec &  Preview &  80 &  tweets, twitter, keywords, headline, youtube, attended, trends, trend, valid, fake \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Model &       Corpus &   K &                                                                                          Keywords \\\\\n",
      "\\midrule\n",
      "  LDA &  Description &  85 &      instagram, usage, feelings, interview, behaviors, covid, identify, survey, coronavirus, tags \\\\\n",
      "  LDA &  Description &  75 &  workplace, coronavirus, health, choice, personal, write, guaranteed, approval, multiple, opinion \\\\\n",
      "  LDA &        Title &  90 &        research, mental, assessment, personal, health, reviews, study, coronavirus, china, survey \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 200)\n",
    "'''\n",
    "create topic table\n",
    "\n",
    "should read \n",
    "Figure\n",
    "model | K | topics\n",
    "\n",
    "use pandas\n",
    "\n",
    "params:\n",
    "topic_list\n",
    "\n",
    "contains list of lists:\n",
    "\n",
    "['model display name', 'dataset' 'K value', \"keyowr, ads, dasd'\n",
    "\n",
    "'''\n",
    "\n",
    "def create_topic_table(topic_list):\n",
    "        df = pd.DataFrame(data=topic_list, columns=['Model', 'Corpus', 'K', 'Keywords'])\n",
    "#         df = df['Topics'].map(lambda x: re.sub('\\n', '', x))\n",
    "#         print(df)\n",
    "        print(df.to_latex(index=False))\n",
    "\n",
    "# superbowl topics    \n",
    "create_topic_table([\n",
    "    ['LSI', 'Title', '15', \"fox, game, watch, superbowl, nfl, calculate, latency, sports, tv, online\"],\n",
    "    ['LSI', 'Description', '40', \"participants, game, qualify, related, fox, work, compare, rate, superbowl, nfl\"],\n",
    "    ['LDA', 'Description', '85', \"compare, watch, fox, game, superbowl, cable, satellite, nfl, delay, air\"]\n",
    "])\n",
    "\n",
    "print(end='\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "create_topic_table([\n",
    "    ['lda', 'Preview', '100', 'workplace, survey, candidates, study, final, behavior, congressional, actual, emotions, running'],\n",
    "    ['lda2vec', 'Preview', '95', 'candidate, bob, candidates, democratic, amy, editorial, cat, becky, klobuchar, zoomed'],\n",
    "    ['lda2vec', 'Preview', '85', 'screenshots, candidates, klobuchar, amy, sentiment, ethnicity, tagline, democratic, presidential, race'],\n",
    "])\n",
    "\n",
    "print(end='\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "create_topic_table([\n",
    "    ['lda2vec', 'Preview', '100', 'fake, exaggeration, prepare, sausage, reject, presidential, holiday, breakfast, insult, violation'],\n",
    "    ['lda2vec', 'Preview', '55', 'already, stalin, falsehoods, donald, intolerant, more, never, insensitive, death, homage'],\n",
    "    ['lda2vec', 'Preview', '45', 'tweets, trk, facebook, photo, insult, trump, cat, joseph, christ, feeds'],\n",
    "])\n",
    "\n",
    "print(end='\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "\n",
    "create_topic_table([\n",
    "    ['LDA', 'Preview', '80', 'march, people, tweet, tweets, information, vote, relevant, attended, person, term'],\n",
    "    ['lda2vec', 'Preview', '80', 'tweets, twitter, keywords, headline, youtube, attended, trends, trend, valid, fake'],\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "create_topic_table([\n",
    "    ['LDA', 'Description', '85', 'instagram, usage, feelings, interview, behaviors, covid, identify, survey, coronavirus, tags'],\n",
    "    ['LDA', 'Description', '75', 'workplace, coronavirus, health, choice, personal, write, guaranteed, approval, multiple, opinion'],\n",
    "    ['LDA', 'Title', '90', 'research, mental, assessment, personal, health, reviews, study, coronavirus, china, survey'],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  my_column_name_1 my_column_name_2\n",
      "0             foo1             bar1\n",
      "1             foo2             bar2\n"
     ]
    }
   ],
   "source": [
    "my_python_list = [['foo1', 'bar1'],\n",
    "                  ['foo2', 'bar2']]\n",
    "new_df = pd.DataFrame(columns=['my_column_name_1', 'my_column_name_2'], data=my_python_list)\n",
    "\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
