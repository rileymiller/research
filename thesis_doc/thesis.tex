\documentclass[letterpaper,12pt]{article}

% csm-thesis automatically includes the following packages:
%% float
%% setspace
%% geometry
%% graphics
%% textcase
%% subfig

% Note: Two package options exist for your convenience: ``insane'' and ``nolabel''.  To use these options together separate them by a comma, ie. \usepackage[insane,nolabel]{csm-thesis}
% * \usepackage[insane]{csm-thesis}
% Turn off all document sanity checks.  This option can be used to render a ``sub-document'' that is part of the root thesis document.  It is important to note that you should NEVER disable this check on your root thesis document, as important format errors and warnings will be disabled.
% * \usepackage[nolabel]{csm-thesis}
% Disables automatic reference ``labeling'' of figures and tables.  By default the thesis template prepends any reference to a figure or table with ``Figure~'' or ``Table~''.  This option is meant for disabling the labeling behavior when a document already has the appropriate labeling.  It is important to note that if your document DOES NOT have the appropriate labeling (the reference label must EXACTLY MATCH the caption label) then it will not pass the format review.
\usepackage{csm-thesis}

% For inserting large multi-page tables:
\usepackage{array}
\usepackage{longtable}

\usepackage{booktabs}
\usepackage{enumitem}
% For inserting sideways tables and figures
\usepackage{rotating}


\usepackage{mathtools}

% Since hyperref and cite don't completely get along, the template now recommends using natbib:
% For an explanation, see http://www.tex.ac.uk/cgi-bin/texfaq2html?label=citesort
\usepackage[numbers]{natbib}
% If you wish to use ``cite'' instead then your choices are:
% 1) Don't use the hyperref package
% 2) Put ``cite'' before hyperref, resulting in no citation hyperlinks
% 3) Put ``cite'' after hyperref, resulting in ugly looking citations

% If you choose not to use natbib then you can set the standard ``numeric'' style like so:
% \bibliographystyle{unsrt}

% Important Note: math-mode in sections, titles, and other bookmarks will generate warnings with hyperref.  You can work around this by either:
% 1) Not using the hyperref package
% 2) Using \texorpdfstring{TeX Code}{PDF Replacement} to display an alternative bookmark (for example, \texorpdfstring{H$_2$O}{Water}).
% The thesis template will automatically import your document information into hyperref, so if you go to ``File | Properties'' in Adobe Acrobat it will display the title and author.  If you would like to over-ride this option then just change the line below to ``\usepackage[]{hyperref}''.
\usepackage{hyperref}

% For inserting programming code:
\usepackage{listings}

% For inserting landscape-mode objects:
\usepackage{pdflscape} % use ``lscape'' if you are not creating a PDF output

% For matrices:
\usepackage{amsmath}

\usepackage{longtable}


\usepackage{array}
\newcolumntype{L}{>{\arraybackslash}m{5.5in}}

% For using helvetica instead of Computer Modern
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}

%% For automatic equation breaking (experimental!):
%\usepackage{breqn}

% For using row-spanning and column-spanning in tables:
%\usepackage{multirow}

% The thesis title MUST be in an inverted pyramid shape.  To do this you can either specify the returns in the title manually or have the thesis style attempt to automatically build the title in the shape of an inverted pyramid.  The thesis style will automatically choose the appropriate behavior by detecting the presence of returns (\\) in the title.
% Please note that by ``inverted pyramid'' the graduate office really means a regular trapezoid with the larger base on the top.
% <<MANUAL PYRAMID:>>
% Use ``\\" to end a line, all normal LaTeX should function properly.
	%\title{%
	%	Developing a \atom{12}{6}{Th}{2+}{3}esis Template\\%
	%	to Help Students Graduate\\%
	%	in a Reasonable Time%
	%}
% <<AUTOMATIC PYRAMID:>>
% Do not put any carriage returns (\\), all normal LaTeX should function properly.
\title{Topic Modeling on Amazon MTURK}
% Please note: If you are generating a title containing math mode then it is best to use \texorpdfstring to provide an alternative text for the PDF Title.  If you do not do this then you will see a ''Token not allowed in a PDFDocEncoded string`` warning when rendering your document.
% eg. \texorpdfstring{H$_2$O}{Water}
% One final word of caution: The usage of atoms/molecules in titles <may> need to be spelled out on the cover since the binding company cannot typeset them.

\degreetitle{Master of Science}
\discipline{Computer Science}
\department{Computer Science}

\author{Riley Miller}
\advisor{Dr. Chuan Yue}
% Comment out the following line if you do not have a co-advisor:
% \coadvisor{Dr. Secondary B. Advisor}
\dpthead{Dr. Tracy Camp}{Department Head}

\begin{document}
% \nocite{*}
% Parts of a Thesis

%% Parts of a Thesis - Front Matter

\frontmatter

%%% Parts of a Thesis - Front Matter - Title Page (required)

\maketitle
\newpage

%%% Parts of a Thesis - Front Matter - Copyright Page (optional)

% If the copyright for your document spans multiple years, or does not match the current year, then replace ''\the\year`` below with the appropriate text.
\makecopyright{\the\year}
\newpage

%%% Parts of a Thesis - Front Matter - Signature Page (required)

\makesubmittal
\newpage

%%% Parts of a Thesis - Front Matter - Abstract (required)

\begin{abstract}
<Insert abstraction upon completion of the document>
\end{abstract}

\newpage

%%% Parts of a Thesis - Front Matter - Table of Contents (required)

\tableofcontents
\newpage

%%% Parts of a Thesis - Front Matter - List of Figures (if applicable)
%%% Parts of a Thesis - Front Matter - List of Tables (if applicable)

% NOTE: If you have more than 2 items in either list they must be separate.
% This case is generally handled automatically, but if you are told to separate the lists then comment or remove the two lines below:
% \listoffiguresandtables
% \newpages

% ... and then uncomment these four lines to force separate lists:
\listoffigures
\newpage
\listoftables
\newpage


%% NOTE: If included in the front matter, a glossary, a list of abbreviations, or a list of symbols is placed as the last list. If these lists are included in the back matter, they are placed immediately before the REFERENCES CITED.

%%% Parts of a Thesis - Front Matter - Glossary (if applicable)
%\glossary

%%% Parts of a Thesis - Front Matter - List of Symbols (if applicable)

% Place this call before ''\listofsymbols`` to make the symbols appear on the left instead of the right:
%\ShowSymbolFirst
% To call the “List of Symbols” “Nomenclature” instead use:
%\listofsymbols[Nomenclature]
% To autosort the list use a star after the command (ie. \listofsymbols*[Nomenclature] or \listofsymbols*)
% \listofsymbols
% With very large symbol lists it is sometimes good to split the list into multiple sub-lists.  To output the lists just use the extended \listofsymbols command (below) and to add an element to the list use the optional parameter to ''\addsymbol``.
%\listofsymbols{General Nomenclature}
%\listofsymbols{Greek Letters}
% \newpage

% Note that you may define symbols anywhere in the document, when you re-run LaTeX they
% will be added to the list (just like all other lists)
% \addsymbol{absorption coefficient}{$\alpha_c$}
% \addsymbol{absorption cross section}{$\alpha_{\sigma}$}
% \addsymbol{average radius of cylindrical shell}{$c$}
% \addsymbol{activation energy of oxidation reaction of a-C in excited state}{$E^{\ast}_{act}$}

% Example for sub-list symbols (optional parameter specifies which list to use):
%\addsymbol[General Nomenclature]{absorption coefficient}{$\alpha_c$}
%\addsymbol[General Nomenclature]{absorption cross section}{$\alpha_{\sigma}$}
%\addsymbol[Greek Letters]{average radius of cylindrical shell}{$c$}
%\addsymbol[Greek Letters]{activation energy of oxidation reaction of a-C in excited state}{$E^{\ast}_{act}$}

%%% Parts of a Thesis - Front Matter - List of Abbreviations (if applicable)
% To autosort the list use a star after the command (ie. \listofabbreviations*)
% \listofabbreviations
% \newpage

% Note that you may define abbreviations anywhere in the document, when you re-run LaTeX they
% will be added to the list (just like all other lists)
% \addabbreviation{Bio Force Gun, Model 9000}{BFG9000}
% \addabbreviation{Mammoth Armed Reclamation Vehicle}{MARV}
% \addabbreviation{Stone of Jordan}{SoJ}
% %\addabbreviation{Field flow fractionation-inductively coupled plasma-mass\newline spectrometry% with extra% magic and stuff and things
% \addabbreviation{Field flow fractionation-inductively coupled plasma-mass spectrometry% with extra% magic and stuff and things
% }{FFF-ICP-MS}

%%% Parts of a Thesis - Front Matter - Acknowledgments (optional)

\begin{acknowledgments}
I would like to thank my advisor, Dr. Chuan Yue, for the opportunity to work on this project and for his continued guidance.\
I would also like to thank my research team for their insights and assistance: Zhiju Yang, Weiping Pei, and Thien Ngo Le. \
This project would never have succeeded without the love and support of my friends, girlfriend, and family.
\end{acknowledgments}
\newpage

%%% Parts of a Thesis - Front Matter - Dedication (optional)

% \begin{dedication}
% For those that shall follow after.
% \end{dedication}
% \newpage

%% Parts of a Thesis - Body

\bodymatter

%%% Parts of a Thesis - Body - Introduction (optional)
%\chapter{Introduction}
% A fun introduction would go here, just uncomment!

%%% Parts of a Thesis - Body - All Chapters and Sections (required)

\chapter{Introduction}

The following contains my proposal for the research I plan to conduct under the guidance of Dr. Chuan Yue in pursuit of my Masters of Computer Science degree from Colorado School of Mines.\
My research will be centered around the extraction and analysis of topics in Human Intelligence Tasks (HITs) on Amazon MTURK. 

In this section I will outline the problem that we are trying to solve, provide a high level overview of crowdsourcing platforms, ecosystems, and tools.
\subsection{Background}
Crowdsourcing platforms have become increasingly popular over the course of the last\ 
several years, introducing the concept of utilizing ``crowd'' intelligence to complete\
work \cite{kuek2015global}. Growing crowdsourcing platforms are a result of the growing\
online "gig economy" and has started the conversation for changing labor legislature\
to account for workers who don't fit nicely into the mold of employee or independent contractor\
\cite{harris2015proposal}.
These platforms are historically architected to facilitate the relationship\ 
between requesters, who create work on the platform in the form of items commonly\ 
referred to as tasks, and crowd workers who are real people that complete tasks that\ 
are curated by the requesters. There are a variety of different crowdsourcing platforms\ 
that have entered the crowdsourcing market such as Figure Eight, ClickWorker, CrowdFlower, Spare5,\ 
Respondent, Swagbucks, and one the most popular platforms: Amazon Mechanical Turk (MTURK).\ 
Amazon MTurk has been used frequently by the research community and has become and integral\
part of outsourcing. \cite{hitlin2016research}\ 
These platforms are used for a variety of different use cases ranging from collecting\
data via surveys, image/video annotation, translations, spreadsheet modifications and\ 
analysis, and writing. \cite{ipeirotis2010Analysis} The typical workflow in crowdsourcing\ 
platforms starts with requesters who breakdown work that they need to get completed\ 
into smaller chunks called tasks. This process usually involves writing a description of what needs to be\ 
done, how it needs to be completed, providing the input the crowd worker needs to\ 
complete the work, setting a qualification so that only certain crowd workers may\ 
have access to the task based off their work history, setting a lifetime on the\ 
task for when the work needs to be completed, and setting a reward for successful\ 
completion of the task. All of these specifications vary by platform since a\ 
universal specification standard between different crowdsourcing platforms fails\ 
to exist. \cite{allahbakhsh2013quality} After the requesters finish a task and publish it to the crowdsourcing\ 
platform, the task becomes available to crowd workers (depending on the platform it\ 
may only be made available to crowd workers who match the qualification set by the\ 
requesters) who can then attempt to complete the task. After the crowd workers are\ 
satisfied with their work on a given task, they can then submit their work on a task\ 
to the requesters for approval. After this work is received by the requesters, the\ 
requesters have the ability to review the work that was completed by the crowd worker\
and determine whether they will accept the work, reject the work, or send the task\ 
back to the crowd worker because the quality of the work was insufficient. This\ 
workflow in crowdsourcing platforms presents several inherent issues that need to\ 
be improved in order for crowdsourcing platforms to continue to grow.

\subsection{Requester Role in Crowdsourcing Platforms}
% paragraph on the motivation for requesters
Requesters often utilize crowdsourcing platforms to outsource large pieces of\ 
work to a distributed workforce to leverage the skills, time, and experience of\ 
crowd workers. \cite{kuek2015global} This allows requesters to offload tasks to ultimately save themselves\ 
time and allow them to focus on higher priority tasks. Some of the practical use\ 
cases that requesters offload to crowdsourcing sites are bulk tasks that require\ 
human input but take a considerable amount of time like data annotation of machine\ 
learning and computer vision datasets which requires a large amount of accurately\ 
labeled data to fuel deep learning algorithms. \cite{hitlin2016research} One of the primary benefits that\
crowdsourcing platforms present requesters is that requesters are able to curate a\ 
massive amount of tasks and distribute them amongst crowd workers for a reasonable cost\ 
considering the average wage for crowd workers lingers around \$2-\$5 and hour \cite{Kaplan2018,hara2018data}.
In fact, many of the tasks that requesters publish on crowdsourcing sites have small rewards ranging\
on average from only a couple cents to several dollars. The low average cost of tasks in crowdsourcing\
platforms inherently allows researchers to publish a large number of tasks which also\
simultaneously benefits crowd workers and crowdsourcing platforms by increasing the volume\
of work available. Some of the issues that exist for requesters in crowdsourcing platforms\
are the amount of time it takes to curate tasks, the amount of time it takes to review and\
approve tasks completed by crowd workers, and the diversity in the quality of work that\
crowd workers perform.

\subsection{Crowd Worker Role in Crowdsourcing Platforms}
% paragraph on the motivation for workers
While requesters supply crowdsourcing platforms with the volume of work, crowd workers supply the labor.\
The primary motivation of crowd workers discovered empirically as apart of a study in 2018 by\
a team of crowdsourcing researchers was simple: earn money \cite{Kaplan2018}. The distributed workforce\
of crowd workers complete tasks all over the world with a wide range of skillsets and experience\
while providing human intelligence to complete work published by requesters. The researchers
who performed the survey on crowdsourcing workers in Amazon MTURK discovered that over half (61.7\%) of\
crowd workers were employed fulltime and 50.2\% of workers had recieved a four year education.\cite{Kaplan2018}\
This data shows that many of the crowd workers don't rely on their crowdsourcing work as a\
primary source of income and that many crowd workers are formally educated. However, since\
many crowd workers aren't reliant on crowdsourcing platforms as their primary source of income,\
this suggests that quality will be a lower priority for crowd workers in exchange for efficiency\
as the repercussions for low quality work have much less severe implications than they would\
for low quality work with their full-time employers. With the primary motivation of crowd workers\
regarding the wage they receive from crowdsourcing\
platforms, efficiency is imperative and many of the crowd workers' interactions with\
crowdsourcing platforms will be guided by the overarching goal of making as much money\
as possible as efficiently as possible. To help optimize their time during crowdsourcing\
working sessions, crowd workers use a variety of different tools to help improve their efficiency\
while completing tasks.

% outline problems with the relationship between requesters and workers
\subsection{Requester vs. Crowd Worker Imbalance}
The two differing motives of both requesters and workers develops inherent tension between\
the primary relationship in crowdsourcing platforms. Requesters desire high quality work, whereas,\
crowd workers desire efficient work resulting in two states in the crowdsourcing platform that\
are relatively mutally exclusive. However, unfortunately for crowd workers, the power in this relationship leans heavily towards\
requesters who are able to dominate the dynamic using several features that are common amongst\
crowdsourcing platforms such as: the ability to specify certain qualifications that workers must\
meet to access certain tasks and having ultimate authority over accepting or rejecting the work\
that crowd workers complete based on their own personal, subjective view on quality. \cite{Kaplan2018,allahbakhsh2013quality}\

This imbalance has been the cause of several different pieces of research to help address unfairness in\
in crowdsourcing. Often times, the fruits of research on this topic has resulted in the develpoment\
of tools to help crowdworkers rate and interact with requesters like Turkopticon \cite{irani2013turkopticon}
which gives crowdworkers the ability to rate requesters. Another interesting tool that was developed\
was Dynamo, a tool to allow crowd workers to organize and pursue collective action in an effort\
to combat the largely unregulated crowdsourcing market \cite{salehi2015we}. One of the structural issues\
with crowdsourcing marketplaces is due in large part to issues with the "gig economy" in general as defined\
by Harris et al. which discussed the inefficiencies in current legislature to protect "gig workers" who \
generally don't qualify for common government protections that apply to full-time employees and independent\
contractors \cite{harris2015proposal}. This problem has recently gained high-profile attention in light\
of the coronavirus pandemic, after the CEO of Uber, Dara Khosrowshahi, wrote a public letter to the \
President of the United States, requesting protection for independed workers, colloquially defined as\
"gig workers" \cite{kohsrowshahi_2020}.

This imbalance places workers at a severe disadvantage in crowdsourcing platforms in addition to flaws in the platforms\
themselves. Some of the disadvantages that crowdsourcing platforms inflict on workers include: naive search\
functionality for surfacing tasks, lack of metrics for how long it will take to complete a task,\
stated feasibility of tasks, and a lack of estimated wage value for tasks \cite{Kaplan2018}.\
Crowd workers are at an inherent disadvantage\
in crowdsourcing platforms which harms worker participation, a fundamental requirement for crowdsourcing\
platforms to be successful.

\subsection{Crowd Worker Difficulties}
More specifically, a survey on crowd workers showed that the biggest painpoints for crowd workers\
in the Amazon MTURK crowdsourcing platform are loss of compensation on rejected or returned tasks,\
data on whether or not a given task is even completeable (often times a task may not even be \
completeable because requesters may not have provided enough information for workers to successfully\
complete the task), and the amount of time it takes to find a task or switch context between different\
types of tasks \cite{Kaplan2018}. Another\
interesting data point the researchers (Kaplan et al) collected from their survey on crowd workers was\
that 30\% of respondents said that finding a task was reported ``4 - Very`` or ``5 - Extremely`` \
difficult, but probably even more intriguing was the data that the most pertinent reason for \
ending a session was that they were unable to find a task worth doing (48\% said this ranked\
as an ``5 - Extremely Important`` reason in terminating a crowdsourcing session). \cite{Kaplan2018} This dissatisfaction\
of crowd workers shows that the current working model for crowdsourcing platforms needs to be improved\
to improve the user experience of crowd workers.
The crowdsourcing community has a vested interest to improve\
the user experience of crowd workers in order to continue to facilitate the adoption\
of crowdsourcing platforms by new crowd workers. This area of crowdsourcing has\
a lot of opportunity for research and is relatively sparse compared to other fields\
of research on crowdsourcing platforms.

\subsection{Goal}
%% This section should explicitly state goals of research
\textbf{To analyze common topics and trends found in crowdsourcing tasks on Amazon Mechanical Turk to\
 improve crowdworker efficiency.}

\subsection{Approach}
I will be focusing my research on improving the efficiency of crowd workers in crowdsourcing\
platforms through the analysis of crowdsource tasks to determine common topics and trends found \
in crowdsourcing tasks. Specifically I will focus on applying unsupervised learning techniques in the\
form of topic modeling to improve the understanding of crowdsourcing tasks administered by requesters \ 
as well as how this information can be leveraged to optimize crowdworker efficiency. 

\chapter{Related Work}

\subsection{Topic Modeling of Crowdsourcing Tasks}
Add stuff here on the topic modeling of crowdsourcing tasks
Talk about papers from the conference that Tom suggested.
\subsection{Crowdworker Efficiency}
\subsubsection{Existing Tools}
Crowd workers have created a variety of user plugins and browser extensions to help out the community of\
crowd workers to try and improve crowd worker efficiency. Some of the more prominent plugins focus on batching\
similar tasks to reduce the time users spend switching context between dissimilar tasks and plugins for workers\
to rate requesters based off their interactions directly and indirectly with how the tasks are structured. I've\
curated a list of tools used by crowd workers from my own research and from the results of a survey of crowd workers\
\cite{Kaplan2018}. 

%% use item for existing tools
\begin{itemize}
	\item \textbf{HIT Scraper:} A web scraper that helps provide additional search filters not offered as apart of the native offering for Amazon MTurk.
	\item \textbf{MTurk Suite:} A browser extension used to combine a plethora of other crowd worker tooling.
	\item \textbf{Turkopticon:} A web tool that allows crowd workers to rate requesters and tasks.
	\item \textbf{Greasemonkey/Tampermonkey:} A browser extension that allows crowd workers to run custom scripts to help boost efficiency in crowdsourcing platforms.
	\item \textbf{Panda Crazy:} A tool used by crowd workers to batch similar tasks together.
	\item \textbf{Turkmaster:} A script that monitors search pages, requesters, and can automatically accepts tasks on Amazon MTurk.
	\item \textbf{Block Requesters:} Allows users to block and ignore requesters from search results, useful if crowd workers have a bad experience with a requester and wish to avoid future interactions.
	\item \textbf{Pending Earning:} Allows crowd workers to view pending earnings for tasks that have been completed and submitted but not approved.
	\item \textbf{MTurk HIT DataBase:} Improved interface for searching tasks that you have worked on previously, Amazon MTurk.
	\item \textbf{MTurk Worst Case Scenario Calculator:} Tool to calculate approval rate and how many rejections it would take to drop your approval percentage to a certain threshold.
	\item \textbf{MTurk Dashboard HIT Status links:} A tool which provides quick access to rejected and pending tasks, Amazon MTurk.
	\item \textbf{MTurk Engine:} A browser extension that combines additional search filters with batching, as well as automated task watching for Amazon MTurk. This tool also includes a dashboard to track earnings.
\end{itemize}

These tools show the desire for improvement in the crowd worker user experience from the native\
crowdsourcing platform and a high level of community involvement and support for crowd workers.\
Although there is existing tooling for batching similar tasks using keywords and search filters\
there still lacks tooling for common painpoints highlighted in the survey results collected by (Kaplan et al).\
Some of the main areas of the crowd worker user experience that still need to be addressed are: surfacing\
useful recommendations of tasks that are curated based off worker history, expertise, and preferences,\
content-based analysis of the feasibility of a task, and Intelligent Batching to reduce time spent switching
context.

% \subsubsection{Existing Recommendation Systems}
% %% This section should touch on some research for text classification and recommendation systems
% There are several different approaches to developing recommendation systems, the two most common are collaborative\
% filtering and content based approaches. Collaborative filtering is generally more accurate than content based approaches,\
% however, collaborative filtering struggles with recommending new items, a characteristic of tasks in crowdsourcing platforms.\
% Alternatively, content based approaches are based on determining the similarity between items and how they associate with users in the platform,\
% crowd workers in our use case.

% Often times in platforms that are trying to develop a recommendation system where new data is constantly entering the platform\
% and old data is constantly becoming outdated, the platform will apply a content based approach instead of collaborative filtering\
% to address the cold start problem and the sparseness of interaction of similar users on similar tasks. One example of\
% this use case to a similar problem is the use of a content based approach to surface relevant news articles to content reviewers for\
% media corporations like Buzzfeed. The researchers (Wang et al.) of the study leverage a character level neual network langauge model (a CNN) to perform\
% low-level textual feature learning \cite{wang2017dynamic}. This is a very applicable approach to my research since tasks in crowdsourcing\
% platforms are constantly getting outdated after a worker completes a task, the specific task is resolved and will not be reused, making
% any type of collaborative filtering approach forseeably ineffective.

% Researchers (Yuen et al.) developed a matrix factorization method of recommending tasks in Amazon MTurk. \cite{yuen2012task} Their approach\
% was predicated on using matrix factorization on crowd worker performance history as well as their task searching history\
% to surface relevant reccomendations to crowd workers. Another set of researchers applied two different techniques based on \
% implicit modeling of user history leveraging a Bag-of-words Approach and a classification approach \cite{ambati2011towards}. One\
% of the downfalls of their research was that they only used 24 users to evaluate their approach.

% From my initial investigation, I haven't found any research that is directly related to my approach of extracting necessary\
% skills from tasks using text classification. The primary targeted application of my research is to eventually create\
% a content based recommendation system that will match a user to a task in a crowdsourcing platform based on implicit skills\
% the user has obtained (based on previously completed tasks and user provided characteristics) and the required skills that\
% are needed to complete a task.

\subsubsection{Text Classification of Crowdsourcing Tasks}
Interesting related work in the field of text classification of tasks in crowdsourcing platforms can be found in an analysis\
of the dynamics of crowdsourcing performed by (Difallah et al) in which they used supervised learning to classify\
types of tasks in Amazon MTurk \cite{difallah2015dynamics}. This research is extremely useful for my approach and\
will allow me to use the common categories that they defined as apart of their research to gather data from crowd workers\
as apart of my research on the \textbf{skills} that are required to complete crowdsource tasks.

\chapter{Topic Modeling Approach and Focus}
This chapter is focused on the explanation of my research and design decisions. In this chapter I cover the dataset that \
was used for the experiments, the preprocessing decisions and methodology, the topic modeling algorithms used to \
analyze the dataset, and the evaluation methods that were leveraged in the experiments.
% \ref{fig:Stomata}

% \csmfigure{Stomata}{figures/stomata}{4in}{A pretty picture from the Squier Group --- this is a test of the emergency long-title system.}
% I will explore the application of the algorithms above to the problem of extracting required skills\
% from tasks and compare my results accordingly.
% \label{sec:important-section}
\subsection{Dataset}
The dataset that was leveraged for the experiments in my thesis was collected over the course of January 26, 2020 to March 12, 2020 \
by Dr. Yue's research team. Specifically, the dataset was collected over the following dates shown in \ref{tab:data_dates}. For the \
topic modeling analysis that was performed in the experiments these datasets were combined and processed as a singular\
dataset. Further analysis is also performed from an individual dataset perspective.
\begin{table}
	\caption{\label{tab:data_dates} Amazon MTurk Webscrape Collection Dates}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			& Dates \\
			\hline
			1 & 01/26/20 - 02/02/20 \\
			\hline
			2 & 02/17/20 - 02/27/20 \\
			\hline
			3 & 02/27/20 - 03/03/20 \\
			\hline
			4 & 03/03/20 - 03/12/20 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}
The dataset was collected by a webscraping tool built by the team that ran on a brand new Amazon MTurk account.\
One of the constraints of this dataset is that the HITs that were collected from the tool will only include HITs that would be accessible\
to crowdworkers who haven't completed any previous HITs. Since requesters have the ability to restrict who can access their HITs based off of \
crowdworker qualifications such as HIT approval rate, HIT submission rate, Location, total approved HITs, as well as several other HIT related\
criteria--the tool is not going to be able to cover the bredth of every possible HIT that would be available on MTurk due to the subjective\
nature of the requester restrictions. To mitigate this, the team decided to collect data from the perspective of a new crowdworker on Amazon MTurk\
and thus the corresponding analysis and topics that are surfaced from the dataset will reflect this perspective.

The tool leveraged several tools to collect the dataset. Specifically the Amazon MTurk account used a Chrome extension to refresh the page periodically\
and record new HITs as they became available in conjunction with Selenium which would be used to access the HIT Preview, record screenshots of the preview\
and access and record external links. All of the data that is collected from a HIT is then stored locally in a MongoDB database on an external hard drive.\
The tool records three different documents for each HIT that are stored whenever a new HIT becomes available: HIT data which store relevant HIT metadata,\
HIT lifetime data which stores metadata corresponding to the lifecycle of the HIT, preview data which holds metadata the tool records from accessing the HIT\
preview, as well as external links data that the tool records while accessing the preview. For my analysis I only leveraged the HIT and preview collections\
in the database since the documents in these collections contained the raw text that was unique to the HIT itself which lends itself nicely to the\
unsupervised analysis of topic modeling on unstructured, raw text.

The specific fields recorded by the tool for a given HIT document are listed in the JSON snippet below:

\begin{lstlisting}
{	
  _id: the ID created by Mongo database
  hit_set_id: the hit set ID that assigned by AMT
  requester_id: the ID of the requester who creates the hit
  requester_name: name of the requester
  title: title of the hit
  description: description of the hit
  assignment_duration_in_seconds: the amount of time the hit has been posted
  creation_time: time when the hit is created
  assignable_hits_count: number of hits available for accepting
  latest_expiration_time: the expiration time of the hit
  caller_meets_requirements: check if the worker meets the requirement to accept the hit (TRUE/FALSE)
  caller_meets_preview_requirements: check if the worker is eligible for opening the preview page 
}
\end{lstlisting}

The corresponding fields recorded by the tool for the preview itself (not including the external links embedded in the preview) are listed in the JSON snippet below:\

\begin{lstlisting}
{
  _id : this ID was created by Mongo Database
  type : type of the link (visualized or href)
  from : location of the preview page? (on AMT or from external link)
  hit_set_id : ID of the hit set, this will correspond to the hit set ID on HITs DATA
  url : link to the preview page
  date : date time format
  uid : requester ID
  has_visited : has the preview page been visited
  links_saved : all the links on preview page
  has_link_visited : whether the preview link has been visited (TRUE/FALSE)
  href_arr : links to all of the urls listed on the preview page
  img_name_map :  images from the preview page
	page_src : source code of the preview page
}	
\end{lstlisting}

In my analysis I leveraged the ``title'' and ``description'' fields from the HIT data and the ``page\_src''\
field from the preview data. This raw text was then passed through preprocessing before eventually being sent on to the different\
topic models that were leveraged for the topic modeling analysis. Preprocessing was a necessary part of the topic modeling analysis.\
There are a variety of different perspectives on the effectiveness of different preprocessing methodologies that I will explain in depth\
in the following section.

\subsection{Preprocessing}
In the field of natural language processing (NLP) and text mining there are a wide variety of techniques related to the preprocessing of data.\
Some of the preprocessing techniques that relate specifically to topic modeling include stemming \cite{lovins1968development}, stop word\
removal \cite{silva2003importance}, and document duplication \cite{bouayad1999duplication}. The effectiveness of these techniques on\
topic modeling is debated and I have taken these considerations into my experimental design decisions.

Stemming and lemmatization are commonly found in NLP libraries. These techniques have bee around for years and have been studied thoroughly \cite{lovins1968development, kanis2010comparison, jivani2011comparative, larkey2002improving}.
The process of stemming essentially consolidates similar words down into a root word, often times removing prefixes and suffixes or other semantic\
modifications. However, in context to topic modeling, stemming techniques have been found to do more harm then good and actually decrease model performance \cite{schofieldunderstanding} \
by "diminishing interpretability" \cite{boyd2014care}. For this reason I chose note to utilize lemmatization techniques as apart of the preprocessing of my \
data so as not to decrease model performance--especially since some of the recent innovations in the field of topic modeling leverage word embeddings \cite{mikolov2013distributed} and the\
semantic meaning of the word becomes more important than distribution based models such as the Latent Dirichlet Allocation (LDA) \cite{blei2003latent} which rely on the appearance of the\
word in the document rather then it's position relative to its surroundings.

Stop word removal, the removal of words that don't add contextual meaning to a sentence such as ``the'', ``and'', ``a'' is another common preprocessing technique in topic modeling.\
As far as literature goes, there are several studies that debate the effectiveness of removing stopwords during processing. One study states that using stop-word lists\
is incomplete since it is difficult for these lists to encompass corpus-specific stopwords \cite{boyd2014care} and that alternative methods are to leverage \emph{tf-idf}\cite{Salton1968AutomaticIO}\
to automate the selection of stopwords by only allowing words over a certain threshold. While another study shows that removing additional stopwords beyond the top 12 most occurent stop words doesn't\
have an acknowledgeable effect on model performance or topic coherence \cite{schofieldunderstanding}. This same study did, however, find that the\
removal of determiners, conjunctions, and prepositions did in fact improve performance. For my performance I decided to use a fixed list of stop words\
as apart of my preprocessing phase. The driving reason for doing so was to implement the ETM model \cite{dieng2019topic}, a state of the art topic model,\
on our dataset which utilizes a stop word list during it's preprocessing step. Removing stop words from the raw data during preprocessing allows for fair comparison across\
models as well as replication of the ETM model. One of the other benefits of using a fixed stop word list over using the \emph{tfidf} approach discussed in \cite{boyd2014care}\
is the persistence of sparse words that would otherwise be removed from the corpus but may have a strong correlation to a specific topic.

Document duplication removal is a rather common text modeling preprocessing technique. In essence, this technique is used to simply remove duplicates from the corpus. \
This is a large consideration in our dataset since requesters in crowdsourcing platforms often publish HITs in large batches ranging from a couple hundred to tens of thousand\
HITs that are duplicates. This is due in large part to one of the primary use cases for requesters on the platform being data annotation, many researchers will\
publish massive annotation task batches that duplicative which would bloat the size of the corpus used in this analysis without additional preprocessing. One study\
found that duplication in the corpus can in fact effect model performance but only for large quantities of duplicates and that these duplicate documents in the corpus\
can be quickly clustered into individual topics \cite{schofieldunderstanding}. This means that document duplication wouldn't have a profound effect on the topics\
discovered by the topic models since the effect would be contained to several specific topics without effecting unrelated topics. However, for the topic analysis I decided \
to remove document duplicates since there exists a large number of duplicate HITS in the corpus which would increase the training time exponentially without any positive effect\
on the results of the models on the corpus.

While the original LDA paper was trained for unigram tokens and made the assumption that words are generally independent of one another \cite{blei2003latent}, the LDA has also\
been extended to perform topic modeling on n-gram tokens as well \cite{wang2005note}. However, the use of \texttt{n-gram} in topic models such as the LDA increase the complexity\
of the model \cite{wang2007topical}. To mitigate increasing the model perplexity and simultaneously increasing the training time of the models and also respecting the implentation of\
the modern topic models that are implemented in the analysis of the corpus which used unigrams as input for their models \cite{moody2016mixing}\cite{dieng2019topic}, the analysis\
will only consider unigrams as input to the models to allow for a fair, consistent, and efficient comparison of the different topic models on our dataset.

In addition to removing stop words from a fixed list and removing duplicates from the corpus, I also filter out punctuation and several comon symbols using a regular expression\
as well as cast all of the unigram tokens to lowercase to consolidate capitalization into one word. During the removal of stop words, the preprocessing step also includes a simple check\
to remove standalone numbers using the \texttt{nltk} \cite{loper2002nltk} \texttt{isnumeric()} function.

For the preview corpus there is also an additional preprocessing step to clean up the page source that captures the raw text of the preview from the web page. Often time when\
extracting data from a web page there exists a good deal of noise that comes with extracting raw text from the web such as HTML, styling, scripts injected into the web page as\
well as web page boiler plate. Gathering input from the page source of a website requires significant preprocessing in order for the data to be a good input for topic models \cite{boyd2014care}.\
To clean up the page source of documents in the preview corpus, each document is put through additional preprocessing than the title and descriptions which are extracted only as strings\
rather than web source code and each preview is stripped of all HTML, scripts, styles, and new lines using the Beautiful Soup tool \cite{richardson2007beautiful}. \
After stripping the web entities out of the page source, the size of the data was massive and had a large and unique vocabulary -- the dataset was reduced to 5\% of \
it's original size. This decision was backed by the fact that the handling of large vocablularies by topic models is still an area yet to be addressed by research \cite{dieng2019topic}.\
To reduce the dataset, the combined dataset was sampled randomly until it was reduced from $32814$ unique documents to $1640$ unique documents. After being reduced in size,\
this dataset was then manually checked for combined/inerrent tokens to allow the models to discover unigram topics. This manual step was deemed necessary after examining\
results from topic models on the preview dataset that contained conjoined words (i.e ``zoomblank'', ``berniesanders'', ``redbox'') which was causing the models\
to create incoherent topics. After manually separating these conjoined tokens, the previews provided the richest topics amongst datasets due in part to the size of the\
documents themselves, and that the vocabulary had a greater diversity than that of the description and title datasets.
\subsection{Topic Models}
Topic modeling is a commonly used approach to perform unsupervised analysis of unstructured text. The field has become increasingly more popular as the amount of unstructured\
data that businesses continue to collect increases, one study estimated that up to 80\% of an organizations data is unstructured \cite{Sint2009CombiningUF} making the development\
of algorithms and models that can extract insights and meaning from this unstructured data increasingly more important. The ability to process and extract meaning\
from unstructured data makes topic modeling an excellent candidate to analyze and extract common topics and themes that appear in HITs on Amazon MTURK. The extraction and understanding\
of this data will be valuable for implementing future works related to improving crowdworker efficiency.

Topic modeling has its origins around the beginning of the 21st century beginning with a several algorithms published on the subject of topic modeling such as \
Probabilistic latent semantic indexing \cite{hofmann1999probabilistic} and Latent semantic indexing (LSI) \cite{papadimitriou2000latent} (LSI) which would pave the way for the\
creation of the Latent Dirichlet allocation (LDA) \cite{blei2003latent} which is the most commonly implemented topic modeling algorithm in use today. In recent years, \
researchers have begun to develop topic modeling algorithms that have acheived state of the art performance leveraging word embeddings \cite{mikolov2013distributed} \cite{dieng2019topic}.\
Word embeddings have opened the door for innovations across the field of natural language processing and topic modeling is no different. 

The dataset will be analyzed using four different topic models namely LSI, LDA, lda2vec, and the Embedded Topic Model (ETM). 
\subsubsection{Latent Semantic Indexing (LSI)}
Latent Semantic indexing was originally proposed in the paper ``Indexing by latent semantic analysis'' by Deerwester et al. \cite{deerwester1990indexing}. The authors\
proposed the algorithm as a means to improve information reteival (IR) by treating IR as a statistical problem. One of the shortcomings in the field of IR at the time\
the paper was written was the inability to capture conceptual meaning from a user search on the corpus. LSI was created to address these problems by assuming that there\
exists underlying semantic meaning in a document and thus through estimating the noise in a given set of data with respect to word choice their methodology can associate\
terms and documents through the expression of a singular value decomposition (SVD) on term to document data.

Specifically, LSI can be defined by a $n$ $x$ $m$ matrix $A$ where the rows are represented by the words in the corpus and the columns represent the documents with a rank of \emph{r}.\
From this matrix, using a SVD, \emph{A} can then be represented as the product of 3 separate matrices as shown in Equation \ref{eqn:lsi}:

\begin{eqnarray}
		A = UDV^T
	\label{eqn:lsi}
\end{eqnarray}

Where $D$ is a diagonal $r$ $x$ $r$ matrix of the eigenvalues of $AA^T$, $U$ is a $n$ $x$ $r$ matrix of rank $r$ that has orthonormal columns,\
and $V$ is an $m$ $x$ $r$ matrix with orthonormal columns as well \cite{papadimitriou2000latent}. This SVD is the LSI space of the term-document\
representation of the corpus. 

For the implementation of the LSI in the experiments, the \emph{gensim} \cite{gensim} library is utilized. The \emph{gensim} implementation of the library\
makes use of the Singular Value Decomposition algorithm described in ``Fast and Faster: A Comparison of Two Streamed Matrix Decomposition Algorithms'' by Radim Rehurek\
which allows the model to traverse the corpus in a single pass, decreasing the amount of time it takes LSI to train \cite{rehurek2011}.

This model was selected due to its similarity to LDA and because many studies often compare the performance of LSI on topic modeling to the performance of LDA.

\subsubsection{Latent Dirichlet Allocation (LDA)}
The LDA is a topic model developed by Blei et al. in 2003, the model has been widely used and accepted since its inception in the field of topic modeling and is frequently\
implemented as the baseline from which new topic models are compared against. From an implementation perspective the LDA is described as a generative probabilistic model of\
a corpus in which documents in a corpus are represented as a random mixture of a fixed number of latent topics and a topic is \
a probability distribution over words in the corpus. In the paper, the LDA is defined by the following generative procedure in \ref{eqn:lda}:

\begin{figure}
	\begin{enumerate}
		\item Choose $N \sim Poisson(\xi)$
		\item Choose $\theta \sim Dir(\alpha)$
		\item For each of the $N$ words $w_n$:
		\begin{itemize}
			\item Choose a topic $z_n \sim Multinomial(\theta)$.
			\item Choose a word $w_n$ from $p(w_n|z_n,\beta)$, a multinomial probability conditioned on the topic $z_n$.
		\end{itemize}
	\end{enumerate}
	\caption{LDA model \cite{blei2003latent}}
	\label{eqn:lda}
\end{figure}

In laymen terms, the LDA is fed a paremeter for the number of topics $z$ that the model will be trained for and from those topics it then generates \
a topic mixture for a document using a Dirichlet distribution over $z$, then for each word, select a topic that the word should belong\
based off of what topics are in the selected document as well as how many times the word has been assigned to a topic in other documents. \
A visualization of this model can be seen in \ref{fig:lda}: 

\csmfigure{lda}{figures/lda_diagram}{4in}{Basic LDA model architecture \cite{blei2003latent}}

In the visualization above (\ref{fig:lda}), $\alpha$ signifies topic-document distribution of the whole corpus, $\theta$ represents the \
document specific topic distribution, $z$ signifies the topic for the selected word in a document, and $w$ is the selected word. Outside \
of the box $\beta$ represents the per-topic word distribution throughout the corpus. The outer boxes $M$ and $N$ are representative of \
the generative approach of the LDA in that $N$ represents the assignment of words to topics in a document while $M$ represents the collection\
of documents that make up the corpus.
The bread and butter of the LDA is this two-headed process by which a word is assigned to a topic according to the topic-document distribution\
in conjunction with how the selected word has been assigned to other topics across documents in the corpus.\
The LDA then repeats this process until it reaches the number of passes that are specified as a parameter to the model, resulting in the formation of $z$\
topics across the corpus with words that were topically assigned based off of the the context of the word in the document it appears as well as how\
the word is assigned in documents across the bredth of the corpus.

From the performance perspective, in the original paper the LDA is cited as outperforming the \emph{pLSI} \cite{hofmann1999probabilistic}, an adaption of the original \emph{LSI},\
measured on the perplexity metric which was commonly used to evaluate early topic models. In the empirical evaluation section of the paper the authors\
cited that the design of the LDA led to the avoidance of some of the shortcomings of the \emph{pLSI} especially overfitting by utilizing a variational\
Bayesian smoothing scheme \cite{attias2000variational} \cite{blei2003latent}. 

For the implementation of the LDA in the experiments, the \emph{gensim} library is leveraged which is based off of the code presented by some of the original authors of the LDA \cite{hoffman2010online}.
The specific hyperparameters that were implemented in the experiments are discussed at greater lengths in experiments in Chapter 4.

The LDA was selected for the experiments because it is the most widely used and implemented topic model and is the baseline for which state of the art\
models compare their performance.
\subsubsection{lda2vec}
The \emph{lda2vec} model was one of the first topic models to develop a hybrid approach which married the sparse document-topic representation of a corpus used in\
traditional topic modeling like the LDA with the innovation of word embeddings. \emph{lda2vec} was developed to address one of the primary shortcoming of traditional\
topic modeling approach--the failure to take advantage of recent innovations in distributed word representation such as the skip gram representation presented in \emph{word2vec} \cite{mikolov2013distributed}.\
Word embeddings as previously addressed have had a substantial effect on the field of NLP over a wide array of applications and the development of hybrid topic models\
that leverage these distributed word representations to account for semantically meaningful relationships between words has led to advances in topic modeling as well as\
the development of state of the art topic models.

From an implentation perspective the \emph{lda2vec} model modifies the Skipgram Negative-Sampling (SGNS) objective from Mikolov et al. \cite{mikolov2013distributed} to learn document-wide\
feature vectors in concurrence with discovering document weights that are loaded onto topic vectors \cite{moody2016mixing}. \
The loss of the \emph{lda2vec} model can be represented with the following equations (\ref{eqn:lda2vec1}, \ref{eqn:lda2vec2} \cite{moody2016mixing}):

\begin{eqnarray}
	\label{eqn:lda2vec1}
		\mathcal{L} = \mathcal{L}^d +\Sigma_{ij} \mathcal{L}^{neg}_{ij}  \\
	\label{eqn:lda2vec2}
	\mathcal{L}^{neg}_{ij} = log \sigma (\vec{c_j} \cdot \vec{w_i}) + \Sigma^n_{l=0} log \sigma (-\vec{c_j} \cdot \vec{w_i})
\end{eqnarray}

In the equations above the total loss $\mathcal{L}$ is represented by the sum of the Dirichlet likelihood of term of document weights $\mathcal{L}^d$ \
while the summation of $\mathcal{L}^{neg}_{ij}$ represents the paper's modification of the Skipgram objective. In further depth \
$\vec{c_j}$ represents the context vector which accounts for the models combination of word and document vectors (See Equation \ref{eqn:lda2vec3} below \cite{moody2016mixing}).

\begin{eqnarray}
	\label{eqn:lda2vec3}
	\vec{c_j} = \vec{w_j} + \vec{d_j}
\end{eqnarray}

For additional explanation please reference the visual of the \emph{lda2vec} model architecture in \ref{fig:lda2vec} \cite{moody2016mixing} on the page below.

\csmfigure{lda2vec}{figures/lda2vec_diagram}{3.65in}{lda2vec model architecture \cite{moody2016mixing} -- note the combination of the Skip grams from \emph{word2vec} \cite{mikolov2013distributed} used in conjunction with the LDA architecture \cite{blei2003latent}}

\newpage
From an application perspective, the author states that he used several preprocessing techniques including lemmatization which as discussed in the preprocessing\
section above, can be detrimental to model performance. The author also leverage pretrained word embeddings in one of the experiments using the\
pretrained values discussed in the Skipgram paper \cite{mikolov2013distributed}. For the implementation of \emph{lda2vec} in the experiments on \
the MTURK dataset, pretrained GloVe embeddings referenced in the GloVe paper was leveraged \cite{pennington2014glove}. \
Specifically, the 300d vector that was trained on the 2014 English Wikipedia corpus and English Gigaword 5th Edition corpus with\
6B tokens and a vocab of 400k.
During the experiments the researcher used the \emph{C\_v} topic coherence mertric \
which was shown to correlate with human interpretability \cite{roder2015exploring}. The \emph{C\_v} analysis on this experiment was used with \
the online \emph{Palmetto} \cite{palmetto} topic evaluation tool. However, the \emph{C\_v} coherence metric that was used to evaluate topic coherence in these experiments \
has been discovered to have a negative correlation other topic coherence metrics \cite{palmetto} that correlate with human interpretability \cite{mimno2011optimizing} \cite{newman2010automatic}\
and thus this metric will not be leveraged to evaluate topic coherence in the experiments on the MTurk dataset.

\emph{lda2vec} is an excellent comparison against traditional topic models and will reveal valuable insights on how leveraging topic-document distributions in\
conjunction with word embeddings can effect the interpretability of topics on our dataset.

\subsubsection{Embedded Topic Model (ETM)}
The Embedded Topic Model (ETM) is another topic model that combines traditional topic models together with word embeddings. The ETM is \
a generative probabilistic model like the LDA, however, the word-topic probability is represented as an embedding instead of a probabilistic\
distribution where each topic exists in the word embedding space \cite{dieng2019topic}. Some of the existing benefits of the model that is addressed\
in the paper is that the model is able to handle stop words effectively using the topic representation in the embedding space while also introducing\
an algorithm to efficiently compute the posterior of topic proportions which allows the model to fit to large vocabularies and large corpora, a previously\
unaddressed space in topic modeling \cite{dieng2019topic}.

From an implementation standpoint the ETM focuses on the Continuous Bag of Words (CBOW) version of word embeddings \cite{mikolov2013distributed}, in CBOW\
the representation of a given word can be expressed using the equation below \ref{eqn:etm} \cite{dieng2019topic}: 

\begin{eqnarray}
	\label{eqn:etm}
	w_{dn} \sim softmax(\rho^T\alpha_{dn})
\end{eqnarray}

In the equation the subscripts \emph{d} and \emph{n} denote the \emph{n\-th} word in a document (\emph{d}). In the equation $\rho$ signifies a matrix that contains \
the embedding representation of the vocabulary in the corpus and $\alpha$ represents the \emph{context embedding} which is the sum of the context embedding vectors of \
the words surrounding the current word \cite{dieng2019topic}. In the context of the ETM model, this \emph{context embedding} is referred to as a \emph{topic embedding}\
where the topic embedding is a distributed representation of the current topic in the space of words in the document. What this allows the topic embedding to do is form\
a topic distribution over the words in the corpus and then assign a ``high probability'' to a given word in a given topic based off the agreement between the word and topic\
embeddings \cite{dieng2019topic}.

In the paper, the authors outline two methodologies for creating $\rho$: leverage pretrained word embeddings\
(which the authors refer to Labelled-ETM) or discover the word embeddings during the training process.\
During the analysis section of the paper, the author's explained the Labelled-ETM which used pretrained skipgram embeddings\
outperformed the ETM which learned embeddings during the training process. For this reason, the Labelled-ETM will be\
leveraged for the analysis or our dataset.\
For the experiments ran in the study, the Author's version of the Labelled ETM leverages Skip-gram embeddings\
that were trained on the \emph{New York Times} corpus \cite{sandhaus2008nyt}. However, for the implementation of the ETM\
in the experiments on the MTURK dataset, the GloVe embeddings \cite{pennington2014glove} used in our implementation for \emph{lda2vec} are also\
used for our implementation of the ETM. In the GloVe embedding paper, the GLovE word representation is cited as "performing significantly"\
better than other word representations including the CBOW and Skip gram models proposed by Mikolov et al. \cite{pennington2014glove}.

From a higher level perspective the generative process of the ETM model can be described with the following steps in \ref{eqn:etm-procedure}:
\begin{figure}
	
	\begin{enumerate}
		\item Draw topic proportions $\theta_d \sim \mathcal{L}\mathcal{N}(0,I)$
		\item For each word \emph{n} in the document:
		\begin{enumerate}
			\item Draw the topic assignment $z_{dn} \sim Cat(\theta_d)$
			\item Draw the word $w_{dn} \sim softmax(\rho^T\alpha_{z_{dn}})$
		\end{enumerate}
	\end{enumerate}
	\caption{ETM model generative procedure \cite{dieng2019topic}}
	\label{eqn:etm-procedure}
\end{figure}

The steps 1-2a are very similar to the generative procedure followed by the LDA \cite{blei2003latent}, randomly generating topic proportions over documents in the corpus, then\
for each word in a document, drawing a topic assignment for the word. However, instead of then proceeding to assign the word to a topic like in the LDA based off of the topic proportion\
in the document and the word assignment to topics throughout the corpus, the ETM actually assigns the word based off of the CBOW representation of the topic embedding from the document\
context \cite{dieng2019topic}. Step 2b also differs from the direct CBOW implementation because instead of using a sliding window of words to develop the embeddings, the \
topic embeddings are created with respect to the entire document. 

During the analysis section the authors measure topic quality using the UMass topic coherence score \cite{mimno2011optimizing} in conjunction with their own
metric they define as topic diversity, which assesses the diversity in the topics generated by the topic models \cite{dieng2019topic}. \
Using these metrics, the authors claim that the ETM significantly outperforms both the LDA and the simplistic constrained neural \
variational document model (NVDM), a multinomial factor model of documents \cite{miao2016neural}.\
The ETM is a state of the art topic model according to the analysis in the paper and is cited as outperforming the LDA on the metric of topic\
coherence (which is linked to interpretability \cite{mimno2011optimizing}). This makes the ETM an excellent candidate to test against our dataset.


\subsection{Quantitative Analysis}
Evaluating topic models has generally been a difficult topic to broach. The overarching goal of topic modeling is to garner\
understanding and meaning from unstructured text which has been a difficult problem for researchers to attempt to quantify.\

\subsubsection{Perplexity}
During the early days of topic modeling\
the defacto standard for evaluating topic models was \emph{perplexity}, a general metric for evaluating statistical models
which measures how well a probability distribution can predict a sample.\ In fact, perplexity was the method used \	
to originally evaluate the LDA and is even used by modern topic models that claim they are state of the art using the
metric of perplexity \cite{kesiraju2019learning}. However, as apart of a pivoting study in the field of topic modeling\
that has been frequently cited, researchers discovered that perplexity actually has a negative correlation with human\
interpretability \cite{chang2009reading}. In the study by Chang et al., the researchers performed a large scale human\
study on Amazon MTurk and discovered that humans find that topics generated by models that achieve higher perplexity scores\
are often considered less interpretable in latent spaces \cite{chang2009reading}. This work would give way to a new means\
of evaluating topic coherence.

\subsubsection{Topic Coherence}
The goal of topic coherence is to automate the evaluation of the interpretability of latent topics that topic models generate.\
Topic coherence metrics achieve this by measuring the semantic similiarity between top words in a topic allowing for the separation\
of topics that are actually interpretable and the topics that were simply inferred statistically \cite{stevens2012exploring}.
There have been several approaches that have been confirmed to have a positive correlation with human interpretability\
and have been applied to evaluate the effectiveness of advances in the field of topic modeling.

The two topic coherence that will be used to evaluate topic coherence in the experiments on the datset are UCI \cite{newman2010automatic}\
and UMass \cite{mimno2011optimizing} which have both been shown to correlate with human judgement on topic quality.

\paragraph{UCI}
UCI defines the word pair score between words as a pointwise mutual information between words shown in \ref{eqn:uci} \cite{stevens2012exploring}:
\begin{eqnarray}
	\label{eqn:uci}
	score(v_i,v_j,\epsilon) = log \frac{p(v_i,v_j) + \epsilon}{p(v_i)p(v_j)}
\end{eqnarray}

These word probabilities are then computed using a sliding window over an external corpus like Wikipedia \cite{stevens2012exploring}.

\paragraph{UMass}
Where UCI is based of the word pair score in the topics, the UMass metric is based off of document co-occurence as defined\
by the equation below in \ref{eqn:umass} \cite{mimno2011optimizing}:

\begin{eqnarray}
	\label{eqn:umass}
	score(v_i,v_j,\epsilon) = log \frac{D(v_i,v_j) + \epsilon}{D(v_i)}
\end{eqnarray}

UMass takes more of an internal approach by counting the number of documents that contain words in a topic and computing \
this metric of the corpus the model trained on, rather than an external corpus like UCI \cite{stevens2012exploring}.\
The UMass topic coherence metric was also used to evaluate topic coherence on the ETM model.

\subsection{Number of Topics}
The number of topics that are selected when training a topic model is critically important. The majority\
of topic models (even modern topic model architecture that use word embeddings) are still based on or used\
in conjunction with a probability distribution over a fixed number of latent topics. This fixed parameter\
has been a subfield of research within topic modeling and many researchers approach for tweaking this\
value is via guess and check. One of the approaches that has been pitched as a solution to this problem\
is to train topic models over a set number of topics and evaluate the corresponding topic coherence approaches.\
After the advances of topic coherence metrics which possessed a positive correlation with human interpretability,\
Stevens et al. performed a study on the number of topics and it's effect on topic coherence using several\
heavily implemented topic models. In their study, the researchers discovered that when valuating off of \
topic coherence alone, that topic models reach a point of stabilization in terms of topic coherence using the UCI and UMass\
metrics at around 100 topics. This trend was consistent over the topic models analyzed in the study: LDA, LSI (SVD), and LSI (NMF)\
\cite{stevens2012exploring}. The figure used to analyze these results, published in the paper \
is shown below in \ref{fig:num_topics}:

\csmfigure{num_topics}{figures/num_topics}{6.5in}{Effect of Number of Topics on UCI and UMass Topic Coherence \cite{stevens2012exploring}}

Upon these findings on the topic coherence metrics that will be used to evaluate topic quality in this\
study, each model will be evaluated on the dataset over $10-100$ topics using 5 step intervals. This will\
allow for the comparison of the models over a range of topics and will give statistical backing for the\
latent topics that receive further in-depth analysis.

\subsection{Qualitative Analysis}
For each model implementation on each dataset, a table of topics at a high scoring number of topics (\emph{K}) based\
off the topic coherence metrics will analyzed for common trends and topics. 

\subsection{Approach}
To summarize the approach of the following experiments: the 4 separate datasets that were collected\
from webscraping Amazon MTurk by Dr. Yue's research team will be combined and used as the source of truth over the course\
of the experiment. This dataset will then be preprocessed to remove stopwords and document duplicates, as well a \
web entities in the preview datasets. This dataset will then be analyzed for high level trends \
and relationships that can provide further insight to the results of the topic models and to understand the \
characteristics of the dataset itself. Then, using the LSI, LDA, \emph{lda2vec}, and ETM, these models will \
be used to extract information from the titles, description, and previews of the HITS in the dataset. These models\
will be evaluated over a range of $10-100$ topics, stepping in intervals of 5 topics, and will be analyzed quantitatively\
using the UCI and UMass topic coherence metrics as well as in further depth qualitatively for the high performing\
number of topics for each model on each dataset. 

\chapter{What are the primary topics of Amazon MTURK HITs?}

\subsection{Data}
This section will analyze the data that was collected by Dr. Chuan Yue's research team.

\subsubsection{HIT Dataset}
Chat about the dataset. \
was not optained. This data was based off of the raw dataset, before preprocessing.
\ref{tab:requester_top_10}
\begin{table}
	\caption{\label{tab:requester_top_10} Top 10 Requesters by Number of Hits Created before Preprocessing}
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Requester Name} & \textbf{Number of HITs} \\
			\hline
			Panel              &          400699 \\
			\hline
			Ibotta, Inc.       &           57289 \\
			\hline
			411Richmond        &           55919 \\
			\hline
			Shopping Receipts  &           49162 \\
			\hline
			p9r                &           44926 \\
			\hline
			Rece Capture       &           36025 \\
			\hline
			MLDataLabeler      &           34017 \\
			\hline
			Crowdsurf Support  &           31734 \\
			\hline
			TeamZ              &           31336 \\
			\hline
			lovergingers       &           24841 \\
			\hline
			Rest of Requesters &          342754 \\
			\hline
			\textbf{Total HITs} & \textbf{1108702} \\
			\hline
			\end{tabular}
	\end{center}
\end{table}

\ref{fig:top10-pi}
\csmfigure{top10-pi}{figures/top10-pi}{6.5in}{Top 10 Requesters by Number of HITs before Preprocessing }
\newpage

\ref{tab:title_requester_top_10}
\begin{table}
	\caption{\label{tab:title_requester_top_10} Title -- Top 10 Requesters by Number of Hits Created after Preprocessing}
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Requester Name} &  \textbf{Number of Titles} \\
			\hline
			SET Master Account &              97 \\
			\hline
			quarrel-recast     &              91 \\
			\hline
			Plancky            &              60 \\
			\hline
			PickFu             &              45 \\
			\hline
			Alexey Kuznetsov   &              44 \\
			\hline
			MLDataLabeler      &              36 \\
			\hline
			WikiRealty         &              35 \\
			\hline
			joanna socha       &              31 \\
			\hline
			Angela Listy       &              30 \\
			\hline
			UserBob            &              22 \\
			\hline
			Rest of Requesters &            2657 \\
			\hline
			\textbf{Total Titles} & 3148 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:title-post-top10-pi}
\csmfigure{title-post-top10-pi}{figures/title-post-top10-pi}{6.5in}{Title -- Top 10 Requesters by Number of HITs after Preprocessing }
\newpage

\ref{tab:description_requester_top_10}
\begin{table}
	\caption{\label{tab:description_requester_top_10} Description -- Top 10 Requesters by Number of Hits Created after Preprocessing}
	\begin{center}
		\begin{tabular}{| c | c |}
				\hline
				\textbf{Requester Name} &  \textbf{Number of Descriptions} \\
				\hline
				SET Master Account &              97 \\
				\hline
				Volker Rieck       &              65 \\
				\hline
				Alexey Kuznetsov   &              50 \\
				\hline
				Angela Listy       &              27 \\
				\hline
				Positly            &              16 \\
				\hline
				The Wharton School &              12 \\
				\hline
				Aparna Garimella   &              11 \\
				\hline
				Lindsay Kramer     &              10 \\
				\hline
				Robert Zeithammer  &              10 \\
				\hline
				Oliver Borchers    &               8 \\
				\hline
				Rest of Requesters &            2438 \\
				\hline
				\textbf{Total Descriptions} &  \textbf{2744} \\
				\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:description-post-top10-pi}
\csmfigure{description-post-top10-pi}{figures/description-post-top10-pi}{6.5in}{Description -- Top 10 Requesters by Number of HITs after Preprocessing }

\newpage

adfkalsdmfdsamkl

Explain the findings of HIT Analysis
How would this effect topic modeling?
Cite the literature on how duplicate documents shouldn't influence the result of the topics.

\newpage
\subsection{Experiments}
This section should talk about the experiments that were run

\subsubsection{Implementation}
\paragraph{LSI}
The LSI Experiments were conducted using the \emph{gensim} library \cite{gensim} which implemented the specific\
algorithms described in Chapter 3. For each dataset, an LSI model with a batch size of 100 was trained with the preprocessed\
data described in Ch. 3 for values \emph{K}: $10 - 100$ on a five step interval (i.e 10, 15 .... 95, 100). After the model was trained, the top 10\
words for each topic \emph{K} were written to a text file which was then used to calculate the different topic coherence\
values for each model.

\paragraph{LDA}
The LDA Experiments were conducted using the \emph{gensim} library \cite{gensim} which implemented the specific\
algorithms described in Chapter 3. For each dataset, an LDA model with a chunk size of 100, random state of 100,\
for 13 passes. The model was trained with the preprocessed data described in Ch. 3 for values \emph{K}: $10 - 100$\
 on a five step interval (i.e 10, 15 .... 95, 100). After the model was trained, the top 10\
words for each topic \emph{K} were written to a text file which was then used to calculate the different topic coherence\
values for each model.

\paragraph{LSI}
describe implementation

\paragraph{lda2vec}
lda2vec implementation \cite{lda2vecGithub}

\paragraph{ETM}
describe implementation 
etm implementation \cite{etmGithub}

\subsubsection{HIT Title}
Use this section to discuss the experiments run on the descriptions of HITS in the dataset
\paragraph{Corpus Statistics}
Total unique words 3337
Total words in corpus: 14584
\ref{tab:title_top_words}

\begin{table}
	\caption{\label{tab:title_top_words} Title -- Top 10 Words Corpus}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Word} & \textbf{Count} \\
			\hline
			survey & 956 \\
			\hline
			minutes & 849 \\
			\hline
			answer & 391 \\
			\hline
			study & 230 \\
			\hline
			short & 224 \\
			\hline
			hit & 151 \\
			\hline
			image & 145 \\
			\hline
			minute & 127 \\
			\hline
			compensation & 101 \\
			\hline
			opinions & 100 \\
			\hline
			\textbf{Unique Words} & 3337 \\
			\hline
			\textbf{Total Words} & 14584 \\
			\hline
			\textbf{Number of Documents} & 3148 \\
			\hline
			\textbf{Average Document Length (Characters)} & 38 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:title-50-bar}
\csmfigure{title-50-bar}{figures/title-50-bar}{6.5in}{Title - Top 50 Words}

\ref{fig:title-50-pi}
\csmfigure{title-50-pi}{figures/title-50-pi}{6.5in}{Title -- Corpus Distribution}

\newpage
\paragraph{LSI}
\ref{tab:lsi_title_tc}
\begin{table}
	\caption{\label{tab:lsi_title_tc} Title -- LSI Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{LSI} \\
			\emph{K} &   UMass &     UCI \\
			\midrule
			10  &  \textbf{-9.687} &  \textbf{-5.975} \\
			15  & -10.903 &  -7.084 \\
			20  & -12.441 &  -8.593 \\
			25  & -13.247 &  -9.334 \\
			30  & -12.343 &  -8.254 \\
			35  & -13.305 &  -9.381 \\
			40  & -13.581 &  -9.475 \\
			45  & -13.980 &  -9.874 \\
			50  & -14.417 & -10.234 \\
			55  & -14.632 & -10.434 \\
			60  & -15.160 & -10.886 \\
			65  & -15.409 & -11.101 \\
			70  & -15.473 & -11.114 \\
			75  & -15.886 & -11.498 \\
			80  & -16.046 & -11.632 \\
			85  & -15.921 & -11.504 \\
			90  & -16.202 & -11.714 \\
			95  & -16.366 & -11.847 \\
			100 & -16.500 & -11.922 \\
			\midrule
			$\sigma^2$ & 3.734 & 2.854 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}

\ref{tab:lsi_title_k}
\begin{table}
	\caption{\label{tab:lsi_title_k} Title -- LSI Generated Topics for $\emph{K} = 15$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                          \textbf{Keywords} \\
			\hline
			1  &                     survey, minutes, answer, short, study, opinions, minute, hit, consumer, min \\
			\hline
			2  &             minutes, survey, answer, study, hit, compensation, opinions, question, based, short \\
			\hline
			3  &  image, tagging, clickable, label, classification, categorization, single, drew, receipt, multi \\
			\hline
			4  &              answer, survey, questions, based, minute, question, opinions, hit, children, short \\
			\hline
			5  &                  short, study, hit, article, summary, compensation, il, survey, question, based \\
			\hline
			6  &             study, short, hit, compensation, article, summary, content, adult, decision, worker \\
			\hline
			7  &         study, hit, minutes, adult, content, worker, warning, discretion, advised, compensation \\
			\hline
			8  &            question, based, children, minute, nutritional, child, member, prime, amazon, answer \\
			\hline
			9  &             categorization, web, page, label, tagging, clickable, image, data, collect, contact \\
			\hline
			10 &                        fox, game, watch, superbowl, nfl, calculate, latency, sports, tv, online \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:lsi-umass-title}
\csmfigure{lsi-umass-title}{figures/lsi-umass-title}{5.5in}{Title -- LSI UMass Topic Coherence}

\ref{fig:lsi-uci-title}
\csmfigure{lsi-uci-title}{figures/lsi-uci-title}{5.5in}{Title -- LSI UCI Topic Coherence}
result of LSI on the number of topics, output the topics
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics
\newpage
\paragraph{LDA}
\ref{tab:lda_title_tc}
\begin{table}
	\caption{\label{tab:lda_title_tc} Title -- LDA Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{LDA} \\
			\emph{K} &   UMass &    UCI \\
			\midrule
			10  & -10.364 & -6.718 \\
			15  & \textbf{-10.228} & \textbf{-6.538} \\
			20  & -10.709 & -6.692 \\
			25  & -10.740 & -6.868 \\
			30  & -10.708 & \textbf{-6.569} \\
			35  & -11.444 & -7.125 \\
			40  & -11.344 & -7.071 \\
			45  & -11.450 & -7.126 \\
			50  & -11.988 & -7.488 \\
			55  & -11.665 & -7.217 \\
			60  & -11.961 & -7.291 \\
			65  & -12.617 & -7.851 \\
			70  & -13.407 & -8.348 \\
			75  & -13.113 & -8.078 \\
			80  & -13.244 & -8.077 \\
			85  & -12.938 & -7.647 \\
			90  & -12.812 & -7.394 \\
			95  & -12.608 & -7.185 \\
			100 & -12.657 & -7.005 \\
			\midrule
			$\sigma^2$ & 1.065 & 0.278 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}

\ref{tab:lda_title_k}
\begin{table}
	\caption{\label{tab:lda_title_k} Title -- LDA Generated Topics for $\emph{K} = 20$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                        \textbf{Keywords} \\
			\hline
			1  &       survey, answer, minute, minutes, short, attitudes, respondents, qualify, game, personal \\
			\hline
			2  &               estimate, project, ca, construction, minutes, fees, adu, permit, survey, mobile \\
			\hline
			3  &                     data, collect, information, contact, survey, voice, web, site, life, time \\
			\hline
			4  &  minutes, study, survey, research, online, product, evaluation, perceptions, short, attitudes \\
			\hline
			5  &            survey, answer, minutes, questions, opinion, give, instagram, company, tasks, find \\
			\hline
			6  &         minutes, survey, study, image, behavior, workplace, consumer, complete, images, short \\
			\hline
			7  &                article, short, summary, create, characters, words, story, il, text, companies \\
			\hline
			8  &       survey, label, personality, classification, minutes, short, visual, online, multi, text \\
			\hline
			9  &               question, survey, based, children, female, amazon, member, answer, prime, child \\
			\hline
			10 &                       survey, minutes, answer, short, video, min, hit, study, minute, service \\
			\hline
			11 &              test, marketing, minutes, bounding, boxes, draw, consumer, sentence, user, brand \\
			\hline
			12 &       categorization, page, web, image, minute, survey, upload, screencast, website, judgment \\
			\hline
			13 &                   survey, minutes, find, property, zoning, map, home, short, events, emotions \\
			\hline
			14 &                answer, survey, minutes, questions, media, study, type, business, social, read \\
			\hline
			15 &        decision, minutes, making, study, survey, min, understanding, psychology, quick, bonus \\
			\hline
			16 &              minutes, study, test, tts, relationships, part, image, clickable, tagging, alixr \\
			\hline
			17 &             hit, minutes, compensation, content, survey, worker, adult, easy, answer, advised \\
			\hline
			18 &  survey, answer, minutes, opinions, short, preferences, consumer, experience, questions, task \\
			\hline
			19 &        image, tagging, clickable, grocery, everyday, categories, data, news, climate, minutes \\
			\hline
			20 &                     minutes, short, survey, judgments, rate, answer, mins, design, make, test \\
			\hline
			\end{tabular}
	\end{center}
\end{table}

\ref{fig:lda-umass-title}
\csmfigure{lda-umass-title}{figures/lda-umass-title}{5.5in}{Title -- LDA UMass Topic Coherence}

\ref{fig:lda-uci-title}
\csmfigure{lda-uci-title}{figures/lda-uci-title}{5.5in}{Title -- LDA UCI Topic Coherence}
result of LDA on the number of topics, output the topics
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics
\newpage
\paragraph{lda2vec}

\ref{tab:lda2vec_title_tc}
\begin{table}
	\caption{\label{tab:lda2vec_title_tc} Title -- \emph{lda2vec} Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{lda2vec} \\
			\emph{K} &   UMass &    UCI \\
			\midrule
			10  &  -8.467 & -6.003 \\
			15  & -10.435 & -7.471 \\
			20  &  \textbf{-7.414} & \textbf{-4.765} \\
			25  &  -8.998 & -6.074 \\
			30  &  -7.937 & -5.026 \\
			35  & -10.015 & -7.245 \\
			40  &  \textbf{-7.339} & \textbf{-4.362} \\
			45  &  -8.253 & -5.227 \\
			50  &  -8.824 & -5.880 \\
			55  &  -8.331 & -5.158 \\
			60  &  -9.308 & -6.268 \\
			65  &  -8.696 & -5.493 \\
			70  & -10.347 & -7.495 \\
			75  &  -8.864 & -5.819 \\
			80  &  -8.117 & -5.212 \\
			85  &  -9.374 & -6.693 \\
			90  &  -9.924 & -7.191 \\
			95  &  -9.208 & -6.048 \\
			100 & -10.282 & -7.244 \\
			\midrule
			$\sigma^2$ & 0.903 & 0.936 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}


\ref{tab:lda2vec_title_k}
\begin{table}
	\caption{\label{tab:lda2vec_title_k} Title -- \emph{lda2vec} Generated Topics for $\emph{K} = 20$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                        \textbf{Keywords} \\
			\hline
				1  &        answer, survey, study, part, short, opinions, minutes, question, quick, work \\
				\hline
				2  &   answer, survey, opinions, question, short, study, part, quick, questions, minutes \\
				\hline
				3  &     answer, survey, study, opinions, short, part, question, minutes, quick, opinion \\
				\hline
				4  &     answer, opinions, study, short, question, survey, part, opinion, quick, minutes \\
				\hline
				5  &     answer, survey, opinions, study, question, part, short, quick, minutes, opinion \\
				\hline
				6  &        answer, opinions, survey, study, part, short, question, minutes, quick, work \\
				\hline
				7  &     answer, opinions, question, short, study, part, opinion, quick, minutes, survey \\
				\hline
				8  &  answer, opinions, short, study, part, question, quick, opinion, experience, survey \\
				\hline
				9  &        answer, opinions, short, study, part, question, survey, minutes, work, quick \\
				\hline
				10 &     answer, opinions, question, survey, short, study, part, quick, minutes, opinion \\
				\hline
				11 &     answer, opinions, study, part, short, survey, question, minutes, opinion, quick \\
				\hline
				12 &     answer, opinions, survey, study, question, short, part, quick, minutes, opinion \\
				\hline
				13 &        answer, opinions, survey, study, short, part, minutes, question, quick, work \\
				\hline
				14 &     answer, survey, opinions, study, question, short, part, quick, minutes, opinion \\
				\hline
				15 &     answer, opinions, survey, study, short, part, question, quick, minutes, opinion \\
				\hline
				16 &        answer, study, opinions, part, survey, short, quick, work, question, minutes \\
				\hline
				17 &     answer, opinions, study, survey, part, short, minutes, question, quick, opinion \\
				\hline
				18 &     answer, opinions, study, part, survey, short, question, quick, minutes, opinion \\
				\hline
				19 &   answer, opinions, survey, study, short, question, part, quick, opinion, questions \\
				\hline
				20 &     answer, opinions, survey, study, short, part, question, minutes, opinion, quick \\
				\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:lda2vec-umass-title}
\csmfigure{lda2vec-umass-title}{figures/lda2vec-umass-title}{5.5in}{Title -- \emph{lda2vec} UMass Topic Coherence}

\ref{fig:lda2vec-uci-title}
\csmfigure{lda2vec-uci-title}{figures/lda2vec-uci-title}{5.5in}{Title -- \emph{lda2vec} UCI Topic Coherence}
\newpage
result of Lda2vec on the number of topics, output the topics
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics

\paragraph{ETM}

\ref{tab:etm_title_tc}
\begin{table}
	\caption{\label{tab:etm_title_tc} Title -- ETM Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{ETM} \\
			\emph{K} &  UMass &    UCI \\
			\midrule
			10  & \textbf{-8.265} & \textbf{-5.273} \\
			15  & -9.573 & -6.428 \\
			20  & -9.846 & -6.576 \\
			25  & -9.464 & -6.280 \\
			30  & -9.132 & -6.043 \\
			35  & -9.474 & -6.392 \\
			40  & \textbf{-7.934} & \textbf{-5.010} \\
			45  & -8.366 & -5.353 \\
			50  & -8.853 & -5.888 \\
			55  & -8.901 & -5.886 \\
			60  & -9.555 & -6.461 \\
			65  & -8.659 & -5.651 \\
			70  & -9.415 & -6.341 \\
			75  & -8.808 & -5.822 \\
			80  & -9.650 & -6.488 \\
			85  & -9.124 & -6.102 \\
			90  & -9.846 & -6.690 \\
			95  & -8.814 & -5.773 \\
			100 & -9.672 & -6.582 \\
			\midrule
			$\sigma^2$ & 0.311 & 0.235 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}

\ref{tab:etm_title_k}
\begin{table}
	\caption{\label{tab:etm_title_k} Title -- ETM Generated Topics for $\emph{K} = 10$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                     \textbf{Keywords} \\
			\hline
			1  &  survey, study, research, minutes, questions, participate, complete, minute, people, short \\
			\hline			
			2  &                asked, images, series, shown, read, questions, opinion, page, short, answer \\
			\hline			
			3  &                 asked, page, questions, answer, study, short, survey, minutes, shown, read \\
			\hline			
			4  &  survey, people, study, minutes, research, participate, minute, complete, questions, short \\
			\hline			
			5  &              questions, answer, read, short, asked, people, survey, opinion, task, minutes \\
			\hline			
			6  &             questions, survey, short, answer, task, minutes, people, complete, study, read \\
			\hline			
			7  &              opinion, task, give, survey, questions, answer, minutes, minute, short, study \\
			\hline			
			8  &  survey, study, research, minutes, minute, people, participate, complete, questions, short \\
			\hline			
			9  &       study, task, complete, short, participate, minutes, read, research, questions, asked \\
			\hline			
			10 &              complete, hit, survey, minutes, study, short, task, minute, questions, people \\
			\hline			
		\end{tabular}
	\end{center}
\end{table}

talk about the performance of the ETM

\ref{fig:etm-umass-title}
\csmfigure{etm-umass-title}{figures/etm-umass-title}{5.5in}{Title -- ETM UMass Topic Coherence}

\ref{fig:etm-uci-title}
\csmfigure{etm-uci-title}{figures/etm-uci-title}{5.5in}{Title -- ETM UCI Topic Coherence}
\newpage
\paragraph{Topic Coherence}
Create a figure displaying the different topic coherence results
Explain the scores and what that means

\ref{tab:combo_title_tc}
\begin{table}
	\caption{\label{tab:combo_title_tc} Title -- Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{|l|rr|rr|rr|rr|}
			\hline
			{} & \multicolumn{2}{c|}{\textbf{LSI}} & \multicolumn{2}{c|}{\textbf{LDA}} & \multicolumn{2}{c|}{\textbf{lda2vec}} & \multicolumn{2}{c|}{\textbf{ETM}} \\
			\emph{K} &   UMass &     UCI &   UMass &    UCI &   UMass &    UCI &  UMass &    UCI \\
			\hline
			10  &  \textbf{-9.687} &  \textbf{-5.975} & -10.364 & -6.718 &  -8.467 & -6.003 & \textbf{-8.265} & \textbf{-5.273} \\
			15  & -10.903 &  -7.084 & \textbf{-10.228} & \textbf{-6.538} & -10.435 & -7.471 & -9.573 & -6.428 \\
			20  & -12.441 &  -8.593 & -10.709 & -6.692 & \textbf{-7.414} & \textbf{-4.765} & -9.846 & -6.576 \\
			25  & -13.247 &  -9.334 & -10.740 & -6.868 &  -8.998 & -6.074 & -9.464 & -6.280 \\
			30  & -12.343 &  -8.254 & -10.708 & \textbf{-6.569} &  -7.937 & -5.026 & -9.132 & -6.043 \\
			35  & -13.305 &  -9.381 & -11.444 & -7.125 & -10.015 & -7.245 & -9.474 & -6.392 \\
			40  & -13.581 &  -9.475 & -11.344 & -7.071 &  \textbf{-7.339} & \textbf{-4.362} & \textbf{-7.934} & \textbf{-5.010} \\
			45  & -13.980 &  -9.874 & -11.450 & -7.126 &  -8.253 & -5.227 & -8.366 & -5.353 \\
			50  & -14.417 & -10.234 & -11.988 & -7.488 &  -8.824 & -5.880 & -8.853 & -5.888 \\
			55  & -14.632 & -10.434 & -11.665 & -7.217 &  -8.331 & -5.158 & -8.901 & -5.886 \\
			60  & -15.160 & -10.886 & -11.961 & -7.291 &  -9.308 & -6.268 & -9.555 & -6.461 \\
			65  & -15.409 & -11.101 & -12.617 & -7.851 &  -8.696 & -5.493 & -8.659 & -5.651 \\
			70  & -15.473 & -11.114 & -13.407 & -8.348 & -10.347 & -7.495 & -9.415 & -6.341 \\
			75  & -15.886 & -11.498 & -13.113 & -8.078 &  -8.864 & -5.819 & -8.808 & -5.822 \\
			80  & -16.046 & -11.632 & -13.244 & -8.077 &  -8.117 & -5.212 & -9.650 & -6.488 \\
			85  & -15.921 & -11.504 & -12.938 & -7.647 &  -9.374 & -6.693 & -9.124 & -6.102 \\
			90  & -16.202 & -11.714 & -12.812 & -7.394 &  -9.924 & -7.191 & -9.846 & -6.690 \\
			95  & -16.366 & -11.847 & -12.608 & -7.185 &  -9.208 & -6.048 & -8.814 & -5.773 \\
			100 & -16.500 & -11.922 & -12.657 & -7.005 & -10.282 & -7.244 & -9.672 & -6.582 \\
			\hline
			$\sigma^2$ & 3.734 & 2.854 & 1.065 & 0.278 & 0.903 & 0.936 & 0.311 & 0.236 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\ref{fig:umass-title}
\csmfigure{umass-title}{figures/umass-title}{5.5in}{Title UMass Topic Coherence}

\ref{fig:uci-title}
\csmfigure{uci-title}{figures/uci-title}{5.5in}{Title UCI Topic Coherence}

\newpage

\subsubsection{HIT Descriptions}
Use this section to discuss the experiments run on the descriptions of HITS in the dataset

\paragraph{Corpus Statistics}
Total unique words 4229
Total words in corpus: 23767
\ref{tab:description_top_words}

\begin{table}
	\caption{\label{tab:description_top_words} Description -- Top 10 Words in Corpus}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Word} & \textbf{Count} \\
			\hline
			survey & 880 \\
			\hline
			study & 511 \\
			\hline
			questions & 455 \\
			\hline
			answer & 369 \\
			\hline
			minutes & 335 \\
			\hline
			asked & 298 \\
			\hline
			complete & 275 \\
			\hline
			short & 271 \\
			\hline
			opinion & 256 \\
			\hline
			give & 243 \\
			\hline
			\textbf{Unique Words} & 4229 \\
			\hline
			\textbf{Total Words} & 23767 \\
			\hline
			\textbf{Number of Documents} & 2744 \\
			\hline
			\textbf{Average Document Length (Characters)} & 69 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:description-50-bar}
\csmfigure{description-50-bar}{figures/description-50-bar}{6.5in}{Description - Top 50 Words}

\ref{fig:description-50-pi}
\csmfigure{description-50-pi}{figures/description-50-pi}{6.5in}{Description Corpus Distribution}

\newpage
\paragraph{LSI}
result of LSI on the number of topics, output the topics
\ref{tab:lsi_tc_scores}

\begin{table}
	\caption{\label{tab:lsi_tc_scores} Description -- LSI Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{LSI} \\
			\emph{K} &   UMass &    UCI \\
			\midrule
			10  &  -5.859 & -3.087 \\
			15  &  \textbf{-4.984} & \textbf{-2.546} \\
			20  &  -6.464 & -3.730 \\
			25  &  -5.371 & -2.800 \\
			30  &  -5.505 & -3.068 \\
			35  &  -6.197 & -3.892 \\
			40  &  -7.093 & -4.735 \\
			45  &  -7.416 & -5.037 \\
			50  &  -8.008 & -5.462 \\
			55  &  -8.439 & -5.753 \\
			60  &  -8.833 & -6.179 \\
			65  &  -8.871 & -6.191 \\
			70  &  -9.790 & -6.880 \\
			75  &  -9.749 & -6.873 \\
			80  & -10.414 & -7.567 \\
			85  & -10.697 & -7.795 \\
			90  & -10.873 & -7.913 \\
			95  & -11.292 & -8.359 \\
			100 & -11.395 & -8.325 \\
			\midrule
			$\sigma^2$ & 4.654 & 3.913 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}

Discuss topic coherence scores
\ref{tab:lsi_description_topics}
\begin{table}
	\caption{\label{tab:lsi_description_topics} Description -- LSI Generated Topics for $\emph{K} = 15$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                                \textbf{Keywords} \\
			\hline
			1  &               study, survey, complete, questions, minutes, short, answer, asked, task, participate \\
			\hline
			2  &  survey, study, answer, task, screener, questions, computer, participate, participant, interaction \\
			\hline
			3  &                      questions, asked, answer, survey, shown, series, images, click, red, enclosed \\
			\hline
			4  &                         questions, answer, asked, shown, images, click, box, red, enclosed, series \\
			\hline
			5  &                       page, extract, links, download, stream, http, html, cc, container, filecrypt \\
			\hline
			6  &       short, participant, interaction, screener, read, complete, study, questions, minutes, letter \\
			\hline
			7  &             opinion, give, questions, complete, products, answer, unique, experience, public, text \\
			\hline
			8  &                minutes, complete, part, survey, approximately, short, web, asked, interaction, hit \\
			\hline
			9  &                         web, page, minutes, content, complete, screen, shot, extract, asked, links \\
			\hline
			10 &                        task, study, screener, hit, people, complete, research, web, part, computer \\
			\hline
			11 &                        unique, people, complete, task, words, text, part, write, research, article \\
			\hline
			12 &                 part, complete, hit, minutes, people, unique, asked, task, approximately, research \\
			\hline
			13 &                             unique, write, text, article, accepted, words, people, task, hit, part \\
			\hline
			14 &              part, section, minutes, short, complete, receive, study, link, participate, questions \\
			\hline
			15 &                 hit, research, section, university, people, complete, bonus, romantic, asked, task \\
			\hline
			\end{tabular}
	\end{center}
\end{table}

\ref{fig:lsi-umass-description}
\ref{fig:lsi-uci-description}
\csmfigure{lsi-umass-description}{figures/lsi-umass-description}{5.5in}{Description -- LSI UMass Topic Coherence}

\csmfigure{lsi-uci-description}{figures/lsi-uci-description}{5.5in}{Description -- LSI UCI Topic Coherence}

% lsi-umass-description.png
\newpage
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics

\paragraph{LDA}

\ref{tab:lda_description_tc}
\begin{table}
	\caption{\label{tab:lda_description_tc} Description -- LDA Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{LDA} \\
			\emph{K} &  UMass &    UCI \\
			\midrule
			10  & -5.029 & \textbf{-2.108} \\
			15  & \textbf{-4.982} & -2.166 \\
			20  & -5.500 & -2.685 \\
			25  & -6.044 & -3.063 \\
			30  & -5.728 & -2.860 \\
			35  & -6.394 & -3.325 \\
			40  & -6.402 & -3.462 \\
			45  & -6.353 & -3.189 \\
			50  & -7.371 & -4.098 \\
			55  & -7.116 & -3.905 \\
			60  & -7.108 & -3.846 \\
			65  & -7.067 & -3.781 \\
			70  & -7.887 & -4.547 \\
			75  & -7.675 & -4.205 \\
			80  & -7.919 & -4.410 \\
			85  & -8.869 & -5.257 \\
			90  & -8.374 & -4.769 \\
			95  & -8.354 & -4.755 \\
			100 & -9.127 & -5.264 \\
			\midrule
			$\sigma^2$ & 1.549 & 0.901 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}

\ref{tab:lda_description_topics}
\begin{table}
	\caption{\label{tab:lda_description_topics} Description -- LDA Generated Topics for $\emph{K} = 15$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                   \textbf{Keywords} \\
			\hline
			1  &          questions, answer, survey, short, minutes, read, study, complete, online, asked \\
			\hline
			2  &            survey, study, minute, people, interested, box, test, draw, minutes, bounding \\
			\hline
			3  &         study, survey, minutes, complete, research, short, making, decision, part, asked \\
			\hline
			4  &                    opinion, give, page, extract, links, download, http, html, stream, cc \\
			\hline
			5  &        study, survey, opinion, give, asked, part, complete, research, minutes, behaviors \\
			\hline
			6  &    survey, questions, answer, opinions, minutes, video, complete, product, read, article \\
			\hline
			7  &         survey, short, minute, consumer, task, minutes, complete, based, question, study \\
			\hline
			8  &   collect, study, data, survey, company, marketing, advertising, messages, select, label \\
			\hline
			9  &                     unique, web, page, text, asked, write, words, survey, shown, content \\
			\hline
			10 &                   images, asked, click, series, shown, box, enclosed, red, survey, image \\
			\hline
			11 &  survey, questions, answer, study, social, complete, minutes, information, issues, asked \\
			\hline
			12 &            survey, study, extract, social, media, page, download, links, https, complete \\
			\hline
			13 &      questions, answer, survey, read, give, opinion, asked, scenario, short, personality \\
			\hline
			14 &        survey, study, minutes, questions, short, asked, make, evaluate, products, choice \\
			\hline
			15 &      study, task, survey, participate, read, perceptions, rate, social, question, minute \\
			\hline
			\end{tabular}
	\end{center}
\end{table}


result of LDA on the number of topics, output the topics
\ref{fig:lda-umass-description}
\ref{fig:lda-uci-description}

\csmfigure{lda-umass-description}{figures/lda-umass-description}{5.5in}{Description -- LDA UMass Topic Coherence}
lorem ipsum

lorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsum
lorem ipsum
\csmfigure{lda-uci-description}{figures/lda-uci-description}{5.5in}{Description -- LDA UCI Topic Coherence}
\newpage
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics

\paragraph{lda2vec}

\ref{tab:lda2vec_description_tc}
\begin{table}
	\caption{\label{tab:lda2vec_description_tc} Description -- \emph{lda2vec} Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
				{} & \multicolumn{2}{l}{lda2vec} \\
				\emph{K} &   UMass &    UCI \\
				\midrule
				10  &  \textbf{-8.102} & \textbf{-5.023} \\
				15  &  -8.545 & -5.307 \\
				20  & -10.735 & -7.396 \\
				25  & -10.371 & -7.084 \\
				30  &  -9.153 & -6.115 \\
				35  &  -8.889 & -6.156 \\
				40  &  -8.641 & -5.582 \\
				45  & -10.986 & -7.708 \\
				50  &  -9.806 & -6.577 \\
				55  &  -9.597 & -6.652 \\
				60  &  -9.469 & -6.379 \\
				65  &  -9.580 & -6.328 \\
				70  &  -9.548 & -6.319 \\
				75  &  -9.674 & -6.429 \\
				80  &  -9.204 & -5.955 \\
				85  &  -9.621 & -6.507 \\
				90  &  -9.705 & -6.406 \\
				95  &  -9.562 & -6.425 \\
				100 &  -9.562 & -6.260 \\
			\midrule
			$\sigma^2$ & 0.493 & 0.409 \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}
result of LSI on the number of topics, output the topics

\ref{tab:lda2vec_description_topics}
\begin{table}
	\caption{\label{tab:lda2vec_description_topics} Description -- \emph{lda2vec} Generated Topics for $\emph{K} = 10$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                             \textbf{Keywords} \\
			\hline
			1  &      participants, study, time, understand, making, part, interested, unique, participation, takes \\
			\hline
			2  &   making, completing, participants, study, part, participation, takes, complete, participate, make \\
			\hline
			3  &                            unique, shown, words, write, give, enclosed, red, text, article, stream \\
			\hline
			4  &           takes, participants, time, answer, view, part, scenario, participation, completing, read \\
			\hline
			5  &                          unique, words, collect, find, website, give, text, write, stream, opinion \\
			\hline
			6  &                           unique, website, collect, find, words, stream, shown, text, page, search \\
			\hline
			7  &  part, takes, participants, participation, time, study, understand, interested, making, completing \\
			\hline
			8  &       part, takes, participants, completing, participation, time, complete, making, qualify, study \\
			\hline
			9  &                                shown, enclosed, click, red, page, box, screen, images, web, stream \\
			\hline
			10 &                   unique, shown, give, words, article, website, opinion, content, find, understand \\
			\hline
			\end{tabular}
	\end{center}
\end{table}
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics
\ref{fig:lda2vec-umass-description}
\csmfigure{lda2vec-umass-description}{figures/lda2vec-umass-description}{5.5in}{Description -- \emph{lda2vec} UMass Topic Coherence}

\ref{fig:lda2vec-uci-description}
\csmfigure{lda2vec-uci-description}{figures/lda2vec-uci-description}{5.5in}{Description \emph{lda2vec} UCI Topic Coherence}
\newpage
\paragraph{ETM}

\ref{tab:etm_description_tc}
\begin{table}
	\caption{\label{tab:etm_description_tc} Description -- ETM Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{ETM} \\
			\emph{K} &  UMass &    UCI \\
			\midrule
			10  & -5.536 & -2.517 \\
			15  & -5.176 & -2.330 \\
			20  & \textbf{-4.523} & \textbf{-1.583} \\
			25  & -5.693 & -2.775 \\
			30  & -5.538 & -2.654 \\
			35  & -5.352 & -2.333 \\
			40  & -5.591 & -2.603 \\
			45  & -5.183 & -2.271 \\
			50  & -5.183 & -2.259 \\
			55  & -5.209 & -2.361 \\
			60  & \textbf{-4.529} & \textbf{-1.724} \\
			65  & -5.019 & -2.093 \\
			70  & -5.053 & -2.057 \\
			75  & -5.268 & -2.308 \\
			80  & -5.130 & -2.200 \\
			85  & -5.356 & -2.490 \\
			90  & -5.104 & -2.225 \\
			95  & -5.386 & -2.530 \\
			100 & -5.119 & -2.282 \\
			\midrule
			$\sigma^2$ & 0.094 & 0.086 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}


\ref{tab:etm_description_k}
\begin{table}
	\caption{\label{tab:etm_description_k} Description -- ETM Generated Topics for $\emph{K} = 20$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                             \textbf{Keywords} \\
			\hline
			1  &           study, task, bonus, complete, questions, minutes, survey, participants, participate, min \\
			\hline
			2  &        complete, part, minutes, study, survey, social, approximately, academic, participate, asked \\
			\hline
			3  &          questions, answer, asked, read, short, personality, related, minutes, series, perceptions \\
			\hline
			4  &                           short, video, questions, answer, make, minutes, write, asked, task, read \\
			\hline
			5  &                               asked, images, series, click, red, enclosed, web, page, shown, based \\
			\hline
			6  &                     links, download, unique, stream, text, article, words, extract, related, write \\
			\hline
			7  &  survey, hit, consumer, attitudes, social, opinions, preferences, minute, perceptions, experiences \\
			\hline
			8  &       survey, hit, minute, experiences, opinions, attitudes, consumer, opinion, social, experience \\
			\hline
			9  &                       opinion, product, products, image, person, find, rate, based, give, evaluate \\
			\hline
			10 &         survey, hit, social, minute, consumer, attitudes, opinions, preferences, perceptions, rate \\
			\hline
			11 &               study, short, participate, complete, read, minutes, experience, social, asked, write \\
			\hline
			12 &        study, people, research, make, understand, interested, asked, participate, opinions, survey \\
			\hline
			13 &             questions, answer, read, short, personality, asked, series, perceptions, related, work \\
			\hline
			14 &                       opinion, person, image, find, products, rate, product, give, based, evaluate \\
			\hline
			15 &                           task, data, information, collect, based, time, work, bonus, make, person \\
			\hline
			16 &           survey, minute, hit, attitudes, social, opinions, experiences, consumer, min, experience \\
			\hline
			17 &        survey, online, complete, behaviors, asked, related, minutes, takes, participants, evaluate \\
			\hline
			18 &          minutes, takes, decision, approximately, making, survey, academic, asked, evaluate, study \\
			\hline
			19 &        study, people, research, understand, asked, participate, interested, survey, make, opinions \\
			\hline
			20 &          questions, answer, read, short, personality, related, asked, perceptions, minutes, series \\
			\hline
			\end{tabular}
	\end{center}
\end{table}
talk about the performance of the ETM
\ref{fig:etm-umass-description}
\csmfigure{etm-umass-description}{figures/etm-umass-description}{5.5in}{Description -- ETM UMass Topic Coherence}

\ref{fig:etm-uci-description}
\csmfigure{etm-uci-description}{figures/etm-uci-description}{5.5in}{Description -- ETM UCI Topic Coherence}
\newpage

\paragraph{Topic Coherence}
Create a figure displaying the different topic coherence results
Explain the scores and what that means

\ref{tab:combo_description_tc}
\begin{table}
	\caption{\label{tab:combo_description_tc} Description -- Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{|l|rr|rr|rr|rr|}
			\hline
			{} & \multicolumn{2}{c|}{\textbf{LSI}} & \multicolumn{2}{c|}{\textbf{LDA}} & \multicolumn{2}{c|}{\textbf{lda2vec}} & \multicolumn{2}{c|}{\textbf{ETM}} \\
			\emph{K} &   UMass &    UCI &  UMass &    UCI &   UMass &    UCI &  UMass &    UCI \\
			\hline
			10  &  -5.859 & -3.087 & -5.029 & \textbf{-2.108} &  \textbf{-8.102} & \textbf{-5.023} & -5.536 & -2.517 \\
			15  &  \textbf{-4.984} & \textbf{-2.546} & \textbf{-4.982} & -2.166 &  -8.545 & -5.307 & -5.176 & -2.330 \\
			20  &  -6.464 & -3.730 & -5.500 & -2.685 & -10.735 & -7.396 & \textbf{-4.523} & \textbf{-1.583} \\
			25  &  -5.371 & -2.800 & -6.044 & -3.063 & -10.371 & -7.084 & -5.693 & -2.775 \\
			30  &  -5.505 & -3.068 & -5.728 & -2.860 &  -9.153 & -6.115 & -5.538 & -2.654 \\
			35  &  -6.197 & -3.892 & -6.394 & -3.325 &  -8.889 & -6.156 & -5.352 & -2.333 \\
			40  &  -7.093 & -4.735 & -6.402 & -3.462 &  -8.641 & -5.582 & -5.591 & -2.603 \\
			45  &  -7.416 & -5.037 & -6.353 & -3.189 & -10.986 & -7.708 & -5.183 & -2.271 \\
			50  &  -8.008 & -5.462 & -7.371 & -4.098 &  -9.806 & -6.577 & -5.183 & -2.259 \\
			55  &  -8.439 & -5.753 & -7.116 & -3.905 &  -9.597 & -6.652 & -5.209 & -2.361 \\
			60  &  -8.833 & -6.179 & -7.108 & -3.846 &  -9.469 & -6.379 & -4.529 & -1.724 \\
			65  &  -8.871 & -6.191 & -7.067 & -3.781 &  -9.580 & -6.328 & -5.019 & -2.093 \\
			70  &  -9.790 & -6.880 & -7.887 & -4.547 &  -9.548 & -6.319 & -5.053 & -2.057 \\
			75  &  -9.749 & -6.873 & -7.675 & -4.205 &  -9.674 & -6.429 & -5.268 & -2.308 \\
			80  & -10.414 & -7.567 & -7.919 & -4.410 &  -9.204 & -5.955 & -5.130 & -2.200 \\
			85  & -10.697 & -7.795 & -8.869 & -5.257 &  -9.621 & -6.507 & -5.356 & -2.490 \\
			90  & -10.873 & -7.913 & -8.374 & -4.769 &  -9.705 & -6.406 & -5.104 & -2.225 \\
			95  & -11.292 & -8.359 & -8.354 & -4.755 &  -9.562 & -6.425 & -5.386 & -2.530 \\
			100 & -11.395 & -8.325 & -9.127 & -5.264 &  -9.562 & -6.260 & -5.119 & -2.282 \\
			\hline
			$\sigma^2$ & 4.654 & 3.913 & 1.549 & 0.901 & 0.493 & 0.409 & 0.094 & 0.086 \\ 
			\hline
			\end{tabular}
	\end{center}
\end{table}

\ref{fig:umass-description}
\csmfigure{umass-description}{figures/umass-description}{5.5in}{Description -- UMass Topic Coherence}

\ref{fig:uci-description}
\csmfigure{uci-description}{figures/uci-description}{5.5in}{Description -- UCI Topic Coherence}

\newpage

\paragraph{Discussion}

Discuss the findings and the topics found in the description

Best scoring topic coherence \ref{tab:etm_description_k} \ref{tab:etm_description_tc}


\subsubsection{HIT Preview}
Use this section to discuss the experiments run on the descriptions of HITS in the dataset

\paragraph{Corpus Statistics}
Total unique words 22016
Total words in corpus: 518817
\ref{tab:preview_top_words}

\begin{table}
	\caption{\label{tab:preview_top_words} Preview -- Top 10 Words in Corpus}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Word} & \textbf{Count} \\
			\hline
			sentence & 9038 \\
			\hline
			answer & 7135 \\
			\hline
			original & 5475 \\
			\hline
			receipt & 5121 \\
			\hline
			hit & 3627 \\
			\hline
			text & 3144 \\
			\hline
			brand & 2933 \\
			\hline
			note & 2798 \\
			\hline
			product & 2797 \\
			\hline
			data & 2671 \\
			\hline
			\textbf{Unique Words} & 22016 \\
			\hline
			\textbf{Total Words} & 518817 \\
			\hline
			\textbf{Number of Documents} & 1639 \\
			\hline
			\textbf{Average Document Length (Characters)} & 2349 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:preview-50-bar}
\csmfigure{preview-50-bar}{figures/preview-50-bar}{6.5in}{Preview - Top 50 Words}

\ref{fig:preview-50-pi}
\csmfigure{preview-50-pi}{figures/preview-50-pi}{6.5in}{Preview -- Corpus Distribution}

\newpage
\paragraph{LSI}

\ref{tab:lsi_preview_tc}
\begin{table}
	\caption{\label{tab:lsi_preview_tc} Preview -- LSI Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{LSI} \\
			\emph{K} &  UMass &    UCI \\
			\midrule
			10  & \textbf{-4.915} & -5.294 \\
			15  & -5.281 & -6.560 \\
			20  & -5.217 & -6.153 \\
			25  & \textbf{-4.959} & -5.652 \\
			30  & -5.604 & -5.834 \\
			35  & -5.375 & -5.271 \\
			40  & -5.096 & \textbf{-5.069} \\
			45  & -5.159 & -5.250 \\
			50  & -5.522 & -5.594 \\
			55  & -5.979 & -5.909 \\
			60  & -6.064 & -6.290 \\
			65  & -6.295 & -6.281 \\
			70  & -6.527 & -6.736 \\
			75  & -6.675 & -6.876 \\
			80  & -6.353 & -6.661 \\
			85  & -6.700 & -6.918 \\
			90  & -7.018 & -7.084 \\
			95  & -7.038 & -7.049 \\
			100 & -7.183 & -7.375 \\
			\midrule
			$\sigma^2$ & 0.588 & 0.514 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}


\ref{tab:lsi_preview_k}
\begin{table}
	\caption{\label{tab:lsi_preview_k} Preview -- LSI Generated Topics for $\emph{K} = 25$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
				\textbf{Topic} &                                                                                               \textbf{Keywords} \\
				\hline
				1  &                           info, profile, text, copy, appears, company, linkedin, link, school, click \\
				\hline
				2  &                       answer, original, kiwi, botanicals, sentence, shower, gel, casing, brand, hair \\
				\hline
				3  &                sentence, original, number, answer, kiwi, botanicals, shower, gel, choose, population \\
				\hline
				4  &                     agree, disagree, strongly, slightly, book, mother, kiwi, botanicals, shower, gel \\
				\hline
				5  &                     agree, disagree, mother, strongly, slightly, kiwi, botanicals, shower, gel, book \\
				\hline
				6  &                       book, mother, receipt, hair, kiwi, botanicals, gel, shower, fictional, journal \\
				\hline
				7  &                            receipt, book, journal, data, blank, bird, fictional, item, total, number \\
				\hline
				8  &                 preview, receipt, accept, qualify, years, rate, requester, book, information, review \\
				\hline
				9  &                 answer, mother, kiwi, botanicals, hair, gel, shower, question, acceptable, deodorant \\
				\hline
				10 &                       blank, unclear, rotate, zoom, instructions, receipt, special, ctrl, work, page \\
				\hline
				11 &                    amazon, bird, journal, book, birdwatching, watching, notebook, select, post, gift \\
				\hline
				12 &                  answer, acceptable, question, bad, mother, deodorant, casing, ivory, medical, scent \\
				\hline
				13 &                     medical, bird, records, birdwatching, health, amazon, watching, pet, record, dog \\
				\hline
				14 &                 sale, store, business, data, company, estate, items, categories, traditional, number \\
				\hline
				15 &                          company, website, amazon, correct, country, find, cote, rica, sale, product \\
				\hline
				16 &                       sale, amazon, data, chromatography, medical, age, war, bronze, item, fictional \\
				\hline
				17 &                sale, amazon, data, medical, chromatography, positivity, bird, leave, item, fictional \\
				\hline
				18 &  positivity, journal, bird, amazon, chromatography, gratitude, birdwatching, age, watching, positive \\
				\hline
				19 &                             skill, amazon, skills, product, person, aws, linkedin, click, info, book \\
				\hline
				20 &                              amazon, skill, book, product, aws, page, post, select, fictional, guide \\
				\hline
				21 &                   product, amazon, print, variations, miu, title, products, book, brand, description \\
				\hline
				22 &              chromatography, age, bronze, war, analysis, methods, analytical, gas, chemistry, liquid \\
				\hline
				23 &               fish, identification, age, bronze, species, guide, chromatography, fishes, reef, field \\
				\hline
				24 &                page, invoice, question, count, item, acceptable, questions, amount, multiple, birite \\
				\hline
				25 &                        question, page, invoice, acceptable, questions, bad, count, prompt, war, step \\
				\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:lsi-umass-preview}
\csmfigure{lsi-umass-preview}{figures/lsi-umass-preview}{5.5in}{Preview -- LSI UMass Topic Coherence}

\ref{fig:lsi-uci-preview}
\csmfigure{lsi-uci-preview}{figures/lsi-uci-preview}{5.5in}{Preview -- LSI UCI Topic Coherence}
\newpage


result of LSI on the number of topics, output the topics
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics

\paragraph{LDA}

\ref{tab:lda_preview_tc}
\begin{table}
	\caption{\label{tab:lda_preview_tc} Preview -- LDA Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{LDA} \\
			\emph{K} &  UMass &    UCI \\
			\midrule
			10  & \textbf{-1.904} & \textbf{-1.757} \\
			15  & \textbf{-1.858} & -2.123 \\
			20  & -2.547 & -2.142 \\
			25  & -2.625 & -2.757 \\
			30  & -2.997 & -2.751 \\
			35  & -3.200 & -3.149 \\
			40  & -2.439 & -2.594 \\
			45  & -2.460 & -2.254 \\
			50  & -2.774 & -2.194 \\
			55  & -2.682 & -2.447 \\
			60  & -3.013 & -2.497 \\
			65  & -3.386 & -2.461 \\
			70  & -2.812 & -2.066 \\
			75  & -3.003 & \textbf{-1.707} \\
			80  & -3.039 & -2.335 \\
			85  & -2.800 & \textbf{-1.859} \\
			90  & -3.130 & -2.161 \\
			95  & -3.102 & -2.224 \\
			100 & -3.094 & -2.189 \\
			\midrule
			$\sigma^2$ & 0.167 & 0.128 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}


\ref{tab:lda_preview_k}
\begin{table}
	\caption{\label{tab:lda_preview_k} Preview -- LDA Generated Topics for $\emph{K} = 15$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                            \textbf{Keywords} \\
			\hline
			1  &              answer, acceptable, bad, good, question, answers, content, advice, people, president \\
			\hline
			2  &                    answer, casing, original, brand, kiwi, spelling, botanicals, edit, shower, gel \\
			\hline
			3  &                         agree, disagree, item, strongly, slightly, class, type, text, select, hit \\
			\hline
			4  &                       blank, unclear, zoom, rotate, instructions, special, work, text, ctrl, page \\
			\hline
			5  &                    hit, answer, user, book, agent, text, journal, submit, instructions, responses \\
			\hline
			6  &               sale, article, business, store, paste, title, body, estate, categories, traditional \\
			\hline
			7  &  sentence, original, number, choose, population, universities, university, york, harvard, meaning \\
			\hline
			8  &                          receipt, data, item, reject, match, select, readable, side, left, fields \\
			\hline
			9  &                               mother, hair, shampoo, dry, spray, beach, babe, product, edit, care \\
			\hline
			10 &                          phone, number, email, address, company, find, link, website, https, role \\
			\hline
			11 &              hit, survey, preview, questions, accept, work, answers, information, review, payment \\
			\hline
			12 &                   company, book, amazon, website, find, country, bird, fictional, correct, number \\
			\hline
			13 &          results, veterinary, names, ophthalmology, summary, domain, search, group, workers, link \\
			\hline
			14 &                     receipt, question, answer, store, number, leave, blank, present, total, items \\
			\hline
			15 &                        profile, info, text, copy, linkedin, appears, skill, company, link, school \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:lda-umass-preview}
\csmfigure{lda-umass-preview}{figures/lda-umass-preview}{5.5in}{Preview -- LDA UMass Topic Coherence}

\ref{fig:lda-uci-preview}
\csmfigure{lda-uci-preview}{figures/lda-uci-preview}{5.5in}{Preview -- LDA UCI Topic Coherence}
\newpage

result of LDA on the number of topics, output the topics
visualization of document topic distribution
word clouds of top topics

Observations and analysis on the topics

\paragraph{lda2vec}
\ref{tab:lda2vec_preview_tc}
\begin{table}
	\caption{\label{tab:lda2vec_preview_tc} Preview -- \emph{lda2vec} Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{lda2vec} \\
			\emph{K} &   UMass &     UCI \\
			\midrule
			10  & -16.717 & -13.474 \\
			15  & -15.109 & -13.119 \\
			20  & -15.036 & -12.749 \\
			25  & \textbf{-13.695} & -12.317 \\
			30  & -14.548 & -12.174 \\
			35  & -15.275 & -12.683 \\
			40  & -14.925 & -12.061 \\
			45  & -15.067 & -12.288 \\
			50  & -14.713 & -12.002 \\
			55  & -14.663 & -12.104 \\
			60  & -13.743 & -11.530 \\
			65  & -14.251 & -11.848 \\
			70  & -14.280 & -11.514 \\
			75  & \textbf{-13.646} & \textbf{-11.191} \\
			80  & -13.932 & -11.564 \\
			85  & -13.891 & -11.327 \\
			90  & -14.263 & -11.901 \\
			95  & -13.801 & -11.483 \\
			100 & -14.893 & -11.770 \\
			\midrule
			$\sigma^2$ & 0.167 & 0.128 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}


\ref{tab:lda2vec_preview_k}
\begin{table}
	\caption{\label{tab:lda2vec_preview_k} Preview -- \emph{lda2vec} Generated Topics for $\emph{K} = 20$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
				\textbf{Topic} &                                                                                                       \textbf{Keywords} \\
				\hline
				1  &                               postal, foreign, fuel, threat, dislike, rude, shampoo, moral, unclear, smarter \\
				\hline
				2  &                 trk, bloggers, hairstylist, fendi, influencers, tagline, logomania, instagram, tina, youtube \\
				\hline
				3  &     tease, hairbrush, hairstylist, wording, masculine, exaggeration, husbands, clothe, casserole, intolerant \\
				\hline
				4  &      botanicals, purifying, kiwi, hairstylist, familiarize, consignment, gel, blacked, paraphrase, hairbrush \\
				\hline
				5  &  insensitive, exaggeration, judgmental, intolerant, prejudiced, muslim, violent, offense, imminent, intruder \\
				\hline
				6  &                   wording, ownership, babe, anomalies, diverging, bananas, approval, feeds, omnipresent, bal \\
				\hline
				7  &      sweet, innovative, hairbrush, transcribed, snack, polarity, acceptable, compared, responses, reputation \\
				\hline
				8  &                          stalin, tangle, joseph, babe, jesus, enjoys, perfector, violates, retailer, cashier \\
				\hline
				9  &                                typos, coherent, insole, summary, mistakes, kiwi, neck, jacobs, heel, branded \\
				\hline
				10 &                accurate, tina, unprovable, moderate, insult, chooses, alterations, copied, nonsense, spanish \\
				\hline
				11 &                       purifying, excluding, botanicals, consignment, nedi, pickup, kiwi, thrift, pawn, panel \\
				\hline
				12 &                       requesters, sr, jacobs, sporting, homage, exclamation, slip, spice, masculine, cashier \\
				\hline
				13 &                       brainstorming, straeter, endtime, utc, damon, fine, buttons, responds, donald, proceed \\
				\hline
				14 &     overlaps, sampled, consolidating, semantics, opting, refined, hairbrush, exclamation, annotator, violate \\
				\hline
				15 &                              cent, links, subsidiary, evolved, thrift, gmbh, parent, matched, stating, typos \\
				\hline
				16 &                allotted, brunch, dashboard, punctuated, panel, nedi, unavailable, zoomed, remails, purifying \\
				\hline
				17 &                                   cure, cloquet, spied, potato, tangle, miu, decimals, feeds, aids, conducts \\
				\hline
				18 &                        corp, headquarters, latch, logo, jacobs, sweatshirt, pumps, typos, acceptable, insole \\
				\hline
				19 &                          link, completion, www, register, matched, billings, requesters, survey, hitid, http \\
				\hline
				20 &                       reo, craigslist, instagram, hairstylist, babe, outdated, cardshark, thrift, blog, html \\
				\hline
			\end{tabular}
	\end{center}
\end{table}
result of Lda2vec on the number of topics, output the topics\
visualization of document topic distribution\
word clouds of top topics\
\
bservations and analysis on the topics

To run the \emph{lda2vec} model through the topic coherence metrics a post-processing step was needed\
to remove unicode characters that didn't appear in the \emph{gensim} corpus.

\paragraph{ETM}

\ref{tab:etm_preview_tc}
\begin{table}
	\caption{\label{tab:etm_preview_tc} Preview -- ETM Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{lrr}
			\toprule
			{} & \multicolumn{2}{l}{ETM} \\
			\emph{K} &  UMass &    UCI \\
			\midrule
			10  & \textbf{-1.025} & \textbf{-0.683} \\
			15  & -1.170 & -1.047 \\
			20  & -1.290 & \textbf{-0.839} \\
			25  & -1.284 & -1.009 \\
			30  & -1.374 & -1.617 \\
			35  & -1.385 & -1.648 \\
			40  & -1.418 & -1.702 \\
			45  & -1.447 & -1.812 \\
			50  & -1.448 & -1.961 \\
			55  & -1.474 & -2.083 \\
			60  & -1.470 & -1.938 \\
			65  & -1.471 & -1.948 \\
			70  & -1.518 & -2.013 \\
			75  & -1.587 & -2.375 \\
			80  & -1.559 & -2.333 \\
			85  & -1.582 & -2.487 \\
			90  & -1.539 & -2.288 \\
			95  & -1.538 & -2.352 \\
			100 & -1.510 & -2.523 \\
			\midrule
			$\sigma^2$ & 0.021 & 0.32 \\
			\bottomrule
			\end{tabular}
	\end{center}
\end{table}

\ref{tab:etm_preview_k}
\begin{table}
	\caption{\label{tab:etm_preview_k} Preview -- ETM Generated Topics for $\emph{K} = 20$}
	\begin{center}
		\begin{tabular}{| l | L |}
			\hline
			\textbf{Topic} &                                                                                              \textbf{Keywords} \\
			\hline
			1  &  sentence, original, number, population, university, universities, choose, york, harvard, sentences \\
			\hline			
			2  &                                   person, select, click, full, https, find, post, www, data, search \\
			\hline			
			3  &                          answer, casing, spelling, brand, original, edit, hair, scent, note, proper \\
			\hline			
			4  &                                receipt, leave, blank, number, total, items, id, valid, choose, code \\
			\hline			
			5  &                       company, website, number, phone, find, address, country, email, correct, link \\
			\hline			
			6  &                     answer, acceptable, bad, good, question, content, answers, people, marked, rate \\
			\hline			
			7  &                          image, option, read, submit, task, work, javascript, button, images, table \\
			\hline			
			8  &              product, brand, description, information, products, title, design, images, long, match \\
			\hline			
			9  &                            receipt, data, item, reject, left, match, readable, fields, select, side \\
			\hline			
			10 &                              blank, unclear, zoom, rotate, work, special, text, ctrl, copy, picture \\
			\hline			
			11 &                preview, accept, review, information, rate, requester, years, average, payment, work \\
			\hline			
			12 &                         hits, information, writing, world, report, work, time, guide, meaning, text \\
			\hline			
			13 &                             responses, time, change, check, begin, work, questions, day, good, make \\
			\hline			
			14 &                            item, page, type, price, select, find, search, results, amount, quantity \\
			\hline			
			15 &                amazon, business, text, group, categories, service, english, customer, online, legal \\
			\hline			
			16 &                                profile, text, copy, appears, company, link, click, work, years, job \\
			\hline			
			17 &                                  article, paste, title, copy, body, link, form, text, click, titles \\
			\hline			
			18 &                               survey, link, code, provide, complete, end, box, paste, receive, open \\
			\hline			
			19 &             question, answers, questions, answer, task, sentence, written, work, identify, multiple \\
			\hline			
			20 &                               answer, words, people, don, correct, write, word, based, submit, read \\
			\hline			
		\end{tabular}
	\end{center}
\end{table}

\ref{fig:etm-umass-preview}
\csmfigure{etm-umass-preview}{figures/etm-umass-preview}{5.5in}{Preview -- ETM UMass Topic Coherence}

\ref{fig:etm-uci-preview}
\csmfigure{etm-uci-preview}{figures/etm-uci-preview}{5.5in}{Preview -- ETM UCI Topic Coherence}
\newpage
talk about the performance of the ETM

\paragraph{Topic Coherence}
Create a figure displaying the different topic coherence results
Explain the scores and what that means
\ref{tab:preview_tc}
\begin{table}
	\caption{\label{tab:preview_tc} Preview -- Topic Coherence Scores for \emph{K} Topics}
	\begin{center}
		\begin{tabular}{| l |rr|rr|rr|rr|}
			\hline
			{} & \multicolumn{2}{c|}{\textbf{LSI}} & \multicolumn{2}{c|}{\textbf{LDA}} & \multicolumn{2}{c|}{\textbf{lda2vec}} & \multicolumn{2}{c|}{\textbf{ETM}} \\
			\emph{K} &  UMass &    UCI &  UMass &    UCI &   UMass &     UCI &  UMass &    UCI \\
			\hline
			10  & -4.915 & -5.294 & -1.904 & -1.757 & -16.717 & -13.474 & -1.025 & -0.683 \\
			15  & -5.281 & -6.560 & -1.858 & -2.123 & -15.109 & -13.119 & -1.170 & -1.047 \\
			20  & -5.217 & -6.153 & -2.547 & -2.142 & -15.036 & -12.749 & -1.290 & -0.839 \\
			25  & -4.959 & -5.652 & -2.625 & -2.757 & -13.695 & -12.317 & -1.284 & -1.009 \\
			30  & -5.604 & -5.834 & -2.997 & -2.751 & -14.548 & -12.174 & -1.374 & -1.617 \\
			35  & -5.375 & -5.271 & -3.200 & -3.149 & -15.275 & -12.683 & -1.385 & -1.648 \\
			40  & -5.096 & -5.069 & -2.439 & -2.594 & -14.925 & -12.061 & -1.418 & -1.702 \\
			45  & -5.159 & -5.250 & -2.460 & -2.254 & -15.067 & -12.288 & -1.447 & -1.812 \\
			50  & -5.522 & -5.594 & -2.774 & -2.194 & -14.713 & -12.002 & -1.448 & -1.961 \\
			55  & -5.979 & -5.909 & -2.682 & -2.447 & -14.663 & -12.104 & -1.474 & -2.083 \\
			60  & -6.064 & -6.290 & -3.013 & -2.497 & -13.743 & -11.530 & -1.470 & -1.938 \\
			65  & -6.295 & -6.281 & -3.386 & -2.461 & -14.251 & -11.848 & -1.471 & -1.948 \\
			70  & -6.527 & -6.736 & -2.812 & -2.066 & -14.280 & -11.514 & -1.518 & -2.013 \\
			75  & -6.675 & -6.876 & -3.003 & -1.707 & -13.646 & -11.191 & -1.587 & -2.375 \\
			80  & -6.353 & -6.661 & -3.039 & -2.335 & -13.932 & -11.564 & -1.559 & -2.333 \\
			85  & -6.700 & -6.918 & -2.800 & -1.859 & -13.891 & -11.327 & -1.582 & -2.487 \\
			90  & -7.018 & -7.084 & -3.130 & -2.161 & -14.263 & -11.901 & -1.539 & -2.288 \\
			95  & -7.038 & -7.049 & -3.102 & -2.224 & -13.801 & -11.483 & -1.538 & -2.352 \\
			100 & -7.183 & -7.375 & -3.094 & -2.189 & -14.893 & -11.770 & -1.510 & -2.523 \\
			\hline
			$\sigma^2$ & 0.588 & 0.514 & 0.167 & 0.128 & 0.588 & 0.514 & 0.021 & 0.32 \\
			\hline
			\end{tabular}
	\end{center}
\end{table}
INSERT TABLE FOR MODEL TC COMPARISON

\csmfigure{umass-preview}{figures/umass-preview}{5.5in}{Preview -- UMass Topic Coherence}

\ref{fig:uci-preview}
\csmfigure{uci-preview}{figures/uci-preview}{5.5in}{Preview -- UCI Topic Coherence}
\newpage
\subsubsection{Results}


Discuss the difference between models on the different datasets.
Discuss the differences between different datasets, what gives us more meaningful information


\ref{tab:lsi_data_comp}
\begin{table}
	\caption{\label{tab:lsi_data_comp} LSI -- Topic Coherence Scores for Datasets on \emph{K} Topics}
	\begin{center}
		\begin{tabular}{| l | rr | rr | rr |}
			\hline
			{} & \multicolumn{2}{c|}{\textbf{Title}} & \multicolumn{2}{c|}{\textbf{Description}} & \multicolumn{2}{c|}{\textbf{Preview}} \\
			{} &       UMass &    UCI &  UMass &    UCI &   UMass &    UCI \\
			\hline
				10  &  -9.687 &  -5.975 &      -5.859 & -3.087 &  -4.915 & -5.294 \\
				15  & -10.903 &  -7.084 &      -4.984 & -2.546 &  -5.281 & -6.560 \\
				20  & -12.441 &  -8.593 &      -6.464 & -3.730 &  -5.217 & -6.153 \\
				25  & -13.247 &  -9.334 &      -5.371 & -2.800 &  -4.959 & -5.652 \\
				30  & -12.343 &  -8.254 &      -5.505 & -3.068 &  -5.604 & -5.834 \\
				35  & -13.305 &  -9.381 &      -6.197 & -3.892 &  -5.375 & -5.271 \\
				40  & -13.581 &  -9.475 &      -7.093 & -4.735 &  -5.096 & -5.069 \\
				45  & -13.980 &  -9.874 &      -7.416 & -5.037 &  -5.159 & -5.250 \\
				50  & -14.417 & -10.234 &      -8.008 & -5.462 &  -5.522 & -5.594 \\
				55  & -14.632 & -10.434 &      -8.439 & -5.753 &  -5.979 & -5.909 \\
				60  & -15.160 & -10.886 &      -8.833 & -6.179 &  -6.064 & -6.290 \\
				65  & -15.409 & -11.101 &      -8.871 & -6.191 &  -6.295 & -6.281 \\
				70  & -15.473 & -11.114 &      -9.790 & -6.880 &  -6.527 & -6.736 \\
				75  & -15.886 & -11.498 &      -9.749 & -6.873 &  -6.675 & -6.876 \\
				80  & -16.046 & -11.632 &     -10.414 & -7.567 &  -6.353 & -6.661 \\
				85  & -15.921 & -11.504 &     -10.697 & -7.795 &  -6.700 & -6.918 \\
				90  & -16.202 & -11.714 &     -10.873 & -7.913 &  -7.018 & -7.084 \\
				95  & -16.366 & -11.847 &     -11.292 & -8.359 &  -7.038 & -7.049 \\
				100 & -16.500 & -11.922 &     -11.395 & -8.325 &  -7.183 & -7.375 \\
			\hline
			$\sigma^2$ & 3.734 & 2.854 & 4.654 & 3.913 & 0.588 & 0.514 \\
			\hline
			\end{tabular}
	\end{center}
\end{table}
\ref{fig:umass-preview}

\ref{fig:lsi-umass-dataset-comparison}
\csmfigure{lsi-umass-dataset-comparison}{figures/lsi-umass-dataset-comparison}{5.5in}{LSI UMass Topic Coherence Dataset Comparison}
\ref{fig:lsi-uci-dataset-comparison}
\csmfigure{lsi-uci-dataset-comparison}{figures/lsi-uci-dataset-comparison}{5.5in}{LSI UCI Topic Coherence Dataset Comparison}


\ref{tab:lda_data_comp}
\begin{table}
	\caption{\label{tab:lda_data_comp} LDA -- Topic Coherence Scores for Datasets on \emph{K} Topics}
	\begin{center}
		\begin{tabular}{| l | rr | rr | rr |}
			\hline
			{} & \multicolumn{2}{c|}{\textbf{Title}} & \multicolumn{2}{c|}{\textbf{Description}} & \multicolumn{2}{c|}{\textbf{Preview}} \\
			\emph{K} &       UMass &    UCI &  UMass &    UCI &   UMass &    UCI \\
			\hline
				10  & -10.364 & -6.718 &      -5.029 & -2.108 &  -1.904 & -1.757 \\
				15  & -10.228 & -6.538 &      -4.982 & -2.166 &  -1.858 & -2.123 \\
				20  & -10.709 & -6.692 &      -5.500 & -2.685 &  -2.547 & -2.142 \\
				25  & -10.740 & -6.868 &      -6.044 & -3.063 &  -2.625 & -2.757 \\
				30  & -10.708 & -6.569 &      -5.728 & -2.860 &  -2.997 & -2.751 \\
				35  & -11.444 & -7.125 &      -6.394 & -3.325 &  -3.200 & -3.149 \\
				40  & -11.344 & -7.071 &      -6.402 & -3.462 &  -2.439 & -2.594 \\
				45  & -11.450 & -7.126 &      -6.353 & -3.189 &  -2.460 & -2.254 \\
				50  & -11.988 & -7.488 &      -7.371 & -4.098 &  -2.774 & -2.194 \\
				55  & -11.665 & -7.217 &      -7.116 & -3.905 &  -2.682 & -2.447 \\
				60  & -11.961 & -7.291 &      -7.108 & -3.846 &  -3.013 & -2.497 \\
				65  & -12.617 & -7.851 &      -7.067 & -3.781 &  -3.386 & -2.461 \\
				70  & -13.407 & -8.348 &      -7.887 & -4.547 &  -2.812 & -2.066 \\
				75  & -13.113 & -8.078 &      -7.675 & -4.205 &  -3.003 & -1.707 \\
				80  & -13.244 & -8.077 &      -7.919 & -4.410 &  -3.039 & -2.335 \\
				85  & -12.938 & -7.647 &      -8.869 & -5.257 &  -2.800 & -1.859 \\
				90  & -12.812 & -7.394 &      -8.374 & -4.769 &  -3.130 & -2.161 \\
				95  & -12.608 & -7.185 &      -8.354 & -4.755 &  -3.102 & -2.224 \\
				100 & -12.657 & -7.005 &      -9.127 & -5.264 &  -3.094 & -2.189 \\
			\hline
			$\sigma^2$ & 1.065 & 0.278 & 1.549 & 0.901 & 0.167 & 0.128 \\
			\hline
			\end{tabular}
	\end{center}
\end{table}
\ref{fig:umass-preview}
\ref{fig:lda-umass-dataset-comparison}
\csmfigure{lda-umass-dataset-comparison}{figures/lda-umass-dataset-comparison}{5.5in}{LDA UMass Topic Coherence Dataset Comparison}
\ref{fig:lda-uci-dataset-comparison}
\csmfigure{lda-uci-dataset-comparison}{figures/lda-uci-dataset-comparison}{5.5in}{LDA UCI Topic Coherence Dataset Comparison}


\ref{tab:lda2vec_data_comp}
\begin{table}
	\caption{\label{tab:lda2vec_data_comp} \emph{lda2vec} -- Topic Coherence Scores for Datasets on \emph{K} Topics}
	\begin{center}
		\begin{tabular}{| l | rr | rr | rr |}
			\hline
			{} & \multicolumn{2}{c|}{\textbf{Title}} & \multicolumn{2}{c|}{\textbf{Description}} & \multicolumn{2}{c|}{\textbf{Preview}} \\
			\emph{K} &       UMass &    UCI &  UMass &    UCI &   UMass &    UCI \\
			\hline
			10  &  -8.467 & -6.003 &      -8.102 & -5.023 & -16.717 & -13.474 \\
			15  & -10.435 & -7.471 &      -8.545 & -5.307 & -15.109 & -13.119 \\
			20  &  -7.414 & -4.765 &     -10.735 & -7.396 & -15.036 & -12.749 \\
			25  &  -8.998 & -6.074 &     -10.371 & -7.084 & -13.695 & -12.317 \\
			30  &  -7.937 & -5.026 &      -9.153 & -6.115 & -14.548 & -12.174 \\
			35  & -10.015 & -7.245 &      -8.889 & -6.156 & -15.275 & -12.683 \\
			40  &  -7.339 & -4.362 &      -8.641 & -5.582 & -14.925 & -12.061 \\
			45  &  -8.253 & -5.227 &     -10.986 & -7.708 & -15.067 & -12.288 \\
			50  &  -8.824 & -5.880 &      -9.806 & -6.577 & -14.713 & -12.002 \\
			55  &  -8.331 & -5.158 &      -9.597 & -6.652 & -14.663 & -12.104 \\
			60  &  -9.308 & -6.268 &      -9.469 & -6.379 & -13.743 & -11.530 \\
			65  &  -8.696 & -5.493 &      -9.580 & -6.328 & -14.251 & -11.848 \\
			70  & -10.347 & -7.495 &      -9.548 & -6.319 & -14.280 & -11.514 \\
			75  &  -8.864 & -5.819 &      -9.674 & -6.429 & -13.646 & -11.191 \\
			80  &  -8.117 & -5.212 &      -9.204 & -5.955 & -13.932 & -11.564 \\
			85  &  -9.374 & -6.693 &      -9.621 & -6.507 & -13.891 & -11.327 \\
			90  &  -9.924 & -7.191 &      -9.705 & -6.406 & -14.263 & -11.901 \\
			95  &  -9.208 & -6.048 &      -9.562 & -6.425 & -13.801 & -11.483 \\
			100 & -10.282 & -7.244 &      -9.562 & -6.260 & -14.893 & -11.770 \\
			\hline
			$\sigma^2$ & 0.903 & 0.936 & 0.493 & 0.409 & 0.588 & 0.514 \\
			\hline
			\end{tabular}
	\end{center}
\end{table}
\ref{fig:lda2vec-umass-dataset-comparison}
\csmfigure{lda2vec-umass-dataset-comparison}{figures/lda2vec-umass-dataset-comparison}{5.5in}{lda2vec UMass Topic Coherence Dataset Comparison}
\ref{fig:lda2vec-uci-dataset-comparison}
\csmfigure{lda2vec-uci-dataset-comparison}{figures/lda2vec-uci-dataset-comparison}{5.5in}{lda2vec UCI Topic Coherence Dataset Comparison}

\ref{tab:etm_data_comp}
\begin{table}
	\caption{\label{tab:etm_data_comp} ETM -- Topic Coherence Scores for Datasets on \emph{K} Topics}
	\begin{center}
		\begin{tabular}{| l | rr | rr | rr |}
			\hline
			{} & \multicolumn{2}{c|}{\textbf{Title}} & \multicolumn{2}{c|}{\textbf{Description}} & \multicolumn{2}{c|}{\textbf{Preview}} \\
			\emph{K} &       UMass &    UCI &  UMass &    UCI &   UMass &    UCI \\
			\hline
			10  & -8.265 & -5.273 &      -5.536 & -2.517 &  -1.025 & -0.683 \\
			15  & -9.573 & -6.428 &      -5.176 & -2.330 &  -1.170 & -1.047 \\
			20  & -9.846 & -6.576 &      -4.523 & -1.583 &  -1.290 & -0.839 \\
			25  & -9.464 & -6.280 &      -5.693 & -2.775 &  -1.284 & -1.009 \\
			30  & -9.132 & -6.043 &      -5.538 & -2.654 &  -1.374 & -1.617 \\
			35  & -9.474 & -6.392 &      -5.352 & -2.333 &  -1.385 & -1.648 \\
			40  & -7.934 & -5.010 &      -5.591 & -2.603 &  -1.418 & -1.702 \\
			45  & -8.366 & -5.353 &      -5.183 & -2.271 &  -1.447 & -1.812 \\
			50  & -8.853 & -5.888 &      -5.183 & -2.259 &  -1.448 & -1.961 \\
			55  & -8.901 & -5.886 &      -5.209 & -2.361 &  -1.474 & -2.083 \\
			60  & -9.555 & -6.461 &      -4.529 & -1.724 &  -1.470 & -1.938 \\
			65  & -8.659 & -5.651 &      -5.019 & -2.093 &  -1.471 & -1.948 \\
			70  & -9.415 & -6.341 &      -5.053 & -2.057 &  -1.518 & -2.013 \\
			75  & -8.808 & -5.822 &      -5.268 & -2.308 &  -1.587 & -2.375 \\
			80  & -9.650 & -6.488 &      -5.130 & -2.200 &  -1.559 & -2.333 \\
			85  & -9.124 & -6.102 &      -5.356 & -2.490 &  -1.582 & -2.487 \\
			90  & -9.846 & -6.690 &      -5.104 & -2.225 &  -1.539 & -2.288 \\
			95  & -8.814 & -5.773 &      -5.386 & -2.530 &  -1.538 & -2.352 \\
			100 & -9.672 & -6.582 &      -5.119 & -2.282 &  -1.510 & -2.523 \\
			\hline
			$\sigma^2$ & 0.311 & 0.236 & 0.094 & 0.086 & 0.021 & 0.32 \\
			\hline
			\end{tabular}
	\end{center}
\end{table}
\ref{fig:etm-umass-dataset-comparison}
\csmfigure{etm-umass-dataset-comparison}{figures/etm-umass-dataset-comparison}{5.5in}{ETM UMass Topic Coherence Dataset Comparison}
\ref{fig:etm-uci-dataset-comparison}
\csmfigure{etm-uci-dataset-comparison}{figures/etm-uci-dataset-comparison}{5.5in}{ETM UCI Topic Coherence Dataset Comparison}


%umass-dataset-comparison

\chapter{Further Analysis}
Discuss the implications of this research.

What do these topics tell us about crowdsourcing
What do these topics tell us about the value of different parts of the dataset
What is the most valuable part of HITs on MTURK (in regards to title, description, preview)?
How can this information be used to improve crowdworker efficiency?

Discuss interesting insights like the coronavirus
\chapter{Conclusion}
Conclude


% \begin{eqnarray}
% 	\label{eq:importance}
% 		\textrm{Importance} & \approx & 0 \\
% 	\label{eq:newton}
% 		\sum_{i}^{\infty}\vec{F_{i}} & = & m\,\vec{a}
% \end{eqnarray}

% \begin{align}
% 	\label{eq:vector}
% 	\newcommand{\Tmat}[3][]{{^{#2}_{#3}}#1\mathrm{\mathbf{T}}\;\,}
% 	{\left[\begin{matrix}x\\ y\\ 1\end{matrix}\right]}=\Tmat{S}{W}{\left[\begin{matrix}0\\ 0\\ 1\end{matrix}\right]}
% \end{align}


%% Below is a quick test for a figure that moves to another page
asdfadsf

asdfasdf

dfsdfds

asd 



%% Note: the ''[H]`` below is optional and means ''always put it exactly here``, normally you do NOT want to use it.  However, in some rare circumstances you may wish to override the default placement.
% \csmfigure[H]{Stomata}{figures/stomata}{4in}{A pretty picture from the Squier Group --- this is a test of the emergency long-title system.}
%% End special test

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% If you would like to work on each chapter of your thesis in a separate document then use: %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \include{latex-example-chapter}

% \chapter{Second Generation Chapter}
% \label{cha:important-chapter}
% Another chapter.

% \subsection{Lots of Mistakes Originally}
% Fun fun...

% \subsection{Figured out How to Fix Things}
% Ha-ha!

% \subsection{Could Still Be Better}
% Interesting huh?

% \subsection{Testing Procedure}
% I thought you'd like this.

% \subsection{Final Results}
% It's over (see \ref{fig:Strongbad})!  Also it is important to note the placement of labels in subfigures: \ref{fig:fsm}, and \ref{fig:fsm-pirates}.
% % NOTE: You may wish to provide a shortened caption in the list of figures, \csmlongfigure allows you to do this (the two captions are combined in the text):
% \csmlongfigure{Strongbad}{figures/strongbad}{1in}{A world-class hero}{ of awesomeness~\cite{ref:Wikipedia}.}

% \begin{figure}
% 	\begin{center}
% 		\subfigure[Him]{
% 			% Example for including pictures when using the ``graphicx'' package:
% 			%\includegraphics[width=4in]{figures/fsm}
% 			% Example for including pictures when using the ``graphics'' package:
% 			\resizebox{4in}{!}{\includegraphics{figures/fsm}}
% 		} \\
% 		\subfigure[Importance of Pirates]{
% 			\resizebox{4in}{!}{\includegraphics{figures/fsm-pirates}}
% 			\label{fig:fsm-pirates}
% 		}
% 		% Normal caption:
% 		\caption{\label{fig:fsm}The Flying Spaghetti Monster Knows All}
% 		% This caption is for testing purposes, it has borders :
% 		%\caption{\label{fig:fsm}Total electron density isosurface at 1.7 electrons/\AA$^3$ (a) showing higher electron concentration at 6-6 interfaces compared to 6-5 interfaces. (b) Total wave function density isosurface of 0.04 electrons/\AA$^3$ shows the relatively uniform density over 5-6 membered rings and the definite wave function holes through the eight-membered rings.}
% 	\end{center}
% \end{figure}

% \chapter{The Way Ahead}
% Ugh, another chapter~\cite{ref:D}!

% \subsection{How Things Could Be Better}
% We thought that was the end!

% \subsection{Why We Think Things Aren't Better}
% We really hoped it was anyway.

% \subsection{We Love Our Advisors}
% Are you really still reading this? Ok, then check out \ref{tab:magic}!
\ref{tab:magic}
\begin{table}
	\caption{\label{tab:magic} A table of tabular goodness.}
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			& B & b \\
			\hline
			B & BB & Bb \\
			\hline
			b & Bb & bb \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

%% Below is a test for sideways figure and table floats (it also works to wrap figures and tables in a ``landscape'' environment, but the method below is preferred)
% magic text a
% 
% \begin{sidewaystable}
% 
% \centering
% 
% \caption[Grooved Ware and Beaker Features, their Finds and Radiocarbon
% Dates]{Grooved Ware and Beaker Features, their Finds and Radiocarbon
% Dates; For a breakdown of the Pottery Assemblages see Tables I and
% III; for the Flints see Tables II and IV; for the Animal Bones see
% Table V.}
% 
% \begin{tabular}{|llllllllp{1in}lp{1in}|}
% \hline
% Context   &Length   &Breadth/   &Depth   &Profile   &Pottery   &Flint   &Animal   &Stone   &Other    &C14 Dates \\
%   &         &Diameter   &        &          &          &        & 
% Bones&&&\\
% \hline
% &&&&&&&&&&\\
% \multicolumn{10}{|l}{\bf Grooved Ware}&\\
% 784 &---   &0.90m &0.18m &Sloping U &P1    &$\times$46  &  $\times$8  &&$\times$2 bone&  2150$\pm$ 100 BC\\
% 785 &---   &1.00m &0.12  &Sloping U &P2--4 &$\times$23  &  $\times$21 & Hammerstone &---&---\\
% 962 &---   &1.37m &0.20m &Sloping U &P5--6 &$\times$48  &  $\times$57* & ---&     ---&1990 $\pm$ 80 BC (Layer 4) 1870 $\pm$90 BC (Layer 1)\\
% 983 &0.83m &0.73m &0.25m &Stepped U &---   &$\times$18  &  $\times$8 & ---& Fired clay&---\\
% &&&&&&&&&&\\
% \multicolumn{10}{|l}{\bf Beaker}&\\
% 552 &---   &0.68m &0.12m &Saucer    &P7--14 &---        & --- & --- &--- &---\\
% 790 &---   &0.60m &0.25m &U         &P15    &$\times$12 & --- & Quartzite-lump&--- &---\\
% 794 &2.89m &0.75m &0.25m &Irreg.    &P16    $\times$3   & --- & --- &--- &---\\
% \hline
% \end{tabular}
% \end{sidewaystable} 
% 
% \begin{sidewaystable}
% \centering
% 
% \caption[Grooved Ware and Beaker Features, their Finds and Radiocarbon
% Dates]{Grooved Ware and Beaker Features, their Finds and Radiocarbon
% Dates; For a breakdown of the Pottery Assemblages see Tables I and
% III; for the Flints see Tables II and IV; for the Animal Bones see
% Table V.}
% 
% \begin{tabular}{|llllllllp{1in}lp{1in}|}
% \hline
% Context   &Length   &Breadth/   &Depth   &Profile   &Pottery   &Flint   &Animal   &Stone   &Other    &C14 Dates \\
%   &         &Diameter   &        &          &          &        & 
% Bones&&&\\
% \hline
% &&&&&&&&&&\\
% \multicolumn{10}{|l}{\bf Grooved Ware}&\\
% 784 &---   &0.90m &0.18m &Sloping U &P1    &$\times$46  &  $\times$8  &&$\times$2 bone&  2150$\pm$ 100 BC\\
% 785 &---   &1.00m &0.12  &Sloping U &P2--4 &$\times$23  &  $\times$21 & Hammerstone &---&---\\
% 962 &---   &1.37m &0.20m &Sloping U &P5--6 &$\times$48  &  $\times$57* & ---&     ---&1990 $\pm$ 80 BC (Layer 4) 1870 $\pm$90 BC (Layer 1)\\
% 983 &0.83m &0.73m &0.25m &Stepped U &---   &$\times$18  &  $\times$8 & ---& Fired clay&---\\
% &&&&&&&&&&\\
% \multicolumn{10}{|l}{\bf Beaker}&\\
% 552 &---   &0.68m &0.12m &Saucer    &P7--14 &---        & --- & --- &--- &---\\
% 790 &---   &0.60m &0.25m &U         &P15    &$\times$12 & --- & Quartzite-lump&--- &---\\
% 794 &2.89m &0.75m &0.25m &Irreg.    &P16    $\times$3   & --- & --- &--- &---\\
% \hline
% \end{tabular}
% \end{sidewaystable} 
% 
% magic text b

%% Parts of a Thesis - Back Matter
\backmatter

%%% Parts of a Thesis - Back Matter - References Cited (required)

% Use "Advanced" Bibliography Techniques
% \bibliography{thesis-proposal.bib}
% \bibliographystyle{IEEEannot}
\bibliography{thesis}
% \addbibresource{thesis-proposal.bib}
%\printbibliography % <-- For using biblatex instead of natbib or the built-in bibliography utility

%%% Parts of a Thesis - Back Matter - Selected Bibliography (optional)
%\cleardoublepage
%\begin{selected-bibliography}
% Your selected bibliogrpahy would go here, a page break might also be necessary above.
%\end{selected-bibliography} 

%%% Parts of a Thesis - Back Matter - Appendices (if applicable)
% \appendix{TODO add if I need to add anything in the appendix}\label{app:encoding}
% \ref{tab:encoding} shows how several symbols appear in the rendered document.

% \begin{table}[H]
% 	\caption{\label{tab:encoding}This is where we have fun testing encoding}
% 	\begin{center}
% 		\begin{tabular}{|c|c|c|}
% 			\hline
% 			& Normal & Math \\
% 			\hline
% 			The greater than: & > & $>$ \\
% 			\hline
% 			The lesss than: & < & $<$ \\
% 			\hline
% 			The tilde: & \textasciitilde{} & $\sim$ \\
% 			\hline
% 		\end{tabular}
% 	\end{center}
% \end{table}

% \subsection{Test Appendix Sub-Section}\label{sec:longtable}
% \ref{tab:longtable} is an example of a very large ``longtable.''
% \begin{landscape}
% \begin{longtable}{|>{\centering}p{1.02in}|>{\centering}p{1.15in}|>{\centering}p{1in}|>{\centering}p{0.7in}|>{\centering}p{0.7in}|>{\centering}p{0.67in}|>{\centering}p{2.55in}|} %
% 	\endfirsthead % Remove this line to use the main header for the first page
% 	\hline%
% 	Age & Formation  & Thickness (feet)   & Thickness (feet)  & Thickness (feet)  & Aquifer?  & Lithology%
% 	\endhead%
% 	\caption{Stratigraphy of the Granite Mountains and Lost Creek areas\label{tab:longtable}}\\ %
% 	\hline
% 	Age & Formation%
% 	\footnote{Only major unconformities shown, indicated by break in table.%
% 	} & Thickness (feet)%
% 	\footnote{Generalized thicknesses from.%
% 	}  & Thickness (feet)%
% 	\footnote{Thicknesses shown are approximate and apply to Lost Creek vicinity
% 	only.%
% 	} & Thickness (feet)%
% 	\footnote{Thicknesses shown are from a public screened dataset of logged formation
% 	tops from the 12 townships surrounding Lost Creek.%
% 	} & Aquifer?%
% 	\footnote{Aquifer designations \textendash{} Lost Creek vicinity only.%
% 	} & Lithology \tabularnewline
% 	\hline 
% 	Quaternary  & Alluvium & - & 0-20 & - & Yes & Sands and clays derived chiefly from the Tertiary formations in the
% 	area. \tabularnewline
% 	\hline 
% 	Paleocene & Fort Union  & up to 3,000 & 4,650 & 6,500? & Yes & Consists of alternating fine to coarse grained sandstone siltstone
% 	and mudstone. Contains various layers of lignitic coal beds. \tabularnewline
% 	\hline
% 	\hline 
% 	Cretaceous  & Lance  & 1,700 to 2,700 & 2,950 & 4,000? & Yes & Interbedded sandstone, siltstone and mudstone. Gray to brownish gray.
% 	Locally carbonaceous. Sandstone is white to grayish orange. \tabularnewline
% 	\hline 
% 	Cretaceous & Fox Hills  &  & 550 & 1,800? & No & Consists of coarsening upward shale and fine-grained sand with thin
% 	coal beds near the top. Represents a transition from marine to non-marine
% 	environment. Grades into Lewis Shale at the base. \tabularnewline
% 	\hline 
% 	Cretaceous & Lewis Shale  & 1,250 & 1,200 & 1,050 to 2,000 & No & Interbedded dark-gray and olive-gray shale and olive-gray sandstone. \tabularnewline
% 	\hline
% 	\hline 
% 	Cretaceous & Mesaverde Group  & 0 to 1,000 & 800 & 300 to 500? & No & Gray to dark gray shales with interbedded buff to tan fine to medium
% 	grained sandstones. \tabularnewline
% 	\hline 
% 	Cretaceous & Steele and Niobrara Shales  & Cody Shale 4,500 to 5,000 & 2,000 to 2,500 & 2,400 to 5,000 & No & Steele shale is soft gray marine, Niobrara shale is dark gray and
% 	contains calcareous zones. \tabularnewline
% 	\hline 
% 	Cretaceous & Frontier  & 700 to 900 & 500 to 1,000 & 750 to 1,500 & Yes & Gray sandstone and sandy shale. \tabularnewline
% 	\hline 
% 	Cretaceous & Dakota  &  & 300 to 400 &  & Yes & Marine sandstone, tan to buff, fine to medium grained may contain
% 	carbonaceous shale layer. \tabularnewline
% 	\hline 
% 	Jurassic  & Nugget Sandstone  & 400 to 525 & 500 &  & Yes & Grayish to dull red coarse grained cross-bedded quartz sandstone. \tabularnewline
% 	\hline 
% 	Triassic  & Chugwater  & 1,275 & 1,500 &  & No & Red shale and siltstone contains gypsum partings near the base. \tabularnewline
% 	\hline 
% 	Permian  & Phosphoria  & 275 to 325 & 300 &  & No & Black to dark gray shale, chert and phosphorite. \tabularnewline
% 	\hline 
% 	Pennsylvanian  & Tensleep and Amsden and Madison  & 600 to 700 & 750 &  & No & White to gray sandstone containing thin limestone and dolomite partings.
% 	Red and green shale and dolomite, sandstone near base. \tabularnewline
% 	\hline 
% 	Cambrian  & Undifferentiated  & 900 to 1,000 & 1,000 &  & No & Siltstone and quartzite, including Flathead sandstone. \tabularnewline
% 	\hline
% 	\hline 
% 	Precambrian  & Basement  & - & - &  & No & Granites, metamorphic and igneous rocks. \tabularnewline
% 	\hline
% % %% Extend the above example to cross a double-page boundary
% % 	%\hline
% % 	Quaternary  & Alluvium & - & 0-20 & - & Yes & Sands and clays derived chiefly from the Tertiary formations in the
% % 	area. \tabularnewline
% % 	\hline 
% % 	Paleocene & Fort Union  & up to 3,000 & 4,650 & 6,500? & Yes & Consists of alternating fine to coarse grained sandstone siltstone
% % 	and mudstone. Contains various layers of lignitic coal beds. \tabularnewline
% % 	\hline
% % 	\hline 
% % 	Cretaceous  & Lance  & 1,700 to 2,700 & 2,950 & 4,000? & Yes & Interbedded sandstone, siltstone and mudstone. Gray to brownish gray.
% % 	Locally carbonaceous. Sandstone is white to grayish orange. \tabularnewline
% % 	\hline 
% % 	Cretaceous & Fox Hills  &  & 550 & 1,800? & No & Consists of coarsening upward shale and fine-grained sand with thin
% % 	coal beds near the top. Represents a transition from marine to non-marine
% % 	environment. Grades into Lewis Shale at the base. \tabularnewline
% % 	\hline 
% % 	Cretaceous & Lewis Shale  & 1,250 & 1,200 & 1,050 to 2,000 & No & Interbedded dark-gray and olive-gray shale and olive-gray sandstone. \tabularnewline
% % 	\hline
% % 	\hline 
% % 	Cretaceous & Mesaverde Group  & 0 to 1,000 & 800 & 300 to 500? & No & Gray to dark gray shales with interbedded buff to tan fine to medium
% % 	grained sandstones. \tabularnewline
% % 	\hline 
% % 	Cretaceous & Steele and Niobrara Shales  & Cody Shale 4,500 to 5,000 & 2,000 to 2,500 & 2,400 to 5,000 & No & Steele shale is soft gray marine, Niobrara shale is dark gray and
% % 	contains calcareous zones. \tabularnewline
% % 	\hline 
% % 	Cretaceous & Frontier  & 700 to 900 & 500 to 1,000 & 750 to 1,500 & Yes & Gray sandstone and sandy shale. \tabularnewline
% % 	\hline 
% % 	Cretaceous & Dakota  &  & 300 to 400 &  & Yes & Marine sandstone, tan to buff, fine to medium grained may contain
% % 	carbonaceous shale layer. \tabularnewline
% % 	\hline 
% % 	Jurassic  & Nugget Sandstone  & 400 to 525 & 500 &  & Yes & Grayish to dull red coarse grained cross-bedded quartz sandstone. \tabularnewline
% % 	\hline 
% % 	Triassic  & Chugwater  & 1,275 & 1,500 &  & No & Red shale and siltstone contains gypsum partings near the base. \tabularnewline
% % 	\hline 
% % 	Permian  & Phosphoria  & 275 to 325 & 300 &  & No & Black to dark gray shale, chert and phosphorite. \tabularnewline
% % 	\hline 
% % 	Pennsylvanian  & Tensleep and Amsden and Madison  & 600 to 700 & 750 &  & No & White to gray sandstone containing thin limestone and dolomite partings.
% % 	Red and green shale and dolomite, sandstone near base. \tabularnewline
% % 	\hline 
% % 	Cambrian  & Undifferentiated  & 900 to 1,000 & 1,000 &  & No & Siltstone and quartzite, including Flathead sandstone. \tabularnewline
% % 	\hline
% % 	\hline 
% % 	Precambrian  & Basement  & - & - &  & No & Granites, metamorphic and igneous rocks. \tabularnewline
% % 	\hline
% % %% END DOUBLE PAGE BOUNDARY EXAMPLE
% \end{longtable}
% \end{landscape}

% \begin{landscape}
% \begin{longtable}{|c|c|c|}
% 	\endfirsthead
% 	\caption{Test of a small longtable.} \\
% 	\hline
% 	A & B & C \\
% 	\hline
% 	1 & 2 & 3 \\
% 	\hline
% \end{longtable}
% \end{landscape}

% \begin{landscape}
% \begin{longtable}{|c|c|c|}
% 	\endfirsthead
% 	\caption{Test of a small longtable on the alternate page.} \\
% 	\hline
% 	1 & 2 & 3 \\
% 	\hline
% 	A & B & C \\
% 	\hline
% \end{longtable}
% \end{landscape}

% \subsection{Sub-Sections are Fun}
% Sorta...

% \appendix{Special Coolness}

% Insert ice cubes here (\ref{lst:hello-world}).

% \lstinputlisting[language=Matlab,label={lst:hello-world},caption={A MATLAB ``Hello World`` Example}]{matlab_code.m}

% %% Example for use with ``breqn'' automatic equation breaking:
% \ifbreqn
% 	\appendix{Equation Breaking Tests}
 
% 	\begin{equation*}
% 	r = \frac{i}{n F} = k' c_i \exp\left\{ \frac{-G^{\ddagger}}{R T} \right\}
% 	\end{equation*}
% 	\begin{equation*}
% 	r = \frac{i}{n F} = k' c_i \exp\left\{ \frac{-G^{\ddagger}}{R T} \right\}
% 	\end{equation*}

% 	Replace $j$ by $h-j$ and by $k-j$ in these sums to get [cf.~(\ref{sna74})]
% 	\begin{equation*}
% 	\label{sna74}
% 	\frac{1}{6} \left(\sigma(k,h,0) +\frac{3(h-1)}{h}\right)
% 	+\frac{1}{6} \left(\sigma(h,k,0) +\frac{3(k-1)}{k}\right)
% 	=\frac{1}{6} \left(\frac{h}{k} +\frac{k}{h} +\frac{1}{hk}\right)
% 	+\frac{1}{2} -\frac{1}{2h} -\frac{1}{2k},
% 	\end{equation*}
% 	which is equivalent to the desired result.
% \fi


% %% For figuring out the LyX multi-row problem
% \begin{table}
% \caption{\label{tab:Important-experimental-propertie}General experimental
% properties}
% \centering{}%
% \begin{tabular}{|c|c|c|}
% \cline{2-3} 
% \multicolumn{1}{c|}{} & Parameter & \tabularnewline
% \hline 
% \multirow{8}{*}{Silurian dolomite} & Speed, rpm (drainage) & 1200 \tabularnewline
% \cline{2-3} 
%  & Speed, rpm (forced imbibition) & 884\tabularnewline
% \cline{2-3} 
%  & Surfactant type  & S13D\tabularnewline
% \cline{2-3} 
%  & Surfactant concentration, ppm  & 5000\tabularnewline
% \cline{2-3} 
%  & IFT$\left(\frac{dyne}{cm}\right)$ & 16\tabularnewline
% \cline{2-3} 
%  & $\mu_{o}(cp)$ & 22\tabularnewline
% \cline{2-3} 
%  & $k_{f}(md)$ & 10000\tabularnewline
% \cline{2-3} 
%  & $\phi_{f}$ & 0.9\tabularnewline
% \cline{2-3} 
% \multirow{8}{*}{Thamama} & Speed, rpm (drainage) & 4000\tabularnewline
% \cline{2-3} 
%  & Speed, rpm (forced imbibition) & 3000\tabularnewline
% \cline{2-3} 
%  & Surfactant type  & Ethoxylated alcohol\tabularnewline
% \cline{2-3} 
%  & Surfactant concentration, ppm  & 20000\tabularnewline
% \cline{2-3} 
%  & IFT$\left(\frac{dyne}{cm}\right)$ & 18\tabularnewline
% \cline{2-3} 
%  & $\mu_{o}(cp)$ & 9.5\tabularnewline
% \cline{2-3} 
%  & $k_{f}(md)$ & 7000-10000\tabularnewline
% \cline{2-3} 
%  & $\phi_{f}$ & 0.9\tabularnewline
% \hline 
% \end{tabular}
% \end{table}

\end{document}
